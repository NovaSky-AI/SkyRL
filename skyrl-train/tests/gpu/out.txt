nohup: ignoring input
Downloading virtualenv (5.8MiB)
Downloading ruff (11.0MiB)
 Downloading virtualenv
Downloading black (1.7MiB)
 Downloading black
 Downloading ruff
Installed 14 packages in 40ms
============================= test session starts ==============================
platform linux -- Python 3.12.11, pytest-8.4.1, pluggy-1.6.0 -- /root/tgriggs/SkyRL/skyrl-train/.venv/bin/python3
cachedir: .pytest_cache
rootdir: /root/tgriggs/SkyRL/skyrl-train
configfile: pyproject.toml
plugins: jaxtyping-0.3.2, anyio-4.9.0, hydra-core-1.3.2, asyncio-1.0.0
asyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collecting ... collected 74 items / 64 deselected / 10 selected

tests/gpu/gpu_ci/test_engine_generation.py::test_inference_engines_generation[vllm] 2025-08-29 01:07:40.283 | INFO     | skyrl_train.utils.utils:prepare_runtime_environment:367 - `VLLM_USE_V1` is not specified, setting `VLLM_USE_V1` to 1. To override, set `VLLM_USE_V1` explicitly
2025-08-29 01:07:42,760	WARNING utils.py:426 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.
2025-08-29 01:07:42,761	WARNING utils.py:438 -- Ray currently does not support initializing Ray with fractional cpus. Your num_cpus will be truncated from 108.8 to 108.
2025-08-29 01:07:42,986	INFO worker.py:1927 -- Started a local Ray instance.
2025-08-29 01:07:43,093	INFO packaging.py:588 -- Creating a file package for local module '/root/tgriggs/SkyRL/skyrl-train'.
2025-08-29 01:07:43,184	INFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_9ae412d8c8d67303.zip' (3.00MiB) to Ray cluster...
2025-08-29 01:07:43,203	INFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_9ae412d8c8d67303.zip'.
[33m(raylet)[0m warning: `VIRTUAL_ENV=/root/tgriggs/SkyRL/skyrl-train/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead
[33m(raylet)[0m Using CPython 3.12.11
[33m(raylet)[0m Creating virtual environment at: .venv
[33m(raylet)[0m    Building skyrl-train @ file:///tmp/ray/session_2025-08-29_01-07-40_377947_5522/runtime_resources/working_dir_files/_ray_pkg_9ae412d8c8d67303
[33m(raylet)[0m    Building skyrl-gym @ file:///tmp/ray/session_2025-08-29_01-07-40_377947_5522/runtime_resources/working_dir_files/_ray_pkg_9ae412d8c8d67303/skyrl-gym
[33m(raylet)[0m       Built skyrl-gym @ file:///tmp/ray/session_2025-08-29_01-07-40_377947_5522/runtime_resources/working_dir_files/_ray_pkg_9ae412d8c8d67303/skyrl-gym
[33m(raylet)[0m       Built skyrl-train @ file:///tmp/ray/session_2025-08-29_01-07-40_377947_5522/runtime_resources/working_dir_files/_ray_pkg_9ae412d8c8d67303
[33m(raylet)[0m Installed 192 packages in 794ms
2025-08-29 01:07:49.709 | INFO     | skyrl_train.utils.ppo_utils:sync_registries:505 - Synced registries to ray actor
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1319 examples [00:00, 71995.61 examples/s]
2025-08-29 01:07:51.118 | INFO     | skyrl_train.dataset.dataset:_read_files_and_tokenize:39 - dataset len: 1319
Filtering prompts longer than 512 tokens (num_proc=8):   0%|          | 0/1319 [00:00<?, ? examples/s]Filtering prompts longer than 512 tokens (num_proc=8):  13%|â–ˆâ–Ž        | 165/1319 [00:00<00:05, 206.67 examples/s]Filtering prompts longer than 512 tokens (num_proc=8):  25%|â–ˆâ–ˆâ–Œ       | 330/1319 [00:00<00:02, 387.55 examples/s]Filtering prompts longer than 512 tokens (num_proc=8):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 495/1319 [00:01<00:01, 520.28 examples/s]Filtering prompts longer than 512 tokens (num_proc=8):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 660/1319 [00:01<00:01, 622.79 examples/s]Filtering prompts longer than 512 tokens (num_proc=8):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 825/1319 [00:01<00:00, 688.16 examples/s]Filtering prompts longer than 512 tokens (num_proc=8):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 990/1319 [00:01<00:00, 745.13 examples/s]Filtering prompts longer than 512 tokens (num_proc=8):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1155/1319 [00:01<00:00, 798.52 examples/s]Filtering prompts longer than 512 tokens (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1319/1319 [00:02<00:00, 812.47 examples/s]Filtering prompts longer than 512 tokens (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1319/1319 [00:02<00:00, 598.79 examples/s]
2025-08-29 01:07:53.795 | INFO     | skyrl_train.dataset.dataset:_read_files_and_tokenize:51 - filter dataset len: 1319
Installed 178 packages in 810ms
INFO 08-29 01:08:06 [__init__.py:241] Automatically detected platform cuda.
INFO 08-29 01:08:17 [__init__.py:711] Resolved architecture: Qwen2ForCausalLM
INFO 08-29 01:08:17 [__init__.py:1750] Using max model len 32768
INFO 08-29 01:08:18 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.
INFO 08-29 01:08:18 [__init__.py:3565] Cudagraph is disabled under eager mode
[1;36m(EngineCore_0 pid=13389)[0;0m INFO 08-29 01:08:19 [core.py:636] Waiting for init message from front-end.
[1;36m(EngineCore_0 pid=13389)[0;0m INFO 08-29 01:08:19 [core.py:74] Initializing a V1 LLM engine (v0.10.1.1) with config: model='Qwen/Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2.5-1.5B-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_0 pid=13389)[0;0m WARNING 08-29 01:08:19 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 128 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_0 pid=13389)[0;0m INFO 08-29 01:08:19 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_2ee79fc5'), local_subscribe_addr='ipc:///tmp/9a1df3e4-209f-4422-a6da-8d53aec5ebe8', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=13389)[0;0m INFO 08-29 01:08:22 [worker_base.py:587] Injected <class 'skyrl_train.inference_engines.vllm.vllm_engine.WorkerWrap'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['destroy_weights_update_group', 'init_weight_update_communicator', 'test_rpc', 'update_weights', 'update_weights_cuda_ipc']
[1;36m(EngineCore_0 pid=13389)[0;0m INFO 08-29 01:08:22 [worker_base.py:587] Injected <class 'skyrl_train.inference_engines.vllm.vllm_engine.WorkerWrap'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['destroy_weights_update_group', 'init_weight_update_communicator', 'test_rpc', 'update_weights', 'update_weights_cuda_ipc']
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP0 pid=13395)[0;0m INFO 08-29 01:08:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_23198566'), local_subscribe_addr='ipc:///tmp/3b203e60-369b-4ab7-b23b-1df13c73ee77', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP1 pid=13399)[0;0m INFO 08-29 01:08:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3a8ca0e6'), local_subscribe_addr='ipc:///tmp/82b5ba2e-167b-4cf5-a237-4fbd8cfd8e15', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP1 pid=13399)[0;0m INFO 08-29 01:08:23 [__init__.py:1418] Found nccl from library libnccl.so.2
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP1 pid=13399)[0;0m INFO 08-29 01:08:23 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP0 pid=13395)[0;0m INFO 08-29 01:08:23 [__init__.py:1418] Found nccl from library libnccl.so.2
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP0 pid=13395)[0;0m INFO 08-29 01:08:23 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP1 pid=13399)[0;0m INFO 08-29 01:08:24 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP0 pid=13395)[0;0m INFO 08-29 01:08:24 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP0 pid=13395)[0;0m INFO 08-29 01:08:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_f66b3bdc'), local_subscribe_addr='ipc:///tmp/c9150a7d-56ce-4c6e-8eed-8cbf87d67221', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP0 pid=13395)[0;0m INFO 08-29 01:08:24 [parallel_state.py:1134] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP1 pid=13399)[0;0m INFO 08-29 01:08:24 [parallel_state.py:1134] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP0 pid=13395)[0;0m INFO 08-29 01:08:24 [topk_topp_sampler.py:50] Using FlashInfer for top-p & top-k sampling.
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP1 pid=13399)[0;0m INFO 08-29 01:08:24 [topk_topp_sampler.py:50] Using FlashInfer for top-p & top-k sampling.
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP1 pid=13399)[0;0m INFO 08-29 01:08:24 [gpu_model_runner.py:1953] Starting to load model Qwen/Qwen2.5-1.5B-Instruct...
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP0 pid=13395)[0;0m INFO 08-29 01:08:24 [gpu_model_runner.py:1953] Starting to load model Qwen/Qwen2.5-1.5B-Instruct...
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP0 pid=13395)[0;0m INFO 08-29 01:08:24 [gpu_model_runner.py:1985] Loading model from scratch...
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP1 pid=13399)[0;0m INFO 08-29 01:08:24 [gpu_model_runner.py:1985] Loading model from scratch...
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP0 pid=13395)[0;0m INFO 08-29 01:08:24 [cuda.py:328] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP1 pid=13399)[0;0m INFO 08-29 01:08:24 [cuda.py:328] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP1 pid=13399)[0;0m INFO 08-29 01:08:24 [weight_utils.py:296] Using model weights format ['*.safetensors']
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP0 pid=13395)[0;0m INFO 08-29 01:08:24 [weight_utils.py:296] Using model weights format ['*.safetensors']
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP1 pid=13399)[0;0m INFO 08-29 01:08:50 [weight_utils.py:312] Time spent downloading weights for Qwen/Qwen2.5-1.5B-Instruct: 26.080860 seconds
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP1 pid=13399)[0;0m INFO 08-29 01:08:51 [weight_utils.py:349] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP0 pid=13395)[0;0m INFO 08-29 01:08:51 [weight_utils.py:349] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP0 pid=13395)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP1 pid=13399)[0;0m INFO 08-29 01:08:51 [default_loader.py:262] Loading weights took 0.44 seconds
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP0 pid=13395)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.99it/s]
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP0 pid=13395)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.99it/s]
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP0 pid=13395)[0;0m 
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP0 pid=13395)[0;0m INFO 08-29 01:08:51 [default_loader.py:262] Loading weights took 0.44 seconds
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP1 pid=13399)[0;0m INFO 08-29 01:08:52 [gpu_model_runner.py:2007] Model loading took 1.4490 GiB and 26.941539 seconds
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP0 pid=13395)[0;0m INFO 08-29 01:08:52 [gpu_model_runner.py:2007] Model loading took 1.4490 GiB and 27.124279 seconds
[33m(raylet)[0m warning: `VIRTUAL_ENV=/root/tgriggs/SkyRL/skyrl-train/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP0 pid=13395)[0;0m /root/.cache/uv/builds-v0/.tmpim0SaM/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP0 pid=13395)[0;0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP0 pid=13395)[0;0m   warnings.warn(
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP1 pid=13399)[0;0m /root/.cache/uv/builds-v0/.tmpim0SaM/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP1 pid=13399)[0;0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP1 pid=13399)[0;0m   warnings.warn(
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP0 pid=13395)[0;0m INFO 08-29 01:08:55 [gpu_worker.py:276] Available KV cache memory: 60.22 GiB
[1;36m(EngineCore_0 pid=13389)[0;0m [1;36m(VllmWorker TP1 pid=13399)[0;0m INFO 08-29 01:08:55 [gpu_worker.py:276] Available KV cache memory: 60.22 GiB
[1;36m(EngineCore_0 pid=13389)[0;0m INFO 08-29 01:08:56 [kv_cache_utils.py:849] GPU KV cache size: 4,510,560 tokens
[1;36m(EngineCore_0 pid=13389)[0;0m INFO 08-29 01:08:56 [kv_cache_utils.py:853] Maximum concurrency for 32,768 tokens per request: 137.65x
[1;36m(EngineCore_0 pid=13389)[0;0m INFO 08-29 01:08:56 [kv_cache_utils.py:849] GPU KV cache size: 4,510,560 tokens
[1;36m(EngineCore_0 pid=13389)[0;0m INFO 08-29 01:08:56 [kv_cache_utils.py:853] Maximum concurrency for 32,768 tokens per request: 137.65x
[1;36m(EngineCore_0 pid=13389)[0;0m INFO 08-29 01:08:56 [core.py:214] init engine (profile, create kv cache, warmup model) took 4.22 seconds
CUDA_VISIBLE_DEVICES not set. Using all 4 GPUs: [0, 1, 2, 3]
This might conflict with other processes. Consider setting CUDA_VISIBLE_DEVICES explicitly.
Using GPUs 0,1 for vLLM server (tensor_parallel_size=2)
FAILED
tests/gpu/gpu_ci/test_engine_generation.py::test_token_based_generation[vllm] 2025-08-29 01:08:57.032 | INFO     | skyrl_train.utils.utils:prepare_runtime_environment:367 - `VLLM_USE_V1` is not specified, setting `VLLM_USE_V1` to 1. To override, set `VLLM_USE_V1` explicitly
[1;36m(EngineCore_0 pid=13389)[0;0m INFO 08-29 01:08:58 [__init__.py:3565] Cudagraph is disabled under eager mode
INFO 08-29 01:08:58 [loggers.py:142] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 281910
INFO 08-29 01:08:58 [api_server.py:1611] Supported_tasks: ['generate']
WARNING 08-29 01:08:58 [__init__.py:1625] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
INFO 08-29 01:08:58 [serving_responses.py:120] Using default chat sampling params from model: {'repetition_penalty': 1.1, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
INFO 08-29 01:08:58 [serving_chat.py:134] Using default chat sampling params from model: {'repetition_penalty': 1.1, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
INFO 08-29 01:08:58 [serving_completion.py:77] Using default completion sampling params from model: {'repetition_penalty': 1.1, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
INFO 08-29 01:08:58 [launcher.py:36] Available routes are:
INFO 08-29 01:08:58 [launcher.py:44] Route: /openapi.json, Methods: GET, HEAD
INFO 08-29 01:08:58 [launcher.py:44] Route: /docs, Methods: GET, HEAD
INFO 08-29 01:08:58 [launcher.py:44] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 08-29 01:08:58 [launcher.py:44] Route: /redoc, Methods: GET, HEAD
INFO 08-29 01:08:58 [launcher.py:44] Route: /health, Methods: GET
INFO 08-29 01:08:58 [launcher.py:44] Route: /load, Methods: GET
INFO 08-29 01:08:58 [launcher.py:44] Route: /ping, Methods: POST
INFO 08-29 01:08:58 [launcher.py:44] Route: /ping, Methods: GET
INFO 08-29 01:08:58 [launcher.py:44] Route: /tokenize, Methods: POST
INFO 08-29 01:08:58 [launcher.py:44] Route: /detokenize, Methods: POST
INFO 08-29 01:08:58 [launcher.py:44] Route: /v1/models, Methods: GET
INFO 08-29 01:08:58 [launcher.py:44] Route: /version, Methods: GET
INFO 08-29 01:08:58 [launcher.py:44] Route: /v1/responses, Methods: POST
INFO 08-29 01:08:58 [launcher.py:44] Route: /v1/responses/{response_id}, Methods: GET
INFO 08-29 01:08:58 [launcher.py:44] Route: /v1/responses/{response_id}/cancel, Methods: POST
INFO 08-29 01:08:58 [launcher.py:44] Route: /v1/chat/completions, Methods: POST
INFO 08-29 01:08:58 [launcher.py:44] Route: /v1/completions, Methods: POST
INFO 08-29 01:08:58 [launcher.py:44] Route: /v1/embeddings, Methods: POST
INFO 08-29 01:08:58 [launcher.py:44] Route: /pooling, Methods: POST
INFO 08-29 01:08:58 [launcher.py:44] Route: /classify, Methods: POST
INFO 08-29 01:08:58 [launcher.py:44] Route: /score, Methods: POST
INFO 08-29 01:08:58 [launcher.py:44] Route: /v1/score, Methods: POST
INFO 08-29 01:08:58 [launcher.py:44] Route: /v1/audio/transcriptions, Methods: POST
INFO 08-29 01:08:58 [launcher.py:44] Route: /v1/audio/translations, Methods: POST
INFO 08-29 01:08:58 [launcher.py:44] Route: /rerank, Methods: POST
INFO 08-29 01:08:58 [launcher.py:44] Route: /v1/rerank, Methods: POST
INFO 08-29 01:08:58 [launcher.py:44] Route: /v2/rerank, Methods: POST
INFO 08-29 01:08:58 [launcher.py:44] Route: /scale_elastic_ep, Methods: POST
INFO 08-29 01:08:58 [launcher.py:44] Route: /is_scaling_elastic_ep, Methods: POST
INFO 08-29 01:08:58 [launcher.py:44] Route: /invocations, Methods: POST
INFO 08-29 01:08:58 [launcher.py:44] Route: /metrics, Methods: GET
INFO 08-29 01:08:58 [launcher.py:44] Route: /init_weight_update_communicator, Methods: POST
INFO 08-29 01:08:58 [launcher.py:44] Route: /sleep, Methods: POST
INFO 08-29 01:08:58 [launcher.py:44] Route: /wake_up, Methods: POST
INFO 08-29 01:08:58 [launcher.py:44] Route: /reset_prefix_cache, Methods: POST
INFO 08-29 01:08:58 [launcher.py:44] Route: /update_weights, Methods: POST
INFO 08-29 01:08:58 [launcher.py:44] Route: /destroy_weights_update_group, Methods: POST
INFO:     Started server process [13114]
INFO:     Waiting for application startup.
2025-08-29 01:08:58,952	WARNING utils.py:426 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.
2025-08-29 01:08:58,953	WARNING utils.py:438 -- Ray currently does not support initializing Ray with fractional cpus. Your num_cpus will be truncated from 108.8 to 108.
INFO:     Application startup complete.
2025-08-29 01:08:59,168	INFO worker.py:1927 -- Started a local Ray instance.
2025-08-29 01:08:59,285	INFO packaging.py:588 -- Creating a file package for local module '/root/tgriggs/SkyRL/skyrl-train'.
2025-08-29 01:08:59,381	INFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_34301d00bba99aac.zip' (3.05MiB) to Ray cluster...
2025-08-29 01:08:59,401	INFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_34301d00bba99aac.zip'.
[33m(raylet)[0m warning: `VIRTUAL_ENV=/root/tgriggs/SkyRL/skyrl-train/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead
[33m(raylet)[0m Using CPython 3.12.11
[33m(raylet)[0m Creating virtual environment at: .venv
[33m(raylet)[0m    Building skyrl-train @ file:///tmp/ray/session_2025-08-29_01-08-57_040868_5522/runtime_resources/working_dir_files/_ray_pkg_34301d00bba99aac
[33m(raylet)[0m    Building skyrl-gym @ file:///tmp/ray/session_2025-08-29_01-08-57_040868_5522/runtime_resources/working_dir_files/_ray_pkg_34301d00bba99aac/skyrl-gym
[33m(raylet)[0m       Built skyrl-gym @ file:///tmp/ray/session_2025-08-29_01-08-57_040868_5522/runtime_resources/working_dir_files/_ray_pkg_34301d00bba99aac/skyrl-gym
[33m(raylet)[0m       Built skyrl-train @ file:///tmp/ray/session_2025-08-29_01-08-57_040868_5522/runtime_resources/working_dir_files/_ray_pkg_34301d00bba99aac
[33m(raylet)[0m Installed 192 packages in 790ms
2025-08-29 01:09:05.820 | INFO     | skyrl_train.utils.ppo_utils:sync_registries:505 - Synced registries to ray actor
2025-08-29 01:09:06.438 | INFO     | skyrl_train.dataset.dataset:_read_files_and_tokenize:39 - dataset len: 1319
Filtering prompts longer than 512 tokens (num_proc=8):   0%|          | 0/1319 [00:00<?, ? examples/s]Filtering prompts longer than 512 tokens (num_proc=8):  13%|â–ˆâ–Ž        | 165/1319 [00:00<00:05, 216.50 examples/s]Filtering prompts longer than 512 tokens (num_proc=8):  25%|â–ˆâ–ˆâ–Œ       | 330/1319 [00:00<00:02, 384.24 examples/s]Filtering prompts longer than 512 tokens (num_proc=8):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 495/1319 [00:01<00:01, 537.32 examples/s]Filtering prompts longer than 512 tokens (num_proc=8):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 660/1319 [00:01<00:01, 635.08 examples/s]Filtering prompts longer than 512 tokens (num_proc=8):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 825/1319 [00:01<00:00, 713.68 examples/s]Filtering prompts longer than 512 tokens (num_proc=8):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 990/1319 [00:01<00:00, 749.94 examples/s]Filtering prompts longer than 512 tokens (num_proc=8):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1155/1319 [00:01<00:00, 799.89 examples/s]Filtering prompts longer than 512 tokens (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1319/1319 [00:02<00:00, 826.73 examples/s]Filtering prompts longer than 512 tokens (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1319/1319 [00:02<00:00, 609.42 examples/s]
2025-08-29 01:09:09.119 | INFO     | skyrl_train.dataset.dataset:_read_files_and_tokenize:51 - filter dataset len: 1319
[33m(raylet)[0m warning: `VIRTUAL_ENV=/root/tgriggs/SkyRL/skyrl-train/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead
FAILED
tests/gpu/test_policy_local_engines_e2e.py::test_policy_local_engines_e2e[no_colocate_nccl_fsdp_vllm] 2025-08-29 01:09:12.500 | INFO     | skyrl_train.utils.utils:prepare_runtime_environment:367 - `VLLM_USE_V1` is not specified, setting `VLLM_USE_V1` to 1. To override, set `VLLM_USE_V1` explicitly
2025-08-29 01:09:14,424	WARNING utils.py:426 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.
2025-08-29 01:09:14,424	WARNING utils.py:438 -- Ray currently does not support initializing Ray with fractional cpus. Your num_cpus will be truncated from 108.8 to 108.
2025-08-29 01:09:14,637	INFO worker.py:1927 -- Started a local Ray instance.
2025-08-29 01:09:14,738	INFO packaging.py:588 -- Creating a file package for local module '/root/tgriggs/SkyRL/skyrl-train'.
2025-08-29 01:09:14,832	INFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_d3263afc7c3fe63e.zip' (3.05MiB) to Ray cluster...
2025-08-29 01:09:14,848	INFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_d3263afc7c3fe63e.zip'.
[33m(raylet)[0m warning: `VIRTUAL_ENV=/root/tgriggs/SkyRL/skyrl-train/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead
[33m(raylet)[0m Using CPython 3.12.11
[33m(raylet)[0m Creating virtual environment at: .venv
[33m(raylet)[0m    Building skyrl-train @ file:///tmp/ray/session_2025-08-29_01-09-12_508490_5522/runtime_resources/working_dir_files/_ray_pkg_d3263afc7c3fe63e
[33m(raylet)[0m    Building skyrl-gym @ file:///tmp/ray/session_2025-08-29_01-09-12_508490_5522/runtime_resources/working_dir_files/_ray_pkg_d3263afc7c3fe63e/skyrl-gym
[33m(raylet)[0m       Built skyrl-gym @ file:///tmp/ray/session_2025-08-29_01-09-12_508490_5522/runtime_resources/working_dir_files/_ray_pkg_d3263afc7c3fe63e/skyrl-gym
[33m(raylet)[0m       Built skyrl-train @ file:///tmp/ray/session_2025-08-29_01-09-12_508490_5522/runtime_resources/working_dir_files/_ray_pkg_d3263afc7c3fe63e
[33m(raylet)[0m Installed 192 packages in 778ms
2025-08-29 01:09:21.156 | INFO     | skyrl_train.utils.ppo_utils:sync_registries:505 - Synced registries to ray actor
[33m(raylet)[0m warning: `VIRTUAL_ENV=/root/tgriggs/SkyRL/skyrl-train/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(pid=28832)[0m Using blocking ray.get inside async actor. This blocks the event loop. Please use `await` on object ref with asyncio.gather if you want to yield execution to the event loop instead.
[33m(raylet)[0m warning: `VIRTUAL_ENV=/root/tgriggs/SkyRL/skyrl-train/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead[32m [repeated 2x across cluster][0m
2025-08-29 01:09:36.957 | INFO     | skyrl_train.workers.worker:_initiate_actors:460 - Initializing process group for RayActorGroup
[36m(AsyncVLLMInferenceEngine pid=28711)[0m Exception raised in creation task: The actor died because of an error raised in its creation task, [36mray::AsyncVLLMInferenceEngine.__init__()[39m (pid=28711, ip=172.18.0.2, actor_id=f5e5ad585e882785bc6b26c501000000, repr=<skyrl_train.inference_engines.vllm.vllm_engine.AsyncVLLMInferenceEngine object at 0x74b243a65700>)
[36m(AsyncVLLMInferenceEngine pid=28711)[0m   File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 456, in result
[36m(AsyncVLLMInferenceEngine pid=28711)[0m     return self.__get_result()
[36m(AsyncVLLMInferenceEngine pid=28711)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=28711)[0m   File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
[36m(AsyncVLLMInferenceEngine pid=28711)[0m     raise self._exception
[36m(AsyncVLLMInferenceEngine pid=28711)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=28711)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=28711)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=28711)[0m   File "/tmp/ray/session_2025-08-29_01-09-12_508490_5522/runtime_resources/working_dir_files/_ray_pkg_d3263afc7c3fe63e/skyrl_train/inference_engines/vllm/vllm_engine.py", line 168, in __init__
[36m(AsyncVLLMInferenceEngine pid=28711)[0m     self.llm = self._create_engine(*args, **kwargs)
[36m(AsyncVLLMInferenceEngine pid=28711)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=28711)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=28711)[0m   File "/tmp/ray/session_2025-08-29_01-09-12_508490_5522/runtime_resources/working_dir_files/_ray_pkg_d3263afc7c3fe63e/skyrl_train/inference_engines/vllm/vllm_engine.py", line 317, in _create_engine
[36m(AsyncVLLMInferenceEngine pid=28711)[0m     engine_args = vllm.AsyncEngineArgs(disable_log_requests=True, **kwargs)
[36m(AsyncVLLMInferenceEngine pid=28711)[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=28711)[0m TypeError: AsyncEngineArgs.__init__() got an unexpected keyword argument 'disable_log_requests'
[36m(pid=29035)[0m Using blocking ray.get inside async actor. This blocks the event loop. Please use `await` on object ref with asyncio.gather if you want to yield execution to the event loop instead.
[33m(raylet)[0m warning: `VIRTUAL_ENV=/root/tgriggs/SkyRL/skyrl-train/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead
2025-08-29 01:09:43.342 | INFO     | skyrl_train.workers.worker:_initiate_actors:462 - Initialized process group for RayActorGroup
2025-08-29 01:09:43.347 | INFO     | skyrl_train.workers.worker:_initiate_actors:464 - Mesh Ranks: [MeshRank(dp=0, sp=0, tp=0, world_size=2, dp_size=2), MeshRank(dp=1, sp=0, tp=0, world_size=2, dp_size=2)]
[36m(FSDPPolicyRayActorBase pid=28832)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(FSDPPolicyRayActorBase pid=28832)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2Model is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(FSDPPolicyRayActorBase pid=29035)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2Model is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`[32m [repeated 2x across cluster][0m
InferenceEngineClient initialized with 1 engines.
[36m(AsyncVLLMInferenceEngine pid=28711)[0m creating LLM with bundle_indices=[0, 1]
[36m(AsyncVLLMInferenceEngine pid=28711)[0m INFO 08-29 01:09:35 [__init__.py:241] Automatically detected platform cuda.
FAILED
tests/gpu/test_policy_local_engines_e2e.py::test_policy_local_engines_e2e[colocate_nccl_fsdp_vllm] 2025-08-29 01:09:56.117 | INFO     | skyrl_train.utils.utils:prepare_runtime_environment:367 - `VLLM_USE_V1` is not specified, setting `VLLM_USE_V1` to 1. To override, set `VLLM_USE_V1` explicitly
2025-08-29 01:09:58,072	WARNING utils.py:426 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.
2025-08-29 01:09:58,072	WARNING utils.py:438 -- Ray currently does not support initializing Ray with fractional cpus. Your num_cpus will be truncated from 108.8 to 108.
2025-08-29 01:09:58,287	INFO worker.py:1927 -- Started a local Ray instance.
2025-08-29 01:09:58,396	INFO packaging.py:588 -- Creating a file package for local module '/root/tgriggs/SkyRL/skyrl-train'.
2025-08-29 01:09:58,494	INFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_49c55a07945afadb.zip' (3.13MiB) to Ray cluster...
2025-08-29 01:09:58,516	INFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_49c55a07945afadb.zip'.
[33m(raylet)[0m warning: `VIRTUAL_ENV=/root/tgriggs/SkyRL/skyrl-train/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead
[33m(raylet)[0m Using CPython 3.12.11
[33m(raylet)[0m Creating virtual environment at: .venv
[33m(raylet)[0m    Building skyrl-train @ file:///tmp/ray/session_2025-08-29_01-09-56_124453_5522/runtime_resources/working_dir_files/_ray_pkg_49c55a07945afadb
[33m(raylet)[0m    Building skyrl-gym @ file:///tmp/ray/session_2025-08-29_01-09-56_124453_5522/runtime_resources/working_dir_files/_ray_pkg_49c55a07945afadb/skyrl-gym
[33m(raylet)[0m       Built skyrl-train @ file:///tmp/ray/session_2025-08-29_01-09-56_124453_5522/runtime_resources/working_dir_files/_ray_pkg_49c55a07945afadb
[33m(raylet)[0m       Built skyrl-gym @ file:///tmp/ray/session_2025-08-29_01-09-56_124453_5522/runtime_resources/working_dir_files/_ray_pkg_49c55a07945afadb/skyrl-gym
[33m(raylet)[0m Installed 192 packages in 784ms
2025-08-29 01:10:05.531 | INFO     | skyrl_train.utils.ppo_utils:sync_registries:505 - Synced registries to ray actor
[33m(raylet)[0m warning: `VIRTUAL_ENV=/root/tgriggs/SkyRL/skyrl-train/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead[32m [repeated 2x across cluster][0m
[33m(raylet)[0m warning: `VIRTUAL_ENV=/root/tgriggs/SkyRL/skyrl-train/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead
[36m(AsyncVLLMInferenceEngine pid=36875)[0m creating LLM with bundle_indices=[0, 1]
[36m(AsyncVLLMInferenceEngine pid=36875)[0m INFO 08-29 01:10:19 [__init__.py:241] Automatically detected platform cuda.
FAILED
tests/gpu/test_policy_local_engines_e2e.py::test_policy_local_engines_e2e[no_colocate_gloo_fsdp_vllm] 2025-08-29 01:10:23.554 | INFO     | skyrl_train.utils.utils:prepare_runtime_environment:367 - `VLLM_USE_V1` is not specified, setting `VLLM_USE_V1` to 1. To override, set `VLLM_USE_V1` explicitly
2025-08-29 01:10:25,496	WARNING utils.py:426 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.
2025-08-29 01:10:25,496	WARNING utils.py:438 -- Ray currently does not support initializing Ray with fractional cpus. Your num_cpus will be truncated from 108.8 to 108.
2025-08-29 01:10:25,727	INFO worker.py:1927 -- Started a local Ray instance.
2025-08-29 01:10:25,830	INFO packaging.py:588 -- Creating a file package for local module '/root/tgriggs/SkyRL/skyrl-train'.
2025-08-29 01:10:25,927	INFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_bd2e4c86161387d3.zip' (3.13MiB) to Ray cluster...
2025-08-29 01:10:25,948	INFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_bd2e4c86161387d3.zip'.
[33m(raylet)[0m warning: `VIRTUAL_ENV=/root/tgriggs/SkyRL/skyrl-train/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead
[33m(raylet)[0m Using CPython 3.12.11
[33m(raylet)[0m Creating virtual environment at: .venv
[33m(raylet)[0m    Building skyrl-train @ file:///tmp/ray/session_2025-08-29_01-10-23_562698_5522/runtime_resources/working_dir_files/_ray_pkg_bd2e4c86161387d3
[33m(raylet)[0m    Building skyrl-gym @ file:///tmp/ray/session_2025-08-29_01-10-23_562698_5522/runtime_resources/working_dir_files/_ray_pkg_bd2e4c86161387d3/skyrl-gym
[33m(raylet)[0m       Built skyrl-gym @ file:///tmp/ray/session_2025-08-29_01-10-23_562698_5522/runtime_resources/working_dir_files/_ray_pkg_bd2e4c86161387d3/skyrl-gym
[33m(raylet)[0m       Built skyrl-train @ file:///tmp/ray/session_2025-08-29_01-10-23_562698_5522/runtime_resources/working_dir_files/_ray_pkg_bd2e4c86161387d3
[33m(raylet)[0m Installed 192 packages in 787ms
2025-08-29 01:10:32.225 | INFO     | skyrl_train.utils.ppo_utils:sync_registries:505 - Synced registries to ray actor
[33m(raylet)[0m warning: `VIRTUAL_ENV=/root/tgriggs/SkyRL/skyrl-train/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead[32m [repeated 2x across cluster][0m
[36m(pid=44807)[0m Using blocking ray.get inside async actor. This blocks the event loop. Please use `await` on object ref with asyncio.gather if you want to yield execution to the event loop instead.
[33m(raylet)[0m warning: `VIRTUAL_ENV=/root/tgriggs/SkyRL/skyrl-train/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead[32m [repeated 2x across cluster][0m
2025-08-29 01:10:46.672 | INFO     | skyrl_train.workers.worker:_initiate_actors:460 - Initializing process group for RayActorGroup
[36m(AsyncVLLMInferenceEngine pid=44685)[0m Exception raised in creation task: The actor died because of an error raised in its creation task, [36mray::AsyncVLLMInferenceEngine.__init__()[39m (pid=44685, ip=172.18.0.2, actor_id=a72c003d333b7559c4757c4201000000, repr=<skyrl_train.inference_engines.vllm.vllm_engine.AsyncVLLMInferenceEngine object at 0x73cfbadd47d0>)
[36m(AsyncVLLMInferenceEngine pid=44685)[0m   File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 456, in result
[36m(AsyncVLLMInferenceEngine pid=44685)[0m     return self.__get_result()
[36m(AsyncVLLMInferenceEngine pid=44685)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=44685)[0m   File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
[36m(AsyncVLLMInferenceEngine pid=44685)[0m     raise self._exception
[36m(AsyncVLLMInferenceEngine pid=44685)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=44685)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=44685)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=44685)[0m   File "/tmp/ray/session_2025-08-29_01-10-23_562698_5522/runtime_resources/working_dir_files/_ray_pkg_bd2e4c86161387d3/skyrl_train/inference_engines/vllm/vllm_engine.py", line 168, in __init__
[36m(AsyncVLLMInferenceEngine pid=44685)[0m     self.llm = self._create_engine(*args, **kwargs)
[36m(AsyncVLLMInferenceEngine pid=44685)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=44685)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=44685)[0m   File "/tmp/ray/session_2025-08-29_01-10-23_562698_5522/runtime_resources/working_dir_files/_ray_pkg_bd2e4c86161387d3/skyrl_train/inference_engines/vllm/vllm_engine.py", line 317, in _create_engine
[36m(AsyncVLLMInferenceEngine pid=44685)[0m     engine_args = vllm.AsyncEngineArgs(disable_log_requests=True, **kwargs)
[36m(AsyncVLLMInferenceEngine pid=44685)[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=44685)[0m TypeError: AsyncEngineArgs.__init__() got an unexpected keyword argument 'disable_log_requests'
[36m(pid=45009)[0m Using blocking ray.get inside async actor. This blocks the event loop. Please use `await` on object ref with asyncio.gather if you want to yield execution to the event loop instead.
[33m(raylet)[0m warning: `VIRTUAL_ENV=/root/tgriggs/SkyRL/skyrl-train/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead
2025-08-29 01:10:53.148 | INFO     | skyrl_train.workers.worker:_initiate_actors:462 - Initialized process group for RayActorGroup
2025-08-29 01:10:53.153 | INFO     | skyrl_train.workers.worker:_initiate_actors:464 - Mesh Ranks: [MeshRank(dp=0, sp=0, tp=0, world_size=2, dp_size=2), MeshRank(dp=1, sp=0, tp=0, world_size=2, dp_size=2)]
[36m(FSDPPolicyRayActorBase pid=44807)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(FSDPPolicyRayActorBase pid=44807)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2Model is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(FSDPPolicyRayActorBase pid=45009)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2Model is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`[32m [repeated 2x across cluster][0m
InferenceEngineClient initialized with 1 engines.
[36m(AsyncVLLMInferenceEngine pid=44685)[0m creating LLM with bundle_indices=[0, 1]
[36m(AsyncVLLMInferenceEngine pid=44685)[0m INFO 08-29 01:10:45 [__init__.py:241] Automatically detected platform cuda.
FAILED
tests/gpu/test_policy_local_engines_e2e.py::test_policy_local_engines_e2e[colocate_gloo_fsdp_vllm] 2025-08-29 01:10:59.767 | INFO     | skyrl_train.utils.utils:prepare_runtime_environment:367 - `VLLM_USE_V1` is not specified, setting `VLLM_USE_V1` to 1. To override, set `VLLM_USE_V1` explicitly
2025-08-29 01:11:01,712	WARNING utils.py:426 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.
2025-08-29 01:11:01,713	WARNING utils.py:438 -- Ray currently does not support initializing Ray with fractional cpus. Your num_cpus will be truncated from 108.8 to 108.
2025-08-29 01:11:01,923	INFO worker.py:1927 -- Started a local Ray instance.
2025-08-29 01:11:02,024	INFO packaging.py:588 -- Creating a file package for local module '/root/tgriggs/SkyRL/skyrl-train'.
2025-08-29 01:11:02,122	INFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_ae8601a03e1e2844.zip' (3.14MiB) to Ray cluster...
2025-08-29 01:11:02,142	INFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_ae8601a03e1e2844.zip'.
[33m(raylet)[0m warning: `VIRTUAL_ENV=/root/tgriggs/SkyRL/skyrl-train/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead
[33m(raylet)[0m Using CPython 3.12.11
[33m(raylet)[0m Creating virtual environment at: .venv
[33m(raylet)[0m    Building skyrl-train @ file:///tmp/ray/session_2025-08-29_01-10-59_775751_5522/runtime_resources/working_dir_files/_ray_pkg_ae8601a03e1e2844
[33m(raylet)[0m    Building skyrl-gym @ file:///tmp/ray/session_2025-08-29_01-10-59_775751_5522/runtime_resources/working_dir_files/_ray_pkg_ae8601a03e1e2844/skyrl-gym
[33m(raylet)[0m       Built skyrl-gym @ file:///tmp/ray/session_2025-08-29_01-10-59_775751_5522/runtime_resources/working_dir_files/_ray_pkg_ae8601a03e1e2844/skyrl-gym
[33m(raylet)[0m       Built skyrl-train @ file:///tmp/ray/session_2025-08-29_01-10-59_775751_5522/runtime_resources/working_dir_files/_ray_pkg_ae8601a03e1e2844
[33m(raylet)[0m Installed 192 packages in 779ms
2025-08-29 01:11:08.445 | INFO     | skyrl_train.utils.ppo_utils:sync_registries:505 - Synced registries to ray actor
[33m(raylet)[0m warning: `VIRTUAL_ENV=/root/tgriggs/SkyRL/skyrl-train/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=52651)[0m creating LLM with bundle_indices=[0, 1]
[36m(AsyncVLLMInferenceEngine pid=52651)[0m INFO 08-29 01:11:22 [__init__.py:241] Automatically detected platform cuda.
FAILED
tests/gpu/test_policy_local_engines_e2e.py::test_policy_local_engines_e2e[no_colocate_nccl_deepspeed_vllm] 2025-08-29 01:11:26.675 | INFO     | skyrl_train.utils.utils:prepare_runtime_environment:367 - `VLLM_USE_V1` is not specified, setting `VLLM_USE_V1` to 1. To override, set `VLLM_USE_V1` explicitly
2025-08-29 01:11:28,551	WARNING utils.py:426 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.
2025-08-29 01:11:28,551	WARNING utils.py:438 -- Ray currently does not support initializing Ray with fractional cpus. Your num_cpus will be truncated from 108.8 to 108.
2025-08-29 01:11:28,781	INFO worker.py:1927 -- Started a local Ray instance.
2025-08-29 01:11:28,886	INFO packaging.py:588 -- Creating a file package for local module '/root/tgriggs/SkyRL/skyrl-train'.
2025-08-29 01:11:28,983	INFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_5981cbaeb41a03ac.zip' (3.14MiB) to Ray cluster...
2025-08-29 01:11:29,000	INFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_5981cbaeb41a03ac.zip'.
[33m(raylet)[0m warning: `VIRTUAL_ENV=/root/tgriggs/SkyRL/skyrl-train/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead
[33m(raylet)[0m Using CPython 3.12.11
[33m(raylet)[0m Creating virtual environment at: .venv
[33m(raylet)[0m    Building skyrl-train @ file:///tmp/ray/session_2025-08-29_01-11-26_683779_5522/runtime_resources/working_dir_files/_ray_pkg_5981cbaeb41a03ac
[33m(raylet)[0m    Building skyrl-gym @ file:///tmp/ray/session_2025-08-29_01-11-26_683779_5522/runtime_resources/working_dir_files/_ray_pkg_5981cbaeb41a03ac/skyrl-gym
[33m(raylet)[0m       Built skyrl-gym @ file:///tmp/ray/session_2025-08-29_01-11-26_683779_5522/runtime_resources/working_dir_files/_ray_pkg_5981cbaeb41a03ac/skyrl-gym
[33m(raylet)[0m       Built skyrl-train @ file:///tmp/ray/session_2025-08-29_01-11-26_683779_5522/runtime_resources/working_dir_files/_ray_pkg_5981cbaeb41a03ac
[33m(raylet)[0m Installed 192 packages in 775ms
2025-08-29 01:11:35.366 | INFO     | skyrl_train.utils.ppo_utils:sync_registries:505 - Synced registries to ray actor
[33m(raylet)[0m warning: `VIRTUAL_ENV=/root/tgriggs/SkyRL/skyrl-train/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead[32m [repeated 2x across cluster][0m
InferenceEngineClient initialized with 1 engines.
FAILED
tests/gpu/test_policy_local_engines_e2e.py::test_policy_local_engines_e2e[colocate_nccl_deepspeed_vllm] 2025-08-29 01:11:40.000 | INFO     | skyrl_train.utils.utils:prepare_runtime_environment:367 - `VLLM_USE_V1` is not specified, setting `VLLM_USE_V1` to 1. To override, set `VLLM_USE_V1` explicitly
2025-08-29 01:11:41,920	WARNING utils.py:426 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.
2025-08-29 01:11:41,920	WARNING utils.py:438 -- Ray currently does not support initializing Ray with fractional cpus. Your num_cpus will be truncated from 108.8 to 108.
2025-08-29 01:11:42,141	INFO worker.py:1927 -- Started a local Ray instance.
2025-08-29 01:11:42,245	INFO packaging.py:588 -- Creating a file package for local module '/root/tgriggs/SkyRL/skyrl-train'.
2025-08-29 01:11:42,341	INFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_e407cccff952e435.zip' (3.17MiB) to Ray cluster...
2025-08-29 01:11:42,358	INFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_e407cccff952e435.zip'.
[33m(raylet)[0m warning: `VIRTUAL_ENV=/root/tgriggs/SkyRL/skyrl-train/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead
[33m(raylet)[0m Using CPython 3.12.11
[33m(raylet)[0m Creating virtual environment at: .venv
[33m(raylet)[0m    Building skyrl-train @ file:///tmp/ray/session_2025-08-29_01-11-40_008387_5522/runtime_resources/working_dir_files/_ray_pkg_e407cccff952e435
[33m(raylet)[0m    Building skyrl-gym @ file:///tmp/ray/session_2025-08-29_01-11-40_008387_5522/runtime_resources/working_dir_files/_ray_pkg_e407cccff952e435/skyrl-gym
[33m(raylet)[0m       Built skyrl-gym @ file:///tmp/ray/session_2025-08-29_01-11-40_008387_5522/runtime_resources/working_dir_files/_ray_pkg_e407cccff952e435/skyrl-gym
[33m(raylet)[0m       Built skyrl-train @ file:///tmp/ray/session_2025-08-29_01-11-40_008387_5522/runtime_resources/working_dir_files/_ray_pkg_e407cccff952e435
[33m(raylet)[0m Installed 192 packages in 749ms
2025-08-29 01:11:48.500 | INFO     | skyrl_train.utils.ppo_utils:sync_registries:505 - Synced registries to ray actor
[33m(raylet)[0m warning: `VIRTUAL_ENV=/root/tgriggs/SkyRL/skyrl-train/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=67754)[0m creating LLM with bundle_indices=[0, 1]
[36m(AsyncVLLMInferenceEngine pid=67754)[0m INFO 08-29 01:12:02 [__init__.py:241] Automatically detected platform cuda.
FAILED
tests/gpu/test_policy_local_engines_e2e.py::test_policy_local_engines_e2e[no_colocate_nccl_fsdp2_vllm] 2025-08-29 01:12:06.803 | INFO     | skyrl_train.utils.utils:prepare_runtime_environment:367 - `VLLM_USE_V1` is not specified, setting `VLLM_USE_V1` to 1. To override, set `VLLM_USE_V1` explicitly
2025-08-29 01:12:08,745	WARNING utils.py:426 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.
2025-08-29 01:12:08,745	WARNING utils.py:438 -- Ray currently does not support initializing Ray with fractional cpus. Your num_cpus will be truncated from 108.8 to 108.
2025-08-29 01:12:08,973	INFO worker.py:1927 -- Started a local Ray instance.
2025-08-29 01:12:09,079	INFO packaging.py:588 -- Creating a file package for local module '/root/tgriggs/SkyRL/skyrl-train'.
2025-08-29 01:12:09,176	INFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_1b68f74e2cefa159.zip' (3.17MiB) to Ray cluster...
2025-08-29 01:12:09,196	INFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_1b68f74e2cefa159.zip'.
[33m(raylet)[0m warning: `VIRTUAL_ENV=/root/tgriggs/SkyRL/skyrl-train/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead
[33m(raylet)[0m Using CPython 3.12.11
[33m(raylet)[0m Creating virtual environment at: .venv
[33m(raylet)[0m    Building skyrl-train @ file:///tmp/ray/session_2025-08-29_01-12-06_811922_5522/runtime_resources/working_dir_files/_ray_pkg_1b68f74e2cefa159
[33m(raylet)[0m    Building skyrl-gym @ file:///tmp/ray/session_2025-08-29_01-12-06_811922_5522/runtime_resources/working_dir_files/_ray_pkg_1b68f74e2cefa159/skyrl-gym
[33m(raylet)[0m       Built skyrl-gym @ file:///tmp/ray/session_2025-08-29_01-12-06_811922_5522/runtime_resources/working_dir_files/_ray_pkg_1b68f74e2cefa159/skyrl-gym
[33m(raylet)[0m       Built skyrl-train @ file:///tmp/ray/session_2025-08-29_01-12-06_811922_5522/runtime_resources/working_dir_files/_ray_pkg_1b68f74e2cefa159
[33m(raylet)[0m Installed 192 packages in 767ms
2025-08-29 01:12:15.417 | INFO     | skyrl_train.utils.ppo_utils:sync_registries:505 - Synced registries to ray actor
[33m(raylet)[0m warning: `VIRTUAL_ENV=/root/tgriggs/SkyRL/skyrl-train/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead[32m [repeated 2x across cluster][0m
[36m(pid=75664)[0m Using blocking ray.get inside async actor. This blocks the event loop. Please use `await` on object ref with asyncio.gather if you want to yield execution to the event loop instead.
[33m(raylet)[0m warning: `VIRTUAL_ENV=/root/tgriggs/SkyRL/skyrl-train/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead[32m [repeated 2x across cluster][0m
2025-08-29 01:12:30.054 | INFO     | skyrl_train.workers.worker:_initiate_actors:460 - Initializing process group for RayActorGroup
[36m(AsyncVLLMInferenceEngine pid=75574)[0m Exception raised in creation task: The actor died because of an error raised in its creation task, [36mray::AsyncVLLMInferenceEngine.__init__()[39m (pid=75574, ip=172.18.0.2, actor_id=d8e1de49cd1b9de627e447f101000000, repr=<skyrl_train.inference_engines.vllm.vllm_engine.AsyncVLLMInferenceEngine object at 0x719017e96570>)
[36m(AsyncVLLMInferenceEngine pid=75574)[0m   File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 456, in result
[36m(AsyncVLLMInferenceEngine pid=75574)[0m     return self.__get_result()
[36m(AsyncVLLMInferenceEngine pid=75574)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=75574)[0m   File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
[36m(AsyncVLLMInferenceEngine pid=75574)[0m     raise self._exception
[36m(AsyncVLLMInferenceEngine pid=75574)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=75574)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=75574)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=75574)[0m   File "/tmp/ray/session_2025-08-29_01-12-06_811922_5522/runtime_resources/working_dir_files/_ray_pkg_1b68f74e2cefa159/skyrl_train/inference_engines/vllm/vllm_engine.py", line 168, in __init__
[36m(AsyncVLLMInferenceEngine pid=75574)[0m     self.llm = self._create_engine(*args, **kwargs)
[36m(AsyncVLLMInferenceEngine pid=75574)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=75574)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=75574)[0m   File "/tmp/ray/session_2025-08-29_01-12-06_811922_5522/runtime_resources/working_dir_files/_ray_pkg_1b68f74e2cefa159/skyrl_train/inference_engines/vllm/vllm_engine.py", line 317, in _create_engine
[36m(AsyncVLLMInferenceEngine pid=75574)[0m     engine_args = vllm.AsyncEngineArgs(disable_log_requests=True, **kwargs)
[36m(AsyncVLLMInferenceEngine pid=75574)[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=75574)[0m TypeError: AsyncEngineArgs.__init__() got an unexpected keyword argument 'disable_log_requests'
[36m(pid=75898)[0m Using blocking ray.get inside async actor. This blocks the event loop. Please use `await` on object ref with asyncio.gather if you want to yield execution to the event loop instead.
[33m(raylet)[0m warning: `VIRTUAL_ENV=/root/tgriggs/SkyRL/skyrl-train/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead
2025-08-29 01:12:36.541 | INFO     | skyrl_train.workers.worker:_initiate_actors:462 - Initialized process group for RayActorGroup
2025-08-29 01:12:36.546 | INFO     | skyrl_train.workers.worker:_initiate_actors:464 - Mesh Ranks: [MeshRank(dp=0, sp=0, tp=0, world_size=2, dp_size=2), MeshRank(dp=1, sp=0, tp=0, world_size=2, dp_size=2)]
[36m(FSDPPolicyRayActorBase pid=75664)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(FSDPPolicyRayActorBase pid=75664)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2Model is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(FSDPPolicyRayActorBase pid=75898)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2Model is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`[32m [repeated 2x across cluster][0m
InferenceEngineClient initialized with 1 engines.
[36m(AsyncVLLMInferenceEngine pid=75574)[0m creating LLM with bundle_indices=[0, 1]
[36m(AsyncVLLMInferenceEngine pid=75574)[0m INFO 08-29 01:12:28 [__init__.py:241] Automatically detected platform cuda.
FAILED
tests/gpu/test_policy_local_engines_e2e.py::test_policy_local_engines_e2e[colocate_nccl_fsdp2_vllm] 2025-08-29 01:12:45.422 | INFO     | skyrl_train.utils.utils:prepare_runtime_environment:367 - `VLLM_USE_V1` is not specified, setting `VLLM_USE_V1` to 1. To override, set `VLLM_USE_V1` explicitly
2025-08-29 01:12:47,325	WARNING utils.py:426 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.
2025-08-29 01:12:47,326	WARNING utils.py:438 -- Ray currently does not support initializing Ray with fractional cpus. Your num_cpus will be truncated from 108.8 to 108.
2025-08-29 01:12:47,540	INFO worker.py:1927 -- Started a local Ray instance.
2025-08-29 01:12:47,640	INFO packaging.py:588 -- Creating a file package for local module '/root/tgriggs/SkyRL/skyrl-train'.
2025-08-29 01:12:47,736	INFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_a1deeb22f7f0b8f8.zip' (3.18MiB) to Ray cluster...
2025-08-29 01:12:47,753	INFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_a1deeb22f7f0b8f8.zip'.
[33m(raylet)[0m warning: `VIRTUAL_ENV=/root/tgriggs/SkyRL/skyrl-train/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead
[33m(raylet)[0m Using CPython 3.12.11
[33m(raylet)[0m Creating virtual environment at: .venv
[33m(raylet)[0m    Building skyrl-train @ file:///tmp/ray/session_2025-08-29_01-12-45_431675_5522/runtime_resources/working_dir_files/_ray_pkg_a1deeb22f7f0b8f8
[33m(raylet)[0m    Building skyrl-gym @ file:///tmp/ray/session_2025-08-29_01-12-45_431675_5522/runtime_resources/working_dir_files/_ray_pkg_a1deeb22f7f0b8f8/skyrl-gym
[33m(raylet)[0m       Built skyrl-gym @ file:///tmp/ray/session_2025-08-29_01-12-45_431675_5522/runtime_resources/working_dir_files/_ray_pkg_a1deeb22f7f0b8f8/skyrl-gym
[33m(raylet)[0m       Built skyrl-train @ file:///tmp/ray/session_2025-08-29_01-12-45_431675_5522/runtime_resources/working_dir_files/_ray_pkg_a1deeb22f7f0b8f8
[33m(raylet)[0m Installed 192 packages in 770ms
2025-08-29 01:12:54.136 | INFO     | skyrl_train.utils.ppo_utils:sync_registries:505 - Synced registries to ray actor
[33m(raylet)[0m warning: `VIRTUAL_ENV=/root/tgriggs/SkyRL/skyrl-train/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead[32m [repeated 2x across cluster][0m
[33m(raylet)[0m warning: `VIRTUAL_ENV=/root/tgriggs/SkyRL/skyrl-train/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead
[36m(AsyncVLLMInferenceEngine pid=83535)[0m creating LLM with bundle_indices=[0, 1]
[36m(AsyncVLLMInferenceEngine pid=83535)[0m INFO 08-29 01:13:07 [__init__.py:241] Automatically detected platform cuda.
FAILED

=================================== FAILURES ===================================
___________________ test_inference_engines_generation[vllm] ____________________

self = <urllib3.connection.HTTPConnection object at 0x7edbc99f88f0>

    def _new_conn(self) -> socket.socket:
        """Establish a socket connection and set nodelay settings on it.
    
        :return: New socket connection.
        """
        try:
>           sock = connection.create_connection(
                (self._dns_host, self.port),
                self.timeout,
                source_address=self.source_address,
                socket_options=self.socket_options,
            )

.venv/lib/python3.12/site-packages/urllib3/connection.py:198: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
.venv/lib/python3.12/site-packages/urllib3/util/connection.py:85: in create_connection
    raise err
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

address = ('localhost', 55247), timeout = None, source_address = None
socket_options = [(6, 1, 1)]

    def create_connection(
        address: tuple[str, int],
        timeout: _TYPE_TIMEOUT = _DEFAULT_TIMEOUT,
        source_address: tuple[str, int] | None = None,
        socket_options: _TYPE_SOCKET_OPTIONS | None = None,
    ) -> socket.socket:
        """Connect to *address* and return the socket object.
    
        Convenience function.  Connect to *address* (a 2-tuple ``(host,
        port)``) and return the socket object.  Passing the optional
        *timeout* parameter will set the timeout on the socket instance
        before attempting to connect.  If no *timeout* is supplied, the
        global default timeout setting returned by :func:`socket.getdefaulttimeout`
        is used.  If *source_address* is set it must be a tuple of (host, port)
        for the socket to bind as a source address before making the connection.
        An host of '' or port 0 tells the OS to use the default.
        """
    
        host, port = address
        if host.startswith("["):
            host = host.strip("[]")
        err = None
    
        # Using the value from allowed_gai_family() in the context of getaddrinfo lets
        # us select whether to work with IPv4 DNS records, IPv6 records, or both.
        # The original create_connection function always returns all records.
        family = allowed_gai_family()
    
        try:
            host.encode("idna")
        except UnicodeError:
            raise LocationParseError(f"'{host}', label empty or too long") from None
    
        for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
            af, socktype, proto, canonname, sa = res
            sock = None
            try:
                sock = socket.socket(af, socktype, proto)
    
                # If provided, set socket level options before connecting.
                _set_socket_options(sock, socket_options)
    
                if timeout is not _DEFAULT_TIMEOUT:
                    sock.settimeout(timeout)
                if source_address:
                    sock.bind(source_address)
>               sock.connect(sa)
E               ConnectionRefusedError: [Errno 111] Connection refused

.venv/lib/python3.12/site-packages/urllib3/util/connection.py:73: ConnectionRefusedError

The above exception was the direct cause of the following exception:

self = <urllib3.connectionpool.HTTPConnectionPool object at 0x7edbc99f9670>
method = 'GET', url = '/health', body = None
headers = {'User-Agent': 'python-requests/2.32.4', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}
retries = Retry(total=0, connect=None, read=False, redirect=None, status=None)
redirect = False, assert_same_host = False
timeout = Timeout(connect=None, read=None, total=None), pool_timeout = None
release_conn = False, chunked = False, body_pos = None, preload_content = False
decode_content = False, response_kw = {}
parsed_url = Url(scheme=None, auth=None, host=None, port=None, path='/health', query=None, fragment=None)
destination_scheme = None, conn = None, release_this_conn = True
http_tunnel_required = False, err = None, clean_exit = False

    def urlopen(  # type: ignore[override]
        self,
        method: str,
        url: str,
        body: _TYPE_BODY | None = None,
        headers: typing.Mapping[str, str] | None = None,
        retries: Retry | bool | int | None = None,
        redirect: bool = True,
        assert_same_host: bool = True,
        timeout: _TYPE_TIMEOUT = _DEFAULT_TIMEOUT,
        pool_timeout: int | None = None,
        release_conn: bool | None = None,
        chunked: bool = False,
        body_pos: _TYPE_BODY_POSITION | None = None,
        preload_content: bool = True,
        decode_content: bool = True,
        **response_kw: typing.Any,
    ) -> BaseHTTPResponse:
        """
        Get a connection from the pool and perform an HTTP request. This is the
        lowest level call for making a request, so you'll need to specify all
        the raw details.
    
        .. note::
    
           More commonly, it's appropriate to use a convenience method
           such as :meth:`request`.
    
        .. note::
    
           `release_conn` will only behave as expected if
           `preload_content=False` because we want to make
           `preload_content=False` the default behaviour someday soon without
           breaking backwards compatibility.
    
        :param method:
            HTTP request method (such as GET, POST, PUT, etc.)
    
        :param url:
            The URL to perform the request on.
    
        :param body:
            Data to send in the request body, either :class:`str`, :class:`bytes`,
            an iterable of :class:`str`/:class:`bytes`, or a file-like object.
    
        :param headers:
            Dictionary of custom headers to send, such as User-Agent,
            If-None-Match, etc. If None, pool headers are used. If provided,
            these headers completely replace any pool-specific headers.
    
        :param retries:
            Configure the number of retries to allow before raising a
            :class:`~urllib3.exceptions.MaxRetryError` exception.
    
            If ``None`` (default) will retry 3 times, see ``Retry.DEFAULT``. Pass a
            :class:`~urllib3.util.retry.Retry` object for fine-grained control
            over different types of retries.
            Pass an integer number to retry connection errors that many times,
            but no other types of errors. Pass zero to never retry.
    
            If ``False``, then retries are disabled and any exception is raised
            immediately. Also, instead of raising a MaxRetryError on redirects,
            the redirect response will be returned.
    
        :type retries: :class:`~urllib3.util.retry.Retry`, False, or an int.
    
        :param redirect:
            If True, automatically handle redirects (status codes 301, 302,
            303, 307, 308). Each redirect counts as a retry. Disabling retries
            will disable redirect, too.
    
        :param assert_same_host:
            If ``True``, will make sure that the host of the pool requests is
            consistent else will raise HostChangedError. When ``False``, you can
            use the pool on an HTTP proxy and request foreign hosts.
    
        :param timeout:
            If specified, overrides the default timeout for this one
            request. It may be a float (in seconds) or an instance of
            :class:`urllib3.util.Timeout`.
    
        :param pool_timeout:
            If set and the pool is set to block=True, then this method will
            block for ``pool_timeout`` seconds and raise EmptyPoolError if no
            connection is available within the time period.
    
        :param bool preload_content:
            If True, the response's body will be preloaded into memory.
    
        :param bool decode_content:
            If True, will attempt to decode the body based on the
            'content-encoding' header.
    
        :param release_conn:
            If False, then the urlopen call will not release the connection
            back into the pool once a response is received (but will release if
            you read the entire contents of the response such as when
            `preload_content=True`). This is useful if you're not preloading
            the response's content immediately. You will need to call
            ``r.release_conn()`` on the response ``r`` to return the connection
            back into the pool. If None, it takes the value of ``preload_content``
            which defaults to ``True``.
    
        :param bool chunked:
            If True, urllib3 will send the body using chunked transfer
            encoding. Otherwise, urllib3 will send the body using the standard
            content-length form. Defaults to False.
    
        :param int body_pos:
            Position to seek to in file-like body in the event of a retry or
            redirect. Typically this won't need to be set because urllib3 will
            auto-populate the value when needed.
        """
        parsed_url = parse_url(url)
        destination_scheme = parsed_url.scheme
    
        if headers is None:
            headers = self.headers
    
        if not isinstance(retries, Retry):
            retries = Retry.from_int(retries, redirect=redirect, default=self.retries)
    
        if release_conn is None:
            release_conn = preload_content
    
        # Check host
        if assert_same_host and not self.is_same_host(url):
            raise HostChangedError(self, url, retries)
    
        # Ensure that the URL we're connecting to is properly encoded
        if url.startswith("/"):
            url = to_str(_encode_target(url))
        else:
            url = to_str(parsed_url.url)
    
        conn = None
    
        # Track whether `conn` needs to be released before
        # returning/raising/recursing. Update this variable if necessary, and
        # leave `release_conn` constant throughout the function. That way, if
        # the function recurses, the original value of `release_conn` will be
        # passed down into the recursive call, and its value will be respected.
        #
        # See issue #651 [1] for details.
        #
        # [1] <https://github.com/urllib3/urllib3/issues/651>
        release_this_conn = release_conn
    
        http_tunnel_required = connection_requires_http_tunnel(
            self.proxy, self.proxy_config, destination_scheme
        )
    
        # Merge the proxy headers. Only done when not using HTTP CONNECT. We
        # have to copy the headers dict so we can safely change it without those
        # changes being reflected in anyone else's copy.
        if not http_tunnel_required:
            headers = headers.copy()  # type: ignore[attr-defined]
            headers.update(self.proxy_headers)  # type: ignore[union-attr]
    
        # Must keep the exception bound to a separate variable or else Python 3
        # complains about UnboundLocalError.
        err = None
    
        # Keep track of whether we cleanly exited the except block. This
        # ensures we do proper cleanup in finally.
        clean_exit = False
    
        # Rewind body position, if needed. Record current position
        # for future rewinds in the event of a redirect/retry.
        body_pos = set_file_position(body, body_pos)
    
        try:
            # Request a connection from the queue.
            timeout_obj = self._get_timeout(timeout)
            conn = self._get_conn(timeout=pool_timeout)
    
            conn.timeout = timeout_obj.connect_timeout  # type: ignore[assignment]
    
            # Is this a closed/new connection that requires CONNECT tunnelling?
            if self.proxy is not None and http_tunnel_required and conn.is_closed:
                try:
                    self._prepare_proxy(conn)
                except (BaseSSLError, OSError, SocketTimeout) as e:
                    self._raise_timeout(
                        err=e, url=self.proxy.url, timeout_value=conn.timeout
                    )
                    raise
    
            # If we're going to release the connection in ``finally:``, then
            # the response doesn't need to know about the connection. Otherwise
            # it will also try to release it and we'll have a double-release
            # mess.
            response_conn = conn if not release_conn else None
    
            # Make the request on the HTTPConnection object
>           response = self._make_request(
                conn,
                method,
                url,
                timeout=timeout_obj,
                body=body,
                headers=headers,
                chunked=chunked,
                retries=retries,
                response_conn=response_conn,
                preload_content=preload_content,
                decode_content=decode_content,
                **response_kw,
            )

.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:787: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:493: in _make_request
    conn.request(
.venv/lib/python3.12/site-packages/urllib3/connection.py:494: in request
    self.endheaders()
/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/http/client.py:1333: in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/http/client.py:1093: in _send_output
    self.send(msg)
/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/http/client.py:1037: in send
    self.connect()
.venv/lib/python3.12/site-packages/urllib3/connection.py:325: in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <urllib3.connection.HTTPConnection object at 0x7edbc99f88f0>

    def _new_conn(self) -> socket.socket:
        """Establish a socket connection and set nodelay settings on it.
    
        :return: New socket connection.
        """
        try:
            sock = connection.create_connection(
                (self._dns_host, self.port),
                self.timeout,
                source_address=self.source_address,
                socket_options=self.socket_options,
            )
        except socket.gaierror as e:
            raise NameResolutionError(self.host, self, e) from e
        except SocketTimeout as e:
            raise ConnectTimeoutError(
                self,
                f"Connection to {self.host} timed out. (connect timeout={self.timeout})",
            ) from e
    
        except OSError as e:
>           raise NewConnectionError(
                self, f"Failed to establish a new connection: {e}"
            ) from e
E           urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7edbc99f88f0>: Failed to establish a new connection: [Errno 111] Connection refused

.venv/lib/python3.12/site-packages/urllib3/connection.py:213: NewConnectionError

The above exception was the direct cause of the following exception:

self = <requests.adapters.HTTPAdapter object at 0x7edbc99f8710>
request = <PreparedRequest [GET]>, stream = False
timeout = Timeout(connect=None, read=None, total=None), verify = True
cert = None, proxies = OrderedDict()

    def send(
        self, request, stream=False, timeout=None, verify=True, cert=None, proxies=None
    ):
        """Sends PreparedRequest object. Returns Response object.
    
        :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.
        :param stream: (optional) Whether to stream the request content.
        :param timeout: (optional) How long to wait for the server to send
            data before giving up, as a float, or a :ref:`(connect timeout,
            read timeout) <timeouts>` tuple.
        :type timeout: float or tuple or urllib3 Timeout object
        :param verify: (optional) Either a boolean, in which case it controls whether
            we verify the server's TLS certificate, or a string, in which case it
            must be a path to a CA bundle to use
        :param cert: (optional) Any user-provided SSL certificate to be trusted.
        :param proxies: (optional) The proxies dictionary to apply to the request.
        :rtype: requests.Response
        """
    
        try:
            conn = self.get_connection_with_tls_context(
                request, verify, proxies=proxies, cert=cert
            )
        except LocationValueError as e:
            raise InvalidURL(e, request=request)
    
        self.cert_verify(conn, request.url, verify, cert)
        url = self.request_url(request, proxies)
        self.add_headers(
            request,
            stream=stream,
            timeout=timeout,
            verify=verify,
            cert=cert,
            proxies=proxies,
        )
    
        chunked = not (request.body is None or "Content-Length" in request.headers)
    
        if isinstance(timeout, tuple):
            try:
                connect, read = timeout
                timeout = TimeoutSauce(connect=connect, read=read)
            except ValueError:
                raise ValueError(
                    f"Invalid timeout {timeout}. Pass a (connect, read) timeout tuple, "
                    f"or a single float to set both timeouts to the same value."
                )
        elif isinstance(timeout, TimeoutSauce):
            pass
        else:
            timeout = TimeoutSauce(connect=timeout, read=timeout)
    
        try:
>           resp = conn.urlopen(
                method=request.method,
                url=url,
                body=request.body,
                headers=request.headers,
                redirect=False,
                assert_same_host=False,
                preload_content=False,
                decode_content=False,
                retries=self.max_retries,
                timeout=timeout,
                chunked=chunked,
            )

.venv/lib/python3.12/site-packages/requests/adapters.py:667: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:841: in urlopen
    retries = retries.increment(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Retry(total=0, connect=None, read=False, redirect=None, status=None)
method = 'GET', url = '/health', response = None
error = NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7edbc99f88f0>: Failed to establish a new connection: [Errno 111] Connection refused')
_pool = <urllib3.connectionpool.HTTPConnectionPool object at 0x7edbc99f9670>
_stacktrace = <traceback object at 0x7edbc986fc40>

    def increment(
        self,
        method: str | None = None,
        url: str | None = None,
        response: BaseHTTPResponse | None = None,
        error: Exception | None = None,
        _pool: ConnectionPool | None = None,
        _stacktrace: TracebackType | None = None,
    ) -> Self:
        """Return a new Retry object with incremented retry counters.
    
        :param response: A response object, or None, if the server did not
            return a response.
        :type response: :class:`~urllib3.response.BaseHTTPResponse`
        :param Exception error: An error encountered during the request, or
            None if the response was received successfully.
    
        :return: A new ``Retry`` object.
        """
        if self.total is False and error:
            # Disabled, indicate to re-raise the error.
            raise reraise(type(error), error, _stacktrace)
    
        total = self.total
        if total is not None:
            total -= 1
    
        connect = self.connect
        read = self.read
        redirect = self.redirect
        status_count = self.status
        other = self.other
        cause = "unknown"
        status = None
        redirect_location = None
    
        if error and self._is_connection_error(error):
            # Connect retry?
            if connect is False:
                raise reraise(type(error), error, _stacktrace)
            elif connect is not None:
                connect -= 1
    
        elif error and self._is_read_error(error):
            # Read retry?
            if read is False or method is None or not self._is_method_retryable(method):
                raise reraise(type(error), error, _stacktrace)
            elif read is not None:
                read -= 1
    
        elif error:
            # Other retry?
            if other is not None:
                other -= 1
    
        elif response and response.get_redirect_location():
            # Redirect retry?
            if redirect is not None:
                redirect -= 1
            cause = "too many redirects"
            response_redirect_location = response.get_redirect_location()
            if response_redirect_location:
                redirect_location = response_redirect_location
            status = response.status
    
        else:
            # Incrementing because of a server error like a 500 in
            # status_forcelist and the given method is in the allowed_methods
            cause = ResponseError.GENERIC_ERROR
            if response and response.status:
                if status_count is not None:
                    status_count -= 1
                cause = ResponseError.SPECIFIC_ERROR.format(status_code=response.status)
                status = response.status
    
        history = self.history + (
            RequestHistory(method, url, error, status, redirect_location),
        )
    
        new_retry = self.new(
            total=total,
            connect=connect,
            read=read,
            redirect=redirect,
            status=status_count,
            other=other,
            history=history,
        )
    
        if new_retry.is_exhausted():
            reason = error or ResponseError(cause)
>           raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=55247): Max retries exceeded with url: /health (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7edbc99f88f0>: Failed to establish a new connection: [Errno 111] Connection refused'))

.venv/lib/python3.12/site-packages/urllib3/util/retry.py:519: MaxRetryError

During handling of the above exception, another exception occurred:

url = 'localhost:55247', health_path = 'health', timeout = 60, interval = 1.0

    def wait_for_server(url: str, health_path: str, timeout: int = 60, interval: float = 1.0):
        start_time = time.time()
        while True:
            try:
>               response = requests.get(f"http://{url}/{health_path}")
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/gpu/utils.py:203: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
.venv/lib/python3.12/site-packages/requests/api.py:73: in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.12/site-packages/requests/api.py:59: in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.12/site-packages/requests/sessions.py:589: in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.12/site-packages/requests/sessions.py:703: in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <requests.adapters.HTTPAdapter object at 0x7edbc99f8710>
request = <PreparedRequest [GET]>, stream = False
timeout = Timeout(connect=None, read=None, total=None), verify = True
cert = None, proxies = OrderedDict()

    def send(
        self, request, stream=False, timeout=None, verify=True, cert=None, proxies=None
    ):
        """Sends PreparedRequest object. Returns Response object.
    
        :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.
        :param stream: (optional) Whether to stream the request content.
        :param timeout: (optional) How long to wait for the server to send
            data before giving up, as a float, or a :ref:`(connect timeout,
            read timeout) <timeouts>` tuple.
        :type timeout: float or tuple or urllib3 Timeout object
        :param verify: (optional) Either a boolean, in which case it controls whether
            we verify the server's TLS certificate, or a string, in which case it
            must be a path to a CA bundle to use
        :param cert: (optional) Any user-provided SSL certificate to be trusted.
        :param proxies: (optional) The proxies dictionary to apply to the request.
        :rtype: requests.Response
        """
    
        try:
            conn = self.get_connection_with_tls_context(
                request, verify, proxies=proxies, cert=cert
            )
        except LocationValueError as e:
            raise InvalidURL(e, request=request)
    
        self.cert_verify(conn, request.url, verify, cert)
        url = self.request_url(request, proxies)
        self.add_headers(
            request,
            stream=stream,
            timeout=timeout,
            verify=verify,
            cert=cert,
            proxies=proxies,
        )
    
        chunked = not (request.body is None or "Content-Length" in request.headers)
    
        if isinstance(timeout, tuple):
            try:
                connect, read = timeout
                timeout = TimeoutSauce(connect=connect, read=read)
            except ValueError:
                raise ValueError(
                    f"Invalid timeout {timeout}. Pass a (connect, read) timeout tuple, "
                    f"or a single float to set both timeouts to the same value."
                )
        elif isinstance(timeout, TimeoutSauce):
            pass
        else:
            timeout = TimeoutSauce(connect=timeout, read=timeout)
    
        try:
            resp = conn.urlopen(
                method=request.method,
                url=url,
                body=request.body,
                headers=request.headers,
                redirect=False,
                assert_same_host=False,
                preload_content=False,
                decode_content=False,
                retries=self.max_retries,
                timeout=timeout,
                chunked=chunked,
            )
    
        except (ProtocolError, OSError) as err:
            raise ConnectionError(err, request=request)
    
        except MaxRetryError as e:
            if isinstance(e.reason, ConnectTimeoutError):
                # TODO: Remove this in 3.0.0: see #2811
                if not isinstance(e.reason, NewConnectionError):
                    raise ConnectTimeout(e, request=request)
    
            if isinstance(e.reason, ResponseError):
                raise RetryError(e, request=request)
    
            if isinstance(e.reason, _ProxyError):
                raise ProxyError(e, request=request)
    
            if isinstance(e.reason, _SSLError):
                # This branch is for urllib3 v1.22 and later.
                raise SSLError(e, request=request)
    
>           raise ConnectionError(e, request=request)
E           requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=55247): Max retries exceeded with url: /health (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7edbc99f88f0>: Failed to establish a new connection: [Errno 111] Connection refused'))

.venv/lib/python3.12/site-packages/requests/adapters.py:700: ConnectionError

During handling of the above exception, another exception occurred:

backend = 'vllm', tp_size = 2

    @pytest.mark.parametrize(
        "backend,tp_size",
        [
            pytest.param("vllm", 2, marks=pytest.mark.vllm),
            # TODO(Charlie): add TP > 1 tests for sglang when we support it
            pytest.param("sglang", 1, marks=pytest.mark.sglang),
        ],
        ids=["vllm", "sglang"],
    )
    def test_inference_engines_generation(backend: str, tp_size: int):
        """
        Tests generation with both remote and ray-wrapped engines for the specified backend.
        """
        try:
            cfg = get_test_actor_config()
            cfg.generator.backend = backend
            initialize_ray(cfg)
    
            prompts = get_test_prompts(MODEL)
            tokenizer = AutoTokenizer.from_pretrained(MODEL)
    
            try:
>               llm_client, remote_server_process = init_remote_inference_servers(tp_size, backend, tokenizer)
                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/gpu/gpu_ci/test_engine_generation.py:263: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/gpu/gpu_ci/test_engine_generation.py:127: in init_remote_inference_servers
    wait_for_server(url=f"localhost:{engine_port}", health_path="health")
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

url = 'localhost:55247', health_path = 'health', timeout = 60, interval = 1.0

    def wait_for_server(url: str, health_path: str, timeout: int = 60, interval: float = 1.0):
        start_time = time.time()
        while True:
            try:
                response = requests.get(f"http://{url}/{health_path}")
                if response.ok:
                    return
            except requests.exceptions.ConnectionError:
                if time.time() - start_time > timeout:
>                   raise TimeoutError(f"Server at {url} did not come online within {timeout} seconds")
E                   TimeoutError: Server at localhost:55247 did not come online within 60 seconds

tests/gpu/utils.py:208: TimeoutError

During handling of the above exception, another exception occurred:

backend = 'vllm', tp_size = 2

    @pytest.mark.parametrize(
        "backend,tp_size",
        [
            pytest.param("vllm", 2, marks=pytest.mark.vllm),
            # TODO(Charlie): add TP > 1 tests for sglang when we support it
            pytest.param("sglang", 1, marks=pytest.mark.sglang),
        ],
        ids=["vllm", "sglang"],
    )
    def test_inference_engines_generation(backend: str, tp_size: int):
        """
        Tests generation with both remote and ray-wrapped engines for the specified backend.
        """
        try:
            cfg = get_test_actor_config()
            cfg.generator.backend = backend
            initialize_ray(cfg)
    
            prompts = get_test_prompts(MODEL)
            tokenizer = AutoTokenizer.from_pretrained(MODEL)
    
            try:
                llm_client, remote_server_process = init_remote_inference_servers(tp_size, backend, tokenizer)
    
                # Batched generation
                remote_batch_responses, batch_finish_reasons = asyncio.run(run_batch_generation(llm_client, prompts))
                assert len(remote_batch_responses) == len(
                    prompts
                ), f"Number of responses should match number of prompts, got {len(remote_batch_responses)} responses but {len(prompts)} prompts"
                assert len(batch_finish_reasons) == len(
                    prompts
                ), f"Number of finish reasons should match number of prompts, got {len(batch_finish_reasons)} finish reasons but {len(prompts)} prompts"
    
                # Single generation (ie, submit individual requests)
                remote_single_responses, single_finish_reasons = asyncio.run(run_single_generation(llm_client, prompts))
                assert len(remote_single_responses) == len(
                    prompts
                ), f"Number of responses should match number of prompts, got {len(remote_single_responses)} responses but {len(prompts)} prompts"
                assert len(single_finish_reasons) == len(
                    prompts
                ), f"Number of finish reasons should match number of prompts, got {len(single_finish_reasons)} finish reasons but {len(prompts)} prompts"
    
                # Ensure batched and single generation outputs are (roughly) the same
                for i in range(len(prompts)):
                    if not are_responses_similar(remote_batch_responses[i], remote_single_responses[i], tolerance=0.01):
                        print(
                            f"Remote batch and single generation responses are not similar, got batch={remote_batch_responses[i]} and single={remote_single_responses[i]}"
                        )
    
            finally:
>               remote_server_process.terminate()
                ^^^^^^^^^^^^^^^^^^^^^
E               UnboundLocalError: cannot access local variable 'remote_server_process' where it is not associated with a value

tests/gpu/gpu_ci/test_engine_generation.py:291: UnboundLocalError
______________________ test_token_based_generation[vllm] _______________________

backend = 'vllm', tp_size = 2

    @pytest.mark.parametrize(
        "backend,tp_size",
        [
            pytest.param("vllm", 2, marks=pytest.mark.vllm),
            # TODO(Charlie): add TP > 1 tests for sglang when we support it
            pytest.param("sglang", 1, marks=pytest.mark.sglang),
        ],
        ids=["vllm", "sglang"],
    )
    def test_token_based_generation(backend: str, tp_size: int):
        """Test generation using prompt_token_ids for the specified backend."""
    
        try:
            cfg = get_test_actor_config()
            cfg.generator.backend = backend
            initialize_ray(cfg)
    
            prompts = get_test_prompts(MODEL, 3)
            tokenizer = AutoTokenizer.from_pretrained(MODEL)
            prompt_token_ids = tokenizer.apply_chat_template(
                prompts, add_generation_prompt=True, tokenize=True, return_dict=True
            )["input_ids"]
    
>           llm_client = init_ray_inference_engines(backend, tp_size)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/gpu/gpu_ci/test_engine_generation.py:356: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/gpu/gpu_ci/test_engine_generation.py:173: in init_ray_inference_engines
    sampling_params=get_sampling_params_for_backend(
skyrl_train/inference_engines/utils.py:54: in get_sampling_params_for_backend
    return get_vllm_sampling_params(sampling_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
skyrl_train/inference_engines/utils.py:16: in get_vllm_sampling_params
    "stop": list(sampling_params.stop) if sampling_params.stop is not None else None,
                                          ^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.12/site-packages/omegaconf/dictconfig.py:355: in __getattr__
    self._format_and_raise(
.venv/lib/python3.12/site-packages/omegaconf/base.py:231: in _format_and_raise
    format_and_raise(
.venv/lib/python3.12/site-packages/omegaconf/_utils.py:899: in format_and_raise
    _raise(ex, cause)
.venv/lib/python3.12/site-packages/omegaconf/_utils.py:797: in _raise
    raise ex.with_traceback(sys.exc_info()[2])  # set env var OC_CAUSE=1 for full trace
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.12/site-packages/omegaconf/dictconfig.py:351: in __getattr__
    return self._get_impl(
.venv/lib/python3.12/site-packages/omegaconf/dictconfig.py:442: in _get_impl
    node = self._get_child(
.venv/lib/python3.12/site-packages/omegaconf/basecontainer.py:73: in _get_child
    child = self._get_node(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = {'temperature': 0.0, 'top_p': 1, 'top_k': -1, 'max_generate_length': 1024, 'min_p': 0.0, 'logprobs': None}
key = 'stop', validate_access = True, validate_key = False
throw_on_missing_value = False, throw_on_missing_key = True

    def _get_node(
        self,
        key: DictKeyType,
        validate_access: bool = True,
        validate_key: bool = True,
        throw_on_missing_value: bool = False,
        throw_on_missing_key: bool = False,
    ) -> Optional[Node]:
        try:
            key = self._validate_and_normalize_key(key)
        except KeyValidationError:
            if validate_access and validate_key:
                raise
            else:
                if throw_on_missing_key:
                    raise ConfigAttributeError
                else:
                    return None
    
        if validate_access:
            self._validate_get(key)
    
        value: Optional[Node] = self.__dict__["_content"].get(key)
        if value is None:
            if throw_on_missing_key:
>               raise ConfigKeyError(f"Missing key {key!s}")
E               omegaconf.errors.ConfigAttributeError: Missing key stop
E                   full_key: stop
E                   object_type=dict

.venv/lib/python3.12/site-packages/omegaconf/dictconfig.py:480: ConfigAttributeError
__________ test_policy_local_engines_e2e[no_colocate_nccl_fsdp_vllm] ___________

colocate_all = False, weight_sync_backend = 'nccl', strategy = 'fsdp'
backend = 'vllm', tp_size = 2

    @pytest.mark.parametrize(
        ("colocate_all", "weight_sync_backend", "strategy", "backend", "tp_size"),
        [
            pytest.param(False, "nccl", "fsdp", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(True, "nccl", "fsdp", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(False, "gloo", "fsdp", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(True, "gloo", "fsdp", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(False, "nccl", "deepspeed", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(True, "nccl", "deepspeed", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(False, "nccl", "fsdp2", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(True, "nccl", "fsdp2", "vllm", 2, marks=pytest.mark.vllm),
            # TODO(Charlie): add TP > 1 tests for sglang when we support it
            pytest.param(False, "nccl", "deepspeed", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(True, "nccl", "deepspeed", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(False, "nccl", "fsdp2", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(True, "nccl", "fsdp2", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(False, "gloo", "fsdp", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(True, "gloo", "fsdp", "sglang", 1, marks=pytest.mark.sglang),
        ],
        ids=[
            "no_colocate_nccl_fsdp_vllm",
            "colocate_nccl_fsdp_vllm",
            "no_colocate_gloo_fsdp_vllm",
            "colocate_gloo_fsdp_vllm",
            "no_colocate_nccl_deepspeed_vllm",
            "colocate_nccl_deepspeed_vllm",
            "no_colocate_nccl_fsdp2_vllm",
            "colocate_nccl_fsdp2_vllm",
            "no_colocate_nccl_deepspeed_sglang",
            "colocate_nccl_deepspeed_sglang",
            "no_colocate_nccl_fsdp2_sglang",
            "colocate_nccl_fsdp2_sglang",
            "no_colocate_gloo_fsdp_sglang",
            "colocate_gloo_fsdp_sglang",
        ],
    )
    def test_policy_local_engines_e2e(colocate_all, weight_sync_backend, strategy, backend, tp_size):
        """
        Tests initalizing the policy actor group and inference engine, syncing weights, and performing generation.
        """
        try:
            cfg = get_test_actor_config()
            cfg.trainer.placement.colocate_all = colocate_all
            cfg.generator.weight_sync_backend = weight_sync_backend
            cfg.trainer.strategy = strategy
            cfg.generator.backend = backend
            cfg.generator.inference_engine_tensor_parallel_size = tp_size
    
            # If colocate is True, this will load the engine, sleep, and wake up the engine
            client, pg = init_inference_engines(
                cfg=cfg,
                use_local=True,
                async_engine=cfg.generator.async_engine,
                tp_size=cfg.generator.inference_engine_tensor_parallel_size,
                colocate_all=cfg.trainer.placement.colocate_all,
                backend=backend,
            )
    
            policy = init_worker_with_type(
                "policy",
                shared_pg=pg,
                colocate_all=cfg.trainer.placement.colocate_all,
                num_gpus_per_node=cfg.generator.inference_engine_tensor_parallel_size,
                cfg=cfg,
            )
>           ray.get(policy.async_run_ray_method("pass_through", "init_weight_sync_state", client))

tests/gpu/test_policy_local_engines_e2e.py:154: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
.venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py:22: in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py:104: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.12/site-packages/ray/_private/worker.py:2858: in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <ray._private.worker.Worker object at 0x7edbc65691f0>
object_refs = [ObjectRef(4f4ef6205ce35f90b5ae9b62ffe2bbe860032e420100000001000000), ObjectRef(17ed96eaf1aa4b2adcae35f2abc2418f345930bd0100000001000000)]
timeout = None, return_exceptions = False, skip_deserialization = False

    def get_objects(
        self,
        object_refs: list,
        timeout: Optional[float] = None,
        return_exceptions: bool = False,
        skip_deserialization: bool = False,
    ) -> Tuple[List[serialization.SerializedRayObject], bytes]:
        """Get the values in the object store associated with the IDs.
    
        Return the values from the local object store for object_refs. This
        will block until all the values for object_refs have been written to
        the local object store.
    
        Args:
            object_refs: A list of the object refs
                whose values should be retrieved.
            timeout: The maximum amount of time in
                seconds to wait before returning.
            return_exceptions: If any of the objects deserialize to an
                Exception object, whether to return them as values in the
                returned list. If False, then the first found exception will be
                raised.
            skip_deserialization: If true, only the buffer will be released and
                the object associated with the buffer will not be deserialized.
        Returns:
            list: List of deserialized objects or None if skip_deserialization is True.
            bytes: UUID of the debugger breakpoint we should drop
                into or b"" if there is no breakpoint.
        """
        # Make sure that the values are object refs.
        for object_ref in object_refs:
            if not isinstance(object_ref, ObjectRef):
                raise TypeError(
                    f"Attempting to call `get` on the value {object_ref}, "
                    "which is not an ray.ObjectRef."
                )
    
        timeout_ms = (
            int(timeout * 1000) if timeout is not None and timeout != -1 else -1
        )
        serialized_objects: List[
            serialization.SerializedRayObject
        ] = self.core_worker.get_objects(
            object_refs,
            timeout_ms,
        )
    
        debugger_breakpoint = b""
        for data, metadata, _ in serialized_objects:
            if metadata:
                metadata_fields = metadata.split(b",")
                if len(metadata_fields) >= 2 and metadata_fields[1].startswith(
                    ray_constants.OBJECT_METADATA_DEBUG_PREFIX
                ):
                    debugger_breakpoint = metadata_fields[1][
                        len(ray_constants.OBJECT_METADATA_DEBUG_PREFIX) :
                    ]
        if skip_deserialization:
            return None, debugger_breakpoint
    
        values = self.deserialize_objects(serialized_objects, object_refs)
        if not return_exceptions:
            # Raise exceptions instead of returning them to the user.
            for i, value in enumerate(values):
                if isinstance(value, RayError):
                    if isinstance(value, ray.exceptions.ObjectLostError):
                        global_worker.core_worker.dump_object_store_memory_usage()
                    if isinstance(value, RayTaskError):
>                       raise value.as_instanceof_cause()
E                       ray.exceptions.RayTaskError(ActorDiedError): [36mray::FSDPPolicyRayActorBase.init_weight_sync_state()[39m (pid=28832, ip=172.18.0.2, actor_id=b5ae9b62ffe2bbe860032e4201000000, repr=<skyrl_train.workers.fsdp.fsdp_worker.FSDPPolicyRayActorBase object at 0x7eeaa4df0e90>)
E                         File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 449, in result
E                           return self.__get_result()
E                                  ^^^^^^^^^^^^^^^^^^^
E                         File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
E                           raise self._exception
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                         File "/tmp/ray/session_2025-08-29_01-09-12_508490_5522/runtime_resources/working_dir_files/_ray_pkg_d3263afc7c3fe63e/skyrl_train/workers/worker.py", line 283, in init_weight_sync_state
E                           results = await asyncio.gather(*tasks)
E                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                         File "/tmp/ray/session_2025-08-29_01-09-12_508490_5522/runtime_resources/working_dir_files/_ray_pkg_d3263afc7c3fe63e/skyrl_train/inference_engines/inference_engine_client.py", line 177, in init_weight_update_communicator
E                           assert engine.tp_size is not None, "Engine must have a tp_size"
E                                  ^^^^^^^^^^^^^^
E                         File "/tmp/ray/session_2025-08-29_01-09-12_508490_5522/runtime_resources/working_dir_files/_ray_pkg_d3263afc7c3fe63e/skyrl_train/inference_engines/ray_wrapped_inference_engine.py", line 26, in tp_size
E                           return ray.get(self.inference_engine_actor.tp_size.remote())
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                                  ^^^^^^^^^^^^^^^^^^^
E                                  ^^^^^^^^^^^^^^^^^^^^^
E                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                       ray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, [36mray::AsyncVLLMInferenceEngine.__init__()[39m (pid=28711, ip=172.18.0.2, actor_id=f5e5ad585e882785bc6b26c501000000, repr=<skyrl_train.inference_engines.vllm.vllm_engine.AsyncVLLMInferenceEngine object at 0x74b243a65700>)
E                         File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 456, in result
E                           return self.__get_result()
E                                  ^^^^^^^^^^^^^^^^^^^
E                         File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
E                           raise self._exception
E                                  ^^^^^^^^^^^^^^^^^^^^^
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                         File "/tmp/ray/session_2025-08-29_01-09-12_508490_5522/runtime_resources/working_dir_files/_ray_pkg_d3263afc7c3fe63e/skyrl_train/inference_engines/vllm/vllm_engine.py", line 168, in __init__
E                           self.llm = self._create_engine(*args, **kwargs)
E                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                         File "/tmp/ray/session_2025-08-29_01-09-12_508490_5522/runtime_resources/working_dir_files/_ray_pkg_d3263afc7c3fe63e/skyrl_train/inference_engines/vllm/vllm_engine.py", line 317, in _create_engine
E                           engine_args = vllm.AsyncEngineArgs(disable_log_requests=True, **kwargs)
E                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                       TypeError: AsyncEngineArgs.__init__() got an unexpected keyword argument 'disable_log_requests'

.venv/lib/python3.12/site-packages/ray/_private/worker.py:958: RayTaskError(ActorDiedError)
____________ test_policy_local_engines_e2e[colocate_nccl_fsdp_vllm] ____________

colocate_all = True, weight_sync_backend = 'nccl', strategy = 'fsdp'
backend = 'vllm', tp_size = 2

    @pytest.mark.parametrize(
        ("colocate_all", "weight_sync_backend", "strategy", "backend", "tp_size"),
        [
            pytest.param(False, "nccl", "fsdp", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(True, "nccl", "fsdp", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(False, "gloo", "fsdp", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(True, "gloo", "fsdp", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(False, "nccl", "deepspeed", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(True, "nccl", "deepspeed", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(False, "nccl", "fsdp2", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(True, "nccl", "fsdp2", "vllm", 2, marks=pytest.mark.vllm),
            # TODO(Charlie): add TP > 1 tests for sglang when we support it
            pytest.param(False, "nccl", "deepspeed", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(True, "nccl", "deepspeed", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(False, "nccl", "fsdp2", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(True, "nccl", "fsdp2", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(False, "gloo", "fsdp", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(True, "gloo", "fsdp", "sglang", 1, marks=pytest.mark.sglang),
        ],
        ids=[
            "no_colocate_nccl_fsdp_vllm",
            "colocate_nccl_fsdp_vllm",
            "no_colocate_gloo_fsdp_vllm",
            "colocate_gloo_fsdp_vllm",
            "no_colocate_nccl_deepspeed_vllm",
            "colocate_nccl_deepspeed_vllm",
            "no_colocate_nccl_fsdp2_vllm",
            "colocate_nccl_fsdp2_vllm",
            "no_colocate_nccl_deepspeed_sglang",
            "colocate_nccl_deepspeed_sglang",
            "no_colocate_nccl_fsdp2_sglang",
            "colocate_nccl_fsdp2_sglang",
            "no_colocate_gloo_fsdp_sglang",
            "colocate_gloo_fsdp_sglang",
        ],
    )
    def test_policy_local_engines_e2e(colocate_all, weight_sync_backend, strategy, backend, tp_size):
        """
        Tests initalizing the policy actor group and inference engine, syncing weights, and performing generation.
        """
        try:
            cfg = get_test_actor_config()
            cfg.trainer.placement.colocate_all = colocate_all
            cfg.generator.weight_sync_backend = weight_sync_backend
            cfg.trainer.strategy = strategy
            cfg.generator.backend = backend
            cfg.generator.inference_engine_tensor_parallel_size = tp_size
    
            # If colocate is True, this will load the engine, sleep, and wake up the engine
>           client, pg = init_inference_engines(
                cfg=cfg,
                use_local=True,
                async_engine=cfg.generator.async_engine,
                tp_size=cfg.generator.inference_engine_tensor_parallel_size,
                colocate_all=cfg.trainer.placement.colocate_all,
                backend=backend,
            )

tests/gpu/test_policy_local_engines_e2e.py:138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/gpu/test_policy_local_engines_e2e.py:63: in init_inference_engines
    eps = create_ray_wrapped_inference_engines(
skyrl_train/inference_engines/ray_wrapped_inference_engine.py:207: in create_ray_wrapped_inference_engines
    ray.get(sleep_refs)
.venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py:22: in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py:104: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.12/site-packages/ray/_private/worker.py:2858: in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <ray._private.worker.Worker object at 0x7edbc65691f0>
object_refs = [ObjectRef(b58f0ee91e0a9599c20975cb8ad4e26fd4e01d3d0100000001000000)]
timeout = None, return_exceptions = False, skip_deserialization = False

    def get_objects(
        self,
        object_refs: list,
        timeout: Optional[float] = None,
        return_exceptions: bool = False,
        skip_deserialization: bool = False,
    ) -> Tuple[List[serialization.SerializedRayObject], bytes]:
        """Get the values in the object store associated with the IDs.
    
        Return the values from the local object store for object_refs. This
        will block until all the values for object_refs have been written to
        the local object store.
    
        Args:
            object_refs: A list of the object refs
                whose values should be retrieved.
            timeout: The maximum amount of time in
                seconds to wait before returning.
            return_exceptions: If any of the objects deserialize to an
                Exception object, whether to return them as values in the
                returned list. If False, then the first found exception will be
                raised.
            skip_deserialization: If true, only the buffer will be released and
                the object associated with the buffer will not be deserialized.
        Returns:
            list: List of deserialized objects or None if skip_deserialization is True.
            bytes: UUID of the debugger breakpoint we should drop
                into or b"" if there is no breakpoint.
        """
        # Make sure that the values are object refs.
        for object_ref in object_refs:
            if not isinstance(object_ref, ObjectRef):
                raise TypeError(
                    f"Attempting to call `get` on the value {object_ref}, "
                    "which is not an ray.ObjectRef."
                )
    
        timeout_ms = (
            int(timeout * 1000) if timeout is not None and timeout != -1 else -1
        )
        serialized_objects: List[
            serialization.SerializedRayObject
        ] = self.core_worker.get_objects(
            object_refs,
            timeout_ms,
        )
    
        debugger_breakpoint = b""
        for data, metadata, _ in serialized_objects:
            if metadata:
                metadata_fields = metadata.split(b",")
                if len(metadata_fields) >= 2 and metadata_fields[1].startswith(
                    ray_constants.OBJECT_METADATA_DEBUG_PREFIX
                ):
                    debugger_breakpoint = metadata_fields[1][
                        len(ray_constants.OBJECT_METADATA_DEBUG_PREFIX) :
                    ]
        if skip_deserialization:
            return None, debugger_breakpoint
    
        values = self.deserialize_objects(serialized_objects, object_refs)
        if not return_exceptions:
            # Raise exceptions instead of returning them to the user.
            for i, value in enumerate(values):
                if isinstance(value, RayError):
                    if isinstance(value, ray.exceptions.ObjectLostError):
                        global_worker.core_worker.dump_object_store_memory_usage()
                    if isinstance(value, RayTaskError):
                        raise value.as_instanceof_cause()
                    else:
>                       raise value
E                       ray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, [36mray::AsyncVLLMInferenceEngine.__init__()[39m (pid=36875, ip=172.18.0.2, actor_id=c20975cb8ad4e26fd4e01d3d01000000, repr=<skyrl_train.inference_engines.vllm.vllm_engine.AsyncVLLMInferenceEngine object at 0x75a62c220350>)
E                         File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 449, in result
E                           return self.__get_result()
E                                  ^^^^^^^^^^^^^^^^^^^
E                         File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
E                           raise self._exception
E                                  ^^^^^^^^^^^^^^^^^^^^^
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                         File "/tmp/ray/session_2025-08-29_01-09-56_124453_5522/runtime_resources/working_dir_files/_ray_pkg_49c55a07945afadb/skyrl_train/inference_engines/vllm/vllm_engine.py", line 168, in __init__
E                           self.llm = self._create_engine(*args, **kwargs)
E                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                         File "/tmp/ray/session_2025-08-29_01-09-56_124453_5522/runtime_resources/working_dir_files/_ray_pkg_49c55a07945afadb/skyrl_train/inference_engines/vllm/vllm_engine.py", line 317, in _create_engine
E                           engine_args = vllm.AsyncEngineArgs(disable_log_requests=True, **kwargs)
E                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                       TypeError: AsyncEngineArgs.__init__() got an unexpected keyword argument 'disable_log_requests'

.venv/lib/python3.12/site-packages/ray/_private/worker.py:960: ActorDiedError
__________ test_policy_local_engines_e2e[no_colocate_gloo_fsdp_vllm] ___________

colocate_all = False, weight_sync_backend = 'gloo', strategy = 'fsdp'
backend = 'vllm', tp_size = 2

    @pytest.mark.parametrize(
        ("colocate_all", "weight_sync_backend", "strategy", "backend", "tp_size"),
        [
            pytest.param(False, "nccl", "fsdp", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(True, "nccl", "fsdp", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(False, "gloo", "fsdp", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(True, "gloo", "fsdp", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(False, "nccl", "deepspeed", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(True, "nccl", "deepspeed", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(False, "nccl", "fsdp2", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(True, "nccl", "fsdp2", "vllm", 2, marks=pytest.mark.vllm),
            # TODO(Charlie): add TP > 1 tests for sglang when we support it
            pytest.param(False, "nccl", "deepspeed", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(True, "nccl", "deepspeed", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(False, "nccl", "fsdp2", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(True, "nccl", "fsdp2", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(False, "gloo", "fsdp", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(True, "gloo", "fsdp", "sglang", 1, marks=pytest.mark.sglang),
        ],
        ids=[
            "no_colocate_nccl_fsdp_vllm",
            "colocate_nccl_fsdp_vllm",
            "no_colocate_gloo_fsdp_vllm",
            "colocate_gloo_fsdp_vllm",
            "no_colocate_nccl_deepspeed_vllm",
            "colocate_nccl_deepspeed_vllm",
            "no_colocate_nccl_fsdp2_vllm",
            "colocate_nccl_fsdp2_vllm",
            "no_colocate_nccl_deepspeed_sglang",
            "colocate_nccl_deepspeed_sglang",
            "no_colocate_nccl_fsdp2_sglang",
            "colocate_nccl_fsdp2_sglang",
            "no_colocate_gloo_fsdp_sglang",
            "colocate_gloo_fsdp_sglang",
        ],
    )
    def test_policy_local_engines_e2e(colocate_all, weight_sync_backend, strategy, backend, tp_size):
        """
        Tests initalizing the policy actor group and inference engine, syncing weights, and performing generation.
        """
        try:
            cfg = get_test_actor_config()
            cfg.trainer.placement.colocate_all = colocate_all
            cfg.generator.weight_sync_backend = weight_sync_backend
            cfg.trainer.strategy = strategy
            cfg.generator.backend = backend
            cfg.generator.inference_engine_tensor_parallel_size = tp_size
    
            # If colocate is True, this will load the engine, sleep, and wake up the engine
            client, pg = init_inference_engines(
                cfg=cfg,
                use_local=True,
                async_engine=cfg.generator.async_engine,
                tp_size=cfg.generator.inference_engine_tensor_parallel_size,
                colocate_all=cfg.trainer.placement.colocate_all,
                backend=backend,
            )
    
            policy = init_worker_with_type(
                "policy",
                shared_pg=pg,
                colocate_all=cfg.trainer.placement.colocate_all,
                num_gpus_per_node=cfg.generator.inference_engine_tensor_parallel_size,
                cfg=cfg,
            )
>           ray.get(policy.async_run_ray_method("pass_through", "init_weight_sync_state", client))

tests/gpu/test_policy_local_engines_e2e.py:154: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
.venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py:22: in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py:104: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.12/site-packages/ray/_private/worker.py:2858: in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <ray._private.worker.Worker object at 0x7edbc65691f0>
object_refs = [ObjectRef(e0513aa905843ccf7b5a465dce94ce86f206b5700100000001000000), ObjectRef(5f70e045687d2f9a11ead99d981b9df18489c0720100000001000000)]
timeout = None, return_exceptions = False, skip_deserialization = False

    def get_objects(
        self,
        object_refs: list,
        timeout: Optional[float] = None,
        return_exceptions: bool = False,
        skip_deserialization: bool = False,
    ) -> Tuple[List[serialization.SerializedRayObject], bytes]:
        """Get the values in the object store associated with the IDs.
    
        Return the values from the local object store for object_refs. This
        will block until all the values for object_refs have been written to
        the local object store.
    
        Args:
            object_refs: A list of the object refs
                whose values should be retrieved.
            timeout: The maximum amount of time in
                seconds to wait before returning.
            return_exceptions: If any of the objects deserialize to an
                Exception object, whether to return them as values in the
                returned list. If False, then the first found exception will be
                raised.
            skip_deserialization: If true, only the buffer will be released and
                the object associated with the buffer will not be deserialized.
        Returns:
            list: List of deserialized objects or None if skip_deserialization is True.
            bytes: UUID of the debugger breakpoint we should drop
                into or b"" if there is no breakpoint.
        """
        # Make sure that the values are object refs.
        for object_ref in object_refs:
            if not isinstance(object_ref, ObjectRef):
                raise TypeError(
                    f"Attempting to call `get` on the value {object_ref}, "
                    "which is not an ray.ObjectRef."
                )
    
        timeout_ms = (
            int(timeout * 1000) if timeout is not None and timeout != -1 else -1
        )
        serialized_objects: List[
            serialization.SerializedRayObject
        ] = self.core_worker.get_objects(
            object_refs,
            timeout_ms,
        )
    
        debugger_breakpoint = b""
        for data, metadata, _ in serialized_objects:
            if metadata:
                metadata_fields = metadata.split(b",")
                if len(metadata_fields) >= 2 and metadata_fields[1].startswith(
                    ray_constants.OBJECT_METADATA_DEBUG_PREFIX
                ):
                    debugger_breakpoint = metadata_fields[1][
                        len(ray_constants.OBJECT_METADATA_DEBUG_PREFIX) :
                    ]
        if skip_deserialization:
            return None, debugger_breakpoint
    
        values = self.deserialize_objects(serialized_objects, object_refs)
        if not return_exceptions:
            # Raise exceptions instead of returning them to the user.
            for i, value in enumerate(values):
                if isinstance(value, RayError):
                    if isinstance(value, ray.exceptions.ObjectLostError):
                        global_worker.core_worker.dump_object_store_memory_usage()
                    if isinstance(value, RayTaskError):
>                       raise value.as_instanceof_cause()
E                       ray.exceptions.RayTaskError(ActorDiedError): [36mray::FSDPPolicyRayActorBase.init_weight_sync_state()[39m (pid=44807, ip=172.18.0.2, actor_id=7b5a465dce94ce86f206b57001000000, repr=<skyrl_train.workers.fsdp.fsdp_worker.FSDPPolicyRayActorBase object at 0x71795234a4b0>)
E                         File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 449, in result
E                           return self.__get_result()
E                                  ^^^^^^^^^^^^^^^^^^^
E                         File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
E                           raise self._exception
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                         File "/tmp/ray/session_2025-08-29_01-10-23_562698_5522/runtime_resources/working_dir_files/_ray_pkg_bd2e4c86161387d3/skyrl_train/workers/worker.py", line 283, in init_weight_sync_state
E                           results = await asyncio.gather(*tasks)
E                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                         File "/tmp/ray/session_2025-08-29_01-10-23_562698_5522/runtime_resources/working_dir_files/_ray_pkg_bd2e4c86161387d3/skyrl_train/inference_engines/inference_engine_client.py", line 177, in init_weight_update_communicator
E                           assert engine.tp_size is not None, "Engine must have a tp_size"
E                                  ^^^^^^^^^^^^^^
E                         File "/tmp/ray/session_2025-08-29_01-10-23_562698_5522/runtime_resources/working_dir_files/_ray_pkg_bd2e4c86161387d3/skyrl_train/inference_engines/ray_wrapped_inference_engine.py", line 26, in tp_size
E                           return ray.get(self.inference_engine_actor.tp_size.remote())
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                                  ^^^^^^^^^^^^^^^^^^^
E                                  ^^^^^^^^^^^^^^^^^^^^^
E                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                       ray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, [36mray::AsyncVLLMInferenceEngine.__init__()[39m (pid=44685, ip=172.18.0.2, actor_id=a72c003d333b7559c4757c4201000000, repr=<skyrl_train.inference_engines.vllm.vllm_engine.AsyncVLLMInferenceEngine object at 0x73cfbadd47d0>)
E                         File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 456, in result
E                           return self.__get_result()
E                                  ^^^^^^^^^^^^^^^^^^^
E                         File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
E                           raise self._exception
E                                  ^^^^^^^^^^^^^^^^^^^^^
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                         File "/tmp/ray/session_2025-08-29_01-10-23_562698_5522/runtime_resources/working_dir_files/_ray_pkg_bd2e4c86161387d3/skyrl_train/inference_engines/vllm/vllm_engine.py", line 168, in __init__
E                           self.llm = self._create_engine(*args, **kwargs)
E                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                         File "/tmp/ray/session_2025-08-29_01-10-23_562698_5522/runtime_resources/working_dir_files/_ray_pkg_bd2e4c86161387d3/skyrl_train/inference_engines/vllm/vllm_engine.py", line 317, in _create_engine
E                           engine_args = vllm.AsyncEngineArgs(disable_log_requests=True, **kwargs)
E                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                       TypeError: AsyncEngineArgs.__init__() got an unexpected keyword argument 'disable_log_requests'

.venv/lib/python3.12/site-packages/ray/_private/worker.py:958: RayTaskError(ActorDiedError)
____________ test_policy_local_engines_e2e[colocate_gloo_fsdp_vllm] ____________

colocate_all = True, weight_sync_backend = 'gloo', strategy = 'fsdp'
backend = 'vllm', tp_size = 2

    @pytest.mark.parametrize(
        ("colocate_all", "weight_sync_backend", "strategy", "backend", "tp_size"),
        [
            pytest.param(False, "nccl", "fsdp", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(True, "nccl", "fsdp", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(False, "gloo", "fsdp", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(True, "gloo", "fsdp", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(False, "nccl", "deepspeed", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(True, "nccl", "deepspeed", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(False, "nccl", "fsdp2", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(True, "nccl", "fsdp2", "vllm", 2, marks=pytest.mark.vllm),
            # TODO(Charlie): add TP > 1 tests for sglang when we support it
            pytest.param(False, "nccl", "deepspeed", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(True, "nccl", "deepspeed", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(False, "nccl", "fsdp2", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(True, "nccl", "fsdp2", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(False, "gloo", "fsdp", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(True, "gloo", "fsdp", "sglang", 1, marks=pytest.mark.sglang),
        ],
        ids=[
            "no_colocate_nccl_fsdp_vllm",
            "colocate_nccl_fsdp_vllm",
            "no_colocate_gloo_fsdp_vllm",
            "colocate_gloo_fsdp_vllm",
            "no_colocate_nccl_deepspeed_vllm",
            "colocate_nccl_deepspeed_vllm",
            "no_colocate_nccl_fsdp2_vllm",
            "colocate_nccl_fsdp2_vllm",
            "no_colocate_nccl_deepspeed_sglang",
            "colocate_nccl_deepspeed_sglang",
            "no_colocate_nccl_fsdp2_sglang",
            "colocate_nccl_fsdp2_sglang",
            "no_colocate_gloo_fsdp_sglang",
            "colocate_gloo_fsdp_sglang",
        ],
    )
    def test_policy_local_engines_e2e(colocate_all, weight_sync_backend, strategy, backend, tp_size):
        """
        Tests initalizing the policy actor group and inference engine, syncing weights, and performing generation.
        """
        try:
            cfg = get_test_actor_config()
            cfg.trainer.placement.colocate_all = colocate_all
            cfg.generator.weight_sync_backend = weight_sync_backend
            cfg.trainer.strategy = strategy
            cfg.generator.backend = backend
            cfg.generator.inference_engine_tensor_parallel_size = tp_size
    
            # If colocate is True, this will load the engine, sleep, and wake up the engine
>           client, pg = init_inference_engines(
                cfg=cfg,
                use_local=True,
                async_engine=cfg.generator.async_engine,
                tp_size=cfg.generator.inference_engine_tensor_parallel_size,
                colocate_all=cfg.trainer.placement.colocate_all,
                backend=backend,
            )

tests/gpu/test_policy_local_engines_e2e.py:138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/gpu/test_policy_local_engines_e2e.py:63: in init_inference_engines
    eps = create_ray_wrapped_inference_engines(
skyrl_train/inference_engines/ray_wrapped_inference_engine.py:207: in create_ray_wrapped_inference_engines
    ray.get(sleep_refs)
.venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py:22: in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py:104: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.12/site-packages/ray/_private/worker.py:2858: in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <ray._private.worker.Worker object at 0x7edbc65691f0>
object_refs = [ObjectRef(72482135a26f4e0fc1e988e116867ae6e4f07a200100000001000000)]
timeout = None, return_exceptions = False, skip_deserialization = False

    def get_objects(
        self,
        object_refs: list,
        timeout: Optional[float] = None,
        return_exceptions: bool = False,
        skip_deserialization: bool = False,
    ) -> Tuple[List[serialization.SerializedRayObject], bytes]:
        """Get the values in the object store associated with the IDs.
    
        Return the values from the local object store for object_refs. This
        will block until all the values for object_refs have been written to
        the local object store.
    
        Args:
            object_refs: A list of the object refs
                whose values should be retrieved.
            timeout: The maximum amount of time in
                seconds to wait before returning.
            return_exceptions: If any of the objects deserialize to an
                Exception object, whether to return them as values in the
                returned list. If False, then the first found exception will be
                raised.
            skip_deserialization: If true, only the buffer will be released and
                the object associated with the buffer will not be deserialized.
        Returns:
            list: List of deserialized objects or None if skip_deserialization is True.
            bytes: UUID of the debugger breakpoint we should drop
                into or b"" if there is no breakpoint.
        """
        # Make sure that the values are object refs.
        for object_ref in object_refs:
            if not isinstance(object_ref, ObjectRef):
                raise TypeError(
                    f"Attempting to call `get` on the value {object_ref}, "
                    "which is not an ray.ObjectRef."
                )
    
        timeout_ms = (
            int(timeout * 1000) if timeout is not None and timeout != -1 else -1
        )
        serialized_objects: List[
            serialization.SerializedRayObject
        ] = self.core_worker.get_objects(
            object_refs,
            timeout_ms,
        )
    
        debugger_breakpoint = b""
        for data, metadata, _ in serialized_objects:
            if metadata:
                metadata_fields = metadata.split(b",")
                if len(metadata_fields) >= 2 and metadata_fields[1].startswith(
                    ray_constants.OBJECT_METADATA_DEBUG_PREFIX
                ):
                    debugger_breakpoint = metadata_fields[1][
                        len(ray_constants.OBJECT_METADATA_DEBUG_PREFIX) :
                    ]
        if skip_deserialization:
            return None, debugger_breakpoint
    
        values = self.deserialize_objects(serialized_objects, object_refs)
        if not return_exceptions:
            # Raise exceptions instead of returning them to the user.
            for i, value in enumerate(values):
                if isinstance(value, RayError):
                    if isinstance(value, ray.exceptions.ObjectLostError):
                        global_worker.core_worker.dump_object_store_memory_usage()
                    if isinstance(value, RayTaskError):
                        raise value.as_instanceof_cause()
                    else:
>                       raise value
E                       ray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, [36mray::AsyncVLLMInferenceEngine.__init__()[39m (pid=52651, ip=172.18.0.2, actor_id=c1e988e116867ae6e4f07a2001000000, repr=<skyrl_train.inference_engines.vllm.vllm_engine.AsyncVLLMInferenceEngine object at 0x71cc1940c830>)
E                         File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 456, in result
E                           return self.__get_result()
E                                  ^^^^^^^^^^^^^^^^^^^
E                         File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
E                           raise self._exception
E                                  ^^^^^^^^^^^^^^^^^^^^^
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                         File "/tmp/ray/session_2025-08-29_01-10-59_775751_5522/runtime_resources/working_dir_files/_ray_pkg_ae8601a03e1e2844/skyrl_train/inference_engines/vllm/vllm_engine.py", line 168, in __init__
E                           self.llm = self._create_engine(*args, **kwargs)
E                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                         File "/tmp/ray/session_2025-08-29_01-10-59_775751_5522/runtime_resources/working_dir_files/_ray_pkg_ae8601a03e1e2844/skyrl_train/inference_engines/vllm/vllm_engine.py", line 317, in _create_engine
E                           engine_args = vllm.AsyncEngineArgs(disable_log_requests=True, **kwargs)
E                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                       TypeError: AsyncEngineArgs.__init__() got an unexpected keyword argument 'disable_log_requests'

.venv/lib/python3.12/site-packages/ray/_private/worker.py:960: ActorDiedError
________ test_policy_local_engines_e2e[no_colocate_nccl_deepspeed_vllm] ________

colocate_all = False, weight_sync_backend = 'nccl', strategy = 'deepspeed'
backend = 'vllm', tp_size = 2

    @pytest.mark.parametrize(
        ("colocate_all", "weight_sync_backend", "strategy", "backend", "tp_size"),
        [
            pytest.param(False, "nccl", "fsdp", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(True, "nccl", "fsdp", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(False, "gloo", "fsdp", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(True, "gloo", "fsdp", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(False, "nccl", "deepspeed", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(True, "nccl", "deepspeed", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(False, "nccl", "fsdp2", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(True, "nccl", "fsdp2", "vllm", 2, marks=pytest.mark.vllm),
            # TODO(Charlie): add TP > 1 tests for sglang when we support it
            pytest.param(False, "nccl", "deepspeed", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(True, "nccl", "deepspeed", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(False, "nccl", "fsdp2", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(True, "nccl", "fsdp2", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(False, "gloo", "fsdp", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(True, "gloo", "fsdp", "sglang", 1, marks=pytest.mark.sglang),
        ],
        ids=[
            "no_colocate_nccl_fsdp_vllm",
            "colocate_nccl_fsdp_vllm",
            "no_colocate_gloo_fsdp_vllm",
            "colocate_gloo_fsdp_vllm",
            "no_colocate_nccl_deepspeed_vllm",
            "colocate_nccl_deepspeed_vllm",
            "no_colocate_nccl_fsdp2_vllm",
            "colocate_nccl_fsdp2_vllm",
            "no_colocate_nccl_deepspeed_sglang",
            "colocate_nccl_deepspeed_sglang",
            "no_colocate_nccl_fsdp2_sglang",
            "colocate_nccl_fsdp2_sglang",
            "no_colocate_gloo_fsdp_sglang",
            "colocate_gloo_fsdp_sglang",
        ],
    )
    def test_policy_local_engines_e2e(colocate_all, weight_sync_backend, strategy, backend, tp_size):
        """
        Tests initalizing the policy actor group and inference engine, syncing weights, and performing generation.
        """
        try:
            cfg = get_test_actor_config()
            cfg.trainer.placement.colocate_all = colocate_all
            cfg.generator.weight_sync_backend = weight_sync_backend
            cfg.trainer.strategy = strategy
            cfg.generator.backend = backend
            cfg.generator.inference_engine_tensor_parallel_size = tp_size
    
            # If colocate is True, this will load the engine, sleep, and wake up the engine
            client, pg = init_inference_engines(
                cfg=cfg,
                use_local=True,
                async_engine=cfg.generator.async_engine,
                tp_size=cfg.generator.inference_engine_tensor_parallel_size,
                colocate_all=cfg.trainer.placement.colocate_all,
                backend=backend,
            )
    
>           policy = init_worker_with_type(
                "policy",
                shared_pg=pg,
                colocate_all=cfg.trainer.placement.colocate_all,
                num_gpus_per_node=cfg.generator.inference_engine_tensor_parallel_size,
                cfg=cfg,
            )

tests/gpu/test_policy_local_engines_e2e.py:147: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/gpu/utils.py:145: in init_worker_with_type
    worker_cls = import_worker(cfg.trainer.strategy, worker_type)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tests/gpu/utils.py:126: in import_worker
    module = importlib.import_module(module_path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:999: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    import asyncio
    from typing import List, Dict
    
>   import deepspeed
E   ModuleNotFoundError: No module named 'deepspeed'

skyrl_train/workers/deepspeed/deepspeed_worker.py:4: ModuleNotFoundError
_________ test_policy_local_engines_e2e[colocate_nccl_deepspeed_vllm] __________

colocate_all = True, weight_sync_backend = 'nccl', strategy = 'deepspeed'
backend = 'vllm', tp_size = 2

    @pytest.mark.parametrize(
        ("colocate_all", "weight_sync_backend", "strategy", "backend", "tp_size"),
        [
            pytest.param(False, "nccl", "fsdp", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(True, "nccl", "fsdp", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(False, "gloo", "fsdp", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(True, "gloo", "fsdp", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(False, "nccl", "deepspeed", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(True, "nccl", "deepspeed", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(False, "nccl", "fsdp2", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(True, "nccl", "fsdp2", "vllm", 2, marks=pytest.mark.vllm),
            # TODO(Charlie): add TP > 1 tests for sglang when we support it
            pytest.param(False, "nccl", "deepspeed", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(True, "nccl", "deepspeed", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(False, "nccl", "fsdp2", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(True, "nccl", "fsdp2", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(False, "gloo", "fsdp", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(True, "gloo", "fsdp", "sglang", 1, marks=pytest.mark.sglang),
        ],
        ids=[
            "no_colocate_nccl_fsdp_vllm",
            "colocate_nccl_fsdp_vllm",
            "no_colocate_gloo_fsdp_vllm",
            "colocate_gloo_fsdp_vllm",
            "no_colocate_nccl_deepspeed_vllm",
            "colocate_nccl_deepspeed_vllm",
            "no_colocate_nccl_fsdp2_vllm",
            "colocate_nccl_fsdp2_vllm",
            "no_colocate_nccl_deepspeed_sglang",
            "colocate_nccl_deepspeed_sglang",
            "no_colocate_nccl_fsdp2_sglang",
            "colocate_nccl_fsdp2_sglang",
            "no_colocate_gloo_fsdp_sglang",
            "colocate_gloo_fsdp_sglang",
        ],
    )
    def test_policy_local_engines_e2e(colocate_all, weight_sync_backend, strategy, backend, tp_size):
        """
        Tests initalizing the policy actor group and inference engine, syncing weights, and performing generation.
        """
        try:
            cfg = get_test_actor_config()
            cfg.trainer.placement.colocate_all = colocate_all
            cfg.generator.weight_sync_backend = weight_sync_backend
            cfg.trainer.strategy = strategy
            cfg.generator.backend = backend
            cfg.generator.inference_engine_tensor_parallel_size = tp_size
    
            # If colocate is True, this will load the engine, sleep, and wake up the engine
>           client, pg = init_inference_engines(
                cfg=cfg,
                use_local=True,
                async_engine=cfg.generator.async_engine,
                tp_size=cfg.generator.inference_engine_tensor_parallel_size,
                colocate_all=cfg.trainer.placement.colocate_all,
                backend=backend,
            )

tests/gpu/test_policy_local_engines_e2e.py:138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/gpu/test_policy_local_engines_e2e.py:63: in init_inference_engines
    eps = create_ray_wrapped_inference_engines(
skyrl_train/inference_engines/ray_wrapped_inference_engine.py:207: in create_ray_wrapped_inference_engines
    ray.get(sleep_refs)
.venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py:22: in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py:104: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.12/site-packages/ray/_private/worker.py:2858: in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <ray._private.worker.Worker object at 0x7edbc65691f0>
object_refs = [ObjectRef(52cbda0069bf747fa8251b40128744458fb179e50100000001000000)]
timeout = None, return_exceptions = False, skip_deserialization = False

    def get_objects(
        self,
        object_refs: list,
        timeout: Optional[float] = None,
        return_exceptions: bool = False,
        skip_deserialization: bool = False,
    ) -> Tuple[List[serialization.SerializedRayObject], bytes]:
        """Get the values in the object store associated with the IDs.
    
        Return the values from the local object store for object_refs. This
        will block until all the values for object_refs have been written to
        the local object store.
    
        Args:
            object_refs: A list of the object refs
                whose values should be retrieved.
            timeout: The maximum amount of time in
                seconds to wait before returning.
            return_exceptions: If any of the objects deserialize to an
                Exception object, whether to return them as values in the
                returned list. If False, then the first found exception will be
                raised.
            skip_deserialization: If true, only the buffer will be released and
                the object associated with the buffer will not be deserialized.
        Returns:
            list: List of deserialized objects or None if skip_deserialization is True.
            bytes: UUID of the debugger breakpoint we should drop
                into or b"" if there is no breakpoint.
        """
        # Make sure that the values are object refs.
        for object_ref in object_refs:
            if not isinstance(object_ref, ObjectRef):
                raise TypeError(
                    f"Attempting to call `get` on the value {object_ref}, "
                    "which is not an ray.ObjectRef."
                )
    
        timeout_ms = (
            int(timeout * 1000) if timeout is not None and timeout != -1 else -1
        )
        serialized_objects: List[
            serialization.SerializedRayObject
        ] = self.core_worker.get_objects(
            object_refs,
            timeout_ms,
        )
    
        debugger_breakpoint = b""
        for data, metadata, _ in serialized_objects:
            if metadata:
                metadata_fields = metadata.split(b",")
                if len(metadata_fields) >= 2 and metadata_fields[1].startswith(
                    ray_constants.OBJECT_METADATA_DEBUG_PREFIX
                ):
                    debugger_breakpoint = metadata_fields[1][
                        len(ray_constants.OBJECT_METADATA_DEBUG_PREFIX) :
                    ]
        if skip_deserialization:
            return None, debugger_breakpoint
    
        values = self.deserialize_objects(serialized_objects, object_refs)
        if not return_exceptions:
            # Raise exceptions instead of returning them to the user.
            for i, value in enumerate(values):
                if isinstance(value, RayError):
                    if isinstance(value, ray.exceptions.ObjectLostError):
                        global_worker.core_worker.dump_object_store_memory_usage()
                    if isinstance(value, RayTaskError):
                        raise value.as_instanceof_cause()
                    else:
>                       raise value
E                       ray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, [36mray::AsyncVLLMInferenceEngine.__init__()[39m (pid=67754, ip=172.18.0.2, actor_id=a8251b40128744458fb179e501000000, repr=<skyrl_train.inference_engines.vllm.vllm_engine.AsyncVLLMInferenceEngine object at 0x72be1902c260>)
E                         File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 449, in result
E                           return self.__get_result()
E                                  ^^^^^^^^^^^^^^^^^^^
E                         File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
E                           raise self._exception
E                                  ^^^^^^^^^^^^^^^^^^^^^
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                         File "/tmp/ray/session_2025-08-29_01-11-40_008387_5522/runtime_resources/working_dir_files/_ray_pkg_e407cccff952e435/skyrl_train/inference_engines/vllm/vllm_engine.py", line 168, in __init__
E                           self.llm = self._create_engine(*args, **kwargs)
E                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                         File "/tmp/ray/session_2025-08-29_01-11-40_008387_5522/runtime_resources/working_dir_files/_ray_pkg_e407cccff952e435/skyrl_train/inference_engines/vllm/vllm_engine.py", line 317, in _create_engine
E                           engine_args = vllm.AsyncEngineArgs(disable_log_requests=True, **kwargs)
E                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                       TypeError: AsyncEngineArgs.__init__() got an unexpected keyword argument 'disable_log_requests'

.venv/lib/python3.12/site-packages/ray/_private/worker.py:960: ActorDiedError
__________ test_policy_local_engines_e2e[no_colocate_nccl_fsdp2_vllm] __________

colocate_all = False, weight_sync_backend = 'nccl', strategy = 'fsdp2'
backend = 'vllm', tp_size = 2

    @pytest.mark.parametrize(
        ("colocate_all", "weight_sync_backend", "strategy", "backend", "tp_size"),
        [
            pytest.param(False, "nccl", "fsdp", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(True, "nccl", "fsdp", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(False, "gloo", "fsdp", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(True, "gloo", "fsdp", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(False, "nccl", "deepspeed", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(True, "nccl", "deepspeed", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(False, "nccl", "fsdp2", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(True, "nccl", "fsdp2", "vllm", 2, marks=pytest.mark.vllm),
            # TODO(Charlie): add TP > 1 tests for sglang when we support it
            pytest.param(False, "nccl", "deepspeed", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(True, "nccl", "deepspeed", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(False, "nccl", "fsdp2", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(True, "nccl", "fsdp2", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(False, "gloo", "fsdp", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(True, "gloo", "fsdp", "sglang", 1, marks=pytest.mark.sglang),
        ],
        ids=[
            "no_colocate_nccl_fsdp_vllm",
            "colocate_nccl_fsdp_vllm",
            "no_colocate_gloo_fsdp_vllm",
            "colocate_gloo_fsdp_vllm",
            "no_colocate_nccl_deepspeed_vllm",
            "colocate_nccl_deepspeed_vllm",
            "no_colocate_nccl_fsdp2_vllm",
            "colocate_nccl_fsdp2_vllm",
            "no_colocate_nccl_deepspeed_sglang",
            "colocate_nccl_deepspeed_sglang",
            "no_colocate_nccl_fsdp2_sglang",
            "colocate_nccl_fsdp2_sglang",
            "no_colocate_gloo_fsdp_sglang",
            "colocate_gloo_fsdp_sglang",
        ],
    )
    def test_policy_local_engines_e2e(colocate_all, weight_sync_backend, strategy, backend, tp_size):
        """
        Tests initalizing the policy actor group and inference engine, syncing weights, and performing generation.
        """
        try:
            cfg = get_test_actor_config()
            cfg.trainer.placement.colocate_all = colocate_all
            cfg.generator.weight_sync_backend = weight_sync_backend
            cfg.trainer.strategy = strategy
            cfg.generator.backend = backend
            cfg.generator.inference_engine_tensor_parallel_size = tp_size
    
            # If colocate is True, this will load the engine, sleep, and wake up the engine
            client, pg = init_inference_engines(
                cfg=cfg,
                use_local=True,
                async_engine=cfg.generator.async_engine,
                tp_size=cfg.generator.inference_engine_tensor_parallel_size,
                colocate_all=cfg.trainer.placement.colocate_all,
                backend=backend,
            )
    
            policy = init_worker_with_type(
                "policy",
                shared_pg=pg,
                colocate_all=cfg.trainer.placement.colocate_all,
                num_gpus_per_node=cfg.generator.inference_engine_tensor_parallel_size,
                cfg=cfg,
            )
>           ray.get(policy.async_run_ray_method("pass_through", "init_weight_sync_state", client))

tests/gpu/test_policy_local_engines_e2e.py:154: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
.venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py:22: in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py:104: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.12/site-packages/ray/_private/worker.py:2858: in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <ray._private.worker.Worker object at 0x7edbc65691f0>
object_refs = [ObjectRef(d99fa8d5c923d7c89dbf2757318bf127934b94060100000001000000), ObjectRef(aa5ec071964645d2e010e8065aeabb00277a134c0100000001000000)]
timeout = None, return_exceptions = False, skip_deserialization = False

    def get_objects(
        self,
        object_refs: list,
        timeout: Optional[float] = None,
        return_exceptions: bool = False,
        skip_deserialization: bool = False,
    ) -> Tuple[List[serialization.SerializedRayObject], bytes]:
        """Get the values in the object store associated with the IDs.
    
        Return the values from the local object store for object_refs. This
        will block until all the values for object_refs have been written to
        the local object store.
    
        Args:
            object_refs: A list of the object refs
                whose values should be retrieved.
            timeout: The maximum amount of time in
                seconds to wait before returning.
            return_exceptions: If any of the objects deserialize to an
                Exception object, whether to return them as values in the
                returned list. If False, then the first found exception will be
                raised.
            skip_deserialization: If true, only the buffer will be released and
                the object associated with the buffer will not be deserialized.
        Returns:
            list: List of deserialized objects or None if skip_deserialization is True.
            bytes: UUID of the debugger breakpoint we should drop
                into or b"" if there is no breakpoint.
        """
        # Make sure that the values are object refs.
        for object_ref in object_refs:
            if not isinstance(object_ref, ObjectRef):
                raise TypeError(
                    f"Attempting to call `get` on the value {object_ref}, "
                    "which is not an ray.ObjectRef."
                )
    
        timeout_ms = (
            int(timeout * 1000) if timeout is not None and timeout != -1 else -1
        )
        serialized_objects: List[
            serialization.SerializedRayObject
        ] = self.core_worker.get_objects(
            object_refs,
            timeout_ms,
        )
    
        debugger_breakpoint = b""
        for data, metadata, _ in serialized_objects:
            if metadata:
                metadata_fields = metadata.split(b",")
                if len(metadata_fields) >= 2 and metadata_fields[1].startswith(
                    ray_constants.OBJECT_METADATA_DEBUG_PREFIX
                ):
                    debugger_breakpoint = metadata_fields[1][
                        len(ray_constants.OBJECT_METADATA_DEBUG_PREFIX) :
                    ]
        if skip_deserialization:
            return None, debugger_breakpoint
    
        values = self.deserialize_objects(serialized_objects, object_refs)
        if not return_exceptions:
            # Raise exceptions instead of returning them to the user.
            for i, value in enumerate(values):
                if isinstance(value, RayError):
                    if isinstance(value, ray.exceptions.ObjectLostError):
                        global_worker.core_worker.dump_object_store_memory_usage()
                    if isinstance(value, RayTaskError):
>                       raise value.as_instanceof_cause()
E                       ray.exceptions.RayTaskError(ActorDiedError): [36mray::FSDPPolicyRayActorBase.init_weight_sync_state()[39m (pid=75664, ip=172.18.0.2, actor_id=9dbf2757318bf127934b940601000000, repr=<skyrl_train.workers.fsdp.fsdp_worker.FSDPPolicyRayActorBase object at 0x7d09c0b13410>)
E                         File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 449, in result
E                           return self.__get_result()
E                                  ^^^^^^^^^^^^^^^^^^^
E                         File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
E                           raise self._exception
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                         File "/tmp/ray/session_2025-08-29_01-12-06_811922_5522/runtime_resources/working_dir_files/_ray_pkg_1b68f74e2cefa159/skyrl_train/workers/worker.py", line 283, in init_weight_sync_state
E                           results = await asyncio.gather(*tasks)
E                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                         File "/tmp/ray/session_2025-08-29_01-12-06_811922_5522/runtime_resources/working_dir_files/_ray_pkg_1b68f74e2cefa159/skyrl_train/inference_engines/inference_engine_client.py", line 177, in init_weight_update_communicator
E                           assert engine.tp_size is not None, "Engine must have a tp_size"
E                                  ^^^^^^^^^^^^^^
E                         File "/tmp/ray/session_2025-08-29_01-12-06_811922_5522/runtime_resources/working_dir_files/_ray_pkg_1b68f74e2cefa159/skyrl_train/inference_engines/ray_wrapped_inference_engine.py", line 26, in tp_size
E                           return ray.get(self.inference_engine_actor.tp_size.remote())
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                                  ^^^^^^^^^^^^^^^^^^^
E                                  ^^^^^^^^^^^^^^^^^^^^^
E                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                       ray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, [36mray::AsyncVLLMInferenceEngine.__init__()[39m (pid=75574, ip=172.18.0.2, actor_id=d8e1de49cd1b9de627e447f101000000, repr=<skyrl_train.inference_engines.vllm.vllm_engine.AsyncVLLMInferenceEngine object at 0x719017e96570>)
E                         File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 456, in result
E                           return self.__get_result()
E                                  ^^^^^^^^^^^^^^^^^^^
E                         File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
E                           raise self._exception
E                                  ^^^^^^^^^^^^^^^^^^^^^
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                         File "/tmp/ray/session_2025-08-29_01-12-06_811922_5522/runtime_resources/working_dir_files/_ray_pkg_1b68f74e2cefa159/skyrl_train/inference_engines/vllm/vllm_engine.py", line 168, in __init__
E                           self.llm = self._create_engine(*args, **kwargs)
E                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                         File "/tmp/ray/session_2025-08-29_01-12-06_811922_5522/runtime_resources/working_dir_files/_ray_pkg_1b68f74e2cefa159/skyrl_train/inference_engines/vllm/vllm_engine.py", line 317, in _create_engine
E                           engine_args = vllm.AsyncEngineArgs(disable_log_requests=True, **kwargs)
E                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                       TypeError: AsyncEngineArgs.__init__() got an unexpected keyword argument 'disable_log_requests'

.venv/lib/python3.12/site-packages/ray/_private/worker.py:958: RayTaskError(ActorDiedError)
___________ test_policy_local_engines_e2e[colocate_nccl_fsdp2_vllm] ____________

colocate_all = True, weight_sync_backend = 'nccl', strategy = 'fsdp2'
backend = 'vllm', tp_size = 2

    @pytest.mark.parametrize(
        ("colocate_all", "weight_sync_backend", "strategy", "backend", "tp_size"),
        [
            pytest.param(False, "nccl", "fsdp", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(True, "nccl", "fsdp", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(False, "gloo", "fsdp", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(True, "gloo", "fsdp", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(False, "nccl", "deepspeed", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(True, "nccl", "deepspeed", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(False, "nccl", "fsdp2", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(True, "nccl", "fsdp2", "vllm", 2, marks=pytest.mark.vllm),
            # TODO(Charlie): add TP > 1 tests for sglang when we support it
            pytest.param(False, "nccl", "deepspeed", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(True, "nccl", "deepspeed", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(False, "nccl", "fsdp2", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(True, "nccl", "fsdp2", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(False, "gloo", "fsdp", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(True, "gloo", "fsdp", "sglang", 1, marks=pytest.mark.sglang),
        ],
        ids=[
            "no_colocate_nccl_fsdp_vllm",
            "colocate_nccl_fsdp_vllm",
            "no_colocate_gloo_fsdp_vllm",
            "colocate_gloo_fsdp_vllm",
            "no_colocate_nccl_deepspeed_vllm",
            "colocate_nccl_deepspeed_vllm",
            "no_colocate_nccl_fsdp2_vllm",
            "colocate_nccl_fsdp2_vllm",
            "no_colocate_nccl_deepspeed_sglang",
            "colocate_nccl_deepspeed_sglang",
            "no_colocate_nccl_fsdp2_sglang",
            "colocate_nccl_fsdp2_sglang",
            "no_colocate_gloo_fsdp_sglang",
            "colocate_gloo_fsdp_sglang",
        ],
    )
    def test_policy_local_engines_e2e(colocate_all, weight_sync_backend, strategy, backend, tp_size):
        """
        Tests initalizing the policy actor group and inference engine, syncing weights, and performing generation.
        """
        try:
            cfg = get_test_actor_config()
            cfg.trainer.placement.colocate_all = colocate_all
            cfg.generator.weight_sync_backend = weight_sync_backend
            cfg.trainer.strategy = strategy
            cfg.generator.backend = backend
            cfg.generator.inference_engine_tensor_parallel_size = tp_size
    
            # If colocate is True, this will load the engine, sleep, and wake up the engine
>           client, pg = init_inference_engines(
                cfg=cfg,
                use_local=True,
                async_engine=cfg.generator.async_engine,
                tp_size=cfg.generator.inference_engine_tensor_parallel_size,
                colocate_all=cfg.trainer.placement.colocate_all,
                backend=backend,
            )

tests/gpu/test_policy_local_engines_e2e.py:138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/gpu/test_policy_local_engines_e2e.py:63: in init_inference_engines
    eps = create_ray_wrapped_inference_engines(
skyrl_train/inference_engines/ray_wrapped_inference_engine.py:207: in create_ray_wrapped_inference_engines
    ray.get(sleep_refs)
.venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py:22: in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py:104: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.12/site-packages/ray/_private/worker.py:2858: in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <ray._private.worker.Worker object at 0x7edbc65691f0>
object_refs = [ObjectRef(8c115ad83d29434cd5c808272af3e45b54efd9c90100000001000000)]
timeout = None, return_exceptions = False, skip_deserialization = False

    def get_objects(
        self,
        object_refs: list,
        timeout: Optional[float] = None,
        return_exceptions: bool = False,
        skip_deserialization: bool = False,
    ) -> Tuple[List[serialization.SerializedRayObject], bytes]:
        """Get the values in the object store associated with the IDs.
    
        Return the values from the local object store for object_refs. This
        will block until all the values for object_refs have been written to
        the local object store.
    
        Args:
            object_refs: A list of the object refs
                whose values should be retrieved.
            timeout: The maximum amount of time in
                seconds to wait before returning.
            return_exceptions: If any of the objects deserialize to an
                Exception object, whether to return them as values in the
                returned list. If False, then the first found exception will be
                raised.
            skip_deserialization: If true, only the buffer will be released and
                the object associated with the buffer will not be deserialized.
        Returns:
            list: List of deserialized objects or None if skip_deserialization is True.
            bytes: UUID of the debugger breakpoint we should drop
                into or b"" if there is no breakpoint.
        """
        # Make sure that the values are object refs.
        for object_ref in object_refs:
            if not isinstance(object_ref, ObjectRef):
                raise TypeError(
                    f"Attempting to call `get` on the value {object_ref}, "
                    "which is not an ray.ObjectRef."
                )
    
        timeout_ms = (
            int(timeout * 1000) if timeout is not None and timeout != -1 else -1
        )
        serialized_objects: List[
            serialization.SerializedRayObject
        ] = self.core_worker.get_objects(
            object_refs,
            timeout_ms,
        )
    
        debugger_breakpoint = b""
        for data, metadata, _ in serialized_objects:
            if metadata:
                metadata_fields = metadata.split(b",")
                if len(metadata_fields) >= 2 and metadata_fields[1].startswith(
                    ray_constants.OBJECT_METADATA_DEBUG_PREFIX
                ):
                    debugger_breakpoint = metadata_fields[1][
                        len(ray_constants.OBJECT_METADATA_DEBUG_PREFIX) :
                    ]
        if skip_deserialization:
            return None, debugger_breakpoint
    
        values = self.deserialize_objects(serialized_objects, object_refs)
        if not return_exceptions:
            # Raise exceptions instead of returning them to the user.
            for i, value in enumerate(values):
                if isinstance(value, RayError):
                    if isinstance(value, ray.exceptions.ObjectLostError):
                        global_worker.core_worker.dump_object_store_memory_usage()
                    if isinstance(value, RayTaskError):
                        raise value.as_instanceof_cause()
                    else:
>                       raise value
E                       ray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, [36mray::AsyncVLLMInferenceEngine.__init__()[39m (pid=83535, ip=172.18.0.2, actor_id=d5c808272af3e45b54efd9c901000000, repr=<skyrl_train.inference_engines.vllm.vllm_engine.AsyncVLLMInferenceEngine object at 0x7e07644e6900>)
E                         File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 456, in result
E                           return self.__get_result()
E                                  ^^^^^^^^^^^^^^^^^^^
E                         File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
E                           raise self._exception
E                                  ^^^^^^^^^^^^^^^^^^^^^
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                         File "/tmp/ray/session_2025-08-29_01-12-45_431675_5522/runtime_resources/working_dir_files/_ray_pkg_a1deeb22f7f0b8f8/skyrl_train/inference_engines/vllm/vllm_engine.py", line 168, in __init__
E                           self.llm = self._create_engine(*args, **kwargs)
E                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                         File "/tmp/ray/session_2025-08-29_01-12-45_431675_5522/runtime_resources/working_dir_files/_ray_pkg_a1deeb22f7f0b8f8/skyrl_train/inference_engines/vllm/vllm_engine.py", line 317, in _create_engine
E                           engine_args = vllm.AsyncEngineArgs(disable_log_requests=True, **kwargs)
E                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                       TypeError: AsyncEngineArgs.__init__() got an unexpected keyword argument 'disable_log_requests'

.venv/lib/python3.12/site-packages/ray/_private/worker.py:960: ActorDiedError
=============================== warnings summary ===============================
<frozen importlib._bootstrap>:488
  <frozen importlib._bootstrap>:488: DeprecationWarning: Type google._upb._message.MessageMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14.

<frozen importlib._bootstrap>:488
  <frozen importlib._bootstrap>:488: DeprecationWarning: Type google._upb._message.ScalarMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14.

tests/gpu/gpu_ci/test_engine_generation.py:244
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/gpu_ci/test_engine_generation.py:244: PytestUnknownMarkWarning: Unknown pytest.mark.vllm - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytest.param("vllm", 2, marks=pytest.mark.vllm),

tests/gpu/gpu_ci/test_engine_generation.py:246
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/gpu_ci/test_engine_generation.py:246: PytestUnknownMarkWarning: Unknown pytest.mark.sglang - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytest.param("sglang", 1, marks=pytest.mark.sglang),

tests/gpu/gpu_ci/test_engine_generation.py:336
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/gpu_ci/test_engine_generation.py:336: PytestUnknownMarkWarning: Unknown pytest.mark.vllm - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytest.param("vllm", 2, marks=pytest.mark.vllm),

tests/gpu/gpu_ci/test_engine_generation.py:338
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/gpu_ci/test_engine_generation.py:338: PytestUnknownMarkWarning: Unknown pytest.mark.sglang - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytest.param("sglang", 1, marks=pytest.mark.sglang),

tests/gpu/gpu_ci/test_skyrl_gym_generator.py:34
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/gpu_ci/test_skyrl_gym_generator.py:34: PytestCollectionWarning: cannot collect test class 'TestEnv' because it has a __init__ constructor (from: tests/gpu/gpu_ci/test_skyrl_gym_generator.py)
    class TestEnv(BaseTextEnv):

tests/gpu/test_grpo_sp_sanity.py:22
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/test_grpo_sp_sanity.py:22: PytestCollectionWarning: cannot collect test class 'TestExp' because it has a __init__ constructor (from: tests/gpu/test_grpo_sp_sanity.py)
    class TestExp(BasePPOExp):

tests/gpu/test_policy_local_engines_e2e.py:92
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/test_policy_local_engines_e2e.py:92: PytestUnknownMarkWarning: Unknown pytest.mark.vllm - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytest.param(False, "nccl", "fsdp", "vllm", 2, marks=pytest.mark.vllm),

tests/gpu/test_policy_local_engines_e2e.py:93
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/test_policy_local_engines_e2e.py:93: PytestUnknownMarkWarning: Unknown pytest.mark.vllm - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytest.param(True, "nccl", "fsdp", "vllm", 2, marks=pytest.mark.vllm),

tests/gpu/test_policy_local_engines_e2e.py:94
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/test_policy_local_engines_e2e.py:94: PytestUnknownMarkWarning: Unknown pytest.mark.vllm - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytest.param(False, "gloo", "fsdp", "vllm", 2, marks=pytest.mark.vllm),

tests/gpu/test_policy_local_engines_e2e.py:95
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/test_policy_local_engines_e2e.py:95: PytestUnknownMarkWarning: Unknown pytest.mark.vllm - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytest.param(True, "gloo", "fsdp", "vllm", 2, marks=pytest.mark.vllm),

tests/gpu/test_policy_local_engines_e2e.py:96
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/test_policy_local_engines_e2e.py:96: PytestUnknownMarkWarning: Unknown pytest.mark.vllm - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytest.param(False, "nccl", "deepspeed", "vllm", 2, marks=pytest.mark.vllm),

tests/gpu/test_policy_local_engines_e2e.py:97
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/test_policy_local_engines_e2e.py:97: PytestUnknownMarkWarning: Unknown pytest.mark.vllm - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytest.param(True, "nccl", "deepspeed", "vllm", 2, marks=pytest.mark.vllm),

tests/gpu/test_policy_local_engines_e2e.py:98
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/test_policy_local_engines_e2e.py:98: PytestUnknownMarkWarning: Unknown pytest.mark.vllm - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytest.param(False, "nccl", "fsdp2", "vllm", 2, marks=pytest.mark.vllm),

tests/gpu/test_policy_local_engines_e2e.py:99
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/test_policy_local_engines_e2e.py:99: PytestUnknownMarkWarning: Unknown pytest.mark.vllm - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytest.param(True, "nccl", "fsdp2", "vllm", 2, marks=pytest.mark.vllm),

tests/gpu/test_policy_local_engines_e2e.py:101
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/test_policy_local_engines_e2e.py:101: PytestUnknownMarkWarning: Unknown pytest.mark.sglang - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytest.param(False, "nccl", "deepspeed", "sglang", 1, marks=pytest.mark.sglang),

tests/gpu/test_policy_local_engines_e2e.py:102
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/test_policy_local_engines_e2e.py:102: PytestUnknownMarkWarning: Unknown pytest.mark.sglang - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytest.param(True, "nccl", "deepspeed", "sglang", 1, marks=pytest.mark.sglang),

tests/gpu/test_policy_local_engines_e2e.py:103
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/test_policy_local_engines_e2e.py:103: PytestUnknownMarkWarning: Unknown pytest.mark.sglang - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytest.param(False, "nccl", "fsdp2", "sglang", 1, marks=pytest.mark.sglang),

tests/gpu/test_policy_local_engines_e2e.py:104
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/test_policy_local_engines_e2e.py:104: PytestUnknownMarkWarning: Unknown pytest.mark.sglang - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytest.param(True, "nccl", "fsdp2", "sglang", 1, marks=pytest.mark.sglang),

tests/gpu/test_policy_local_engines_e2e.py:105
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/test_policy_local_engines_e2e.py:105: PytestUnknownMarkWarning: Unknown pytest.mark.sglang - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytest.param(False, "gloo", "fsdp", "sglang", 1, marks=pytest.mark.sglang),

tests/gpu/test_policy_local_engines_e2e.py:106
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/test_policy_local_engines_e2e.py:106: PytestUnknownMarkWarning: Unknown pytest.mark.sglang - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytest.param(True, "gloo", "fsdp", "sglang", 1, marks=pytest.mark.sglang),

tests/gpu/gpu_ci/test_engine_generation.py::test_inference_engines_generation[vllm]
tests/gpu/gpu_ci/test_engine_generation.py::test_token_based_generation[vllm]
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/gpu_ci/test_engine_generation.py:32: UserWarning: 
  The version_base parameter is not specified.
  Please specify a compatability version level, or None.
  Will assume defaults for version 1.1
    with hydra.initialize_config_dir(config_dir=config_dir):

tests/gpu/gpu_ci/test_engine_generation.py::test_inference_engines_generation[vllm]
tests/gpu/gpu_ci/test_engine_generation.py::test_token_based_generation[vllm]
  /root/tgriggs/SkyRL/skyrl-train/.venv/lib/python3.12/site-packages/datasets/utils/_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.
    obj.co_lnotab,  # for < python 3.10 [not counted in args]

tests/gpu/gpu_ci/test_engine_generation.py: 18 warnings
  /root/tgriggs/SkyRL/skyrl-train/.venv/lib/python3.12/site-packages/multiprocess/popen_fork.py:66: DeprecationWarning: This process (pid=5522) is multi-threaded, use of fork() may lead to deadlocks in the child.
    self.pid = os.fork()

tests/gpu/test_policy_local_engines_e2e.py::test_policy_local_engines_e2e[no_colocate_nccl_fsdp_vllm]
tests/gpu/test_policy_local_engines_e2e.py::test_policy_local_engines_e2e[colocate_nccl_fsdp_vllm]
tests/gpu/test_policy_local_engines_e2e.py::test_policy_local_engines_e2e[no_colocate_gloo_fsdp_vllm]
tests/gpu/test_policy_local_engines_e2e.py::test_policy_local_engines_e2e[colocate_gloo_fsdp_vllm]
tests/gpu/test_policy_local_engines_e2e.py::test_policy_local_engines_e2e[no_colocate_nccl_deepspeed_vllm]
tests/gpu/test_policy_local_engines_e2e.py::test_policy_local_engines_e2e[colocate_nccl_deepspeed_vllm]
tests/gpu/test_policy_local_engines_e2e.py::test_policy_local_engines_e2e[no_colocate_nccl_fsdp2_vllm]
tests/gpu/test_policy_local_engines_e2e.py::test_policy_local_engines_e2e[colocate_nccl_fsdp2_vllm]
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/test_policy_local_engines_e2e.py:32: UserWarning: 
  The version_base parameter is not specified.
  Please specify a compatability version level, or None.
  Will assume defaults for version 1.1
    with hydra.initialize_config_dir(config_dir=config_dir):

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED tests/gpu/gpu_ci/test_engine_generation.py::test_inference_engines_generation[vllm]
FAILED tests/gpu/gpu_ci/test_engine_generation.py::test_token_based_generation[vllm]
FAILED tests/gpu/test_policy_local_engines_e2e.py::test_policy_local_engines_e2e[no_colocate_nccl_fsdp_vllm]
FAILED tests/gpu/test_policy_local_engines_e2e.py::test_policy_local_engines_e2e[colocate_nccl_fsdp_vllm]
FAILED tests/gpu/test_policy_local_engines_e2e.py::test_policy_local_engines_e2e[no_colocate_gloo_fsdp_vllm]
FAILED tests/gpu/test_policy_local_engines_e2e.py::test_policy_local_engines_e2e[colocate_gloo_fsdp_vllm]
FAILED tests/gpu/test_policy_local_engines_e2e.py::test_policy_local_engines_e2e[no_colocate_nccl_deepspeed_vllm]
FAILED tests/gpu/test_policy_local_engines_e2e.py::test_policy_local_engines_e2e[colocate_nccl_deepspeed_vllm]
FAILED tests/gpu/test_policy_local_engines_e2e.py::test_policy_local_engines_e2e[no_colocate_nccl_fsdp2_vllm]
FAILED tests/gpu/test_policy_local_engines_e2e.py::test_policy_local_engines_e2e[colocate_nccl_fsdp2_vllm]
========== 10 failed, 64 deselected, 52 warnings in 332.06s (0:05:32) ==========
