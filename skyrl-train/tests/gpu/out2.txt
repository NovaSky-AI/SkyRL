nohup: ignoring input
============================= test session starts ==============================
platform linux -- Python 3.12.11, pytest-8.4.1, pluggy-1.6.0 -- /root/tgriggs/SkyRL/skyrl-train/.venv/bin/python3
cachedir: .pytest_cache
rootdir: /root/tgriggs/SkyRL/skyrl-train
configfile: pyproject.toml
plugins: jaxtyping-0.3.2, anyio-4.9.0, hydra-core-1.3.2, asyncio-1.0.0
asyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collecting ... collected 1 item

tests/gpu/test_policy_local_engines_e2e.py::test_policy_local_engines_e2e[no_colocate_nccl_fsdp_vllm] 2025-08-29 04:09:57.469 | INFO     | skyrl_train.utils.utils:prepare_runtime_environment:367 - `VLLM_USE_V1` is not specified, setting `VLLM_USE_V1` to 1. To override, set `VLLM_USE_V1` explicitly
2025-08-29 04:09:59,465	WARNING utils.py:426 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.
2025-08-29 04:09:59,465	WARNING utils.py:438 -- Ray currently does not support initializing Ray with fractional cpus. Your num_cpus will be truncated from 108.8 to 108.
2025-08-29 04:09:59,620	INFO worker.py:1927 -- Started a local Ray instance.
2025-08-29 04:09:59,722	INFO packaging.py:588 -- Creating a file package for local module '/root/tgriggs/SkyRL/skyrl-train'.
2025-08-29 04:09:59,820	INFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_018ae2a4554a8be9.zip' (3.31MiB) to Ray cluster...
2025-08-29 04:09:59,846	INFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_018ae2a4554a8be9.zip'.
[33m(raylet)[0m warning: `VIRTUAL_ENV=/root/tgriggs/SkyRL/skyrl-train/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead
[33m(raylet)[0m Using CPython 3.12.11
[33m(raylet)[0m Creating virtual environment at: .venv
[33m(raylet)[0m    Building skyrl-train @ file:///tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9
[33m(raylet)[0m    Building skyrl-gym @ file:///tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/skyrl-gym
[33m(raylet)[0m       Built skyrl-gym @ file:///tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/skyrl-gym
[33m(raylet)[0m       Built skyrl-train @ file:///tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9
[33m(raylet)[0m Installed 192 packages in 768ms
2025-08-29 04:10:06.463 | INFO     | skyrl_train.utils.ppo_utils:sync_registries:505 - Synced registries to ray actor
[33m(raylet)[0m warning: `VIRTUAL_ENV=/root/tgriggs/SkyRL/skyrl-train/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(pid=156541)[0m Using blocking ray.get inside async actor. This blocks the event loop. Please use `await` on object ref with asyncio.gather if you want to yield execution to the event loop instead.
[33m(raylet)[0m warning: `VIRTUAL_ENV=/root/tgriggs/SkyRL/skyrl-train/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead[32m [repeated 2x across cluster][0m
2025-08-29 04:10:21.491 | INFO     | skyrl_train.workers.worker:_initiate_actors:460 - Initializing process group for RayActorGroup
[36m(AsyncVLLMInferenceEngine pid=156418)[0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[36m(pid=157057)[0m Using blocking ray.get inside async actor. This blocks the event loop. Please use `await` on object ref with asyncio.gather if you want to yield execution to the event loop instead.
[33m(raylet)[0m warning: `VIRTUAL_ENV=/root/tgriggs/SkyRL/skyrl-train/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead
2025-08-29 04:10:28.009 | INFO     | skyrl_train.workers.worker:_initiate_actors:462 - Initialized process group for RayActorGroup
2025-08-29 04:10:28.014 | INFO     | skyrl_train.workers.worker:_initiate_actors:464 - Mesh Ranks: [MeshRank(dp=0, sp=0, tp=0, world_size=2, dp_size=2), MeshRank(dp=1, sp=0, tp=0, world_size=2, dp_size=2)]
[36m(FSDPPolicyRayActorBase pid=157057)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(FSDPPolicyRayActorBase pid=157057)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2Model is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(FSDPPolicyRayActorBase pid=157057)[0m /tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
[36m(FSDPPolicyRayActorBase pid=157057)[0m   warnings.warn(  # warn only once
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m 2025-08-29 04:10:37,315	INFO worker.py:1606 -- Using address 172.18.0.2:41496 set in the environment variable RAY_ADDRESS
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m 2025-08-29 04:10:37,327	INFO worker.py:1747 -- Connecting to existing Ray cluster at address: 172.18.0.2:41496...
[36m(FSDPPolicyRayActorBase pid=156541)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2Model is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`[32m [repeated 2x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m 2025-08-29 04:10:37,343	INFO worker.py:1927 -- Connected to Ray cluster.
[33m(raylet)[0m warning: `VIRTUAL_ENV=/root/tgriggs/SkyRL/skyrl-train/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead
InferenceEngineClient initialized with 1 engines.
[36m(AsyncVLLMInferenceEngine pid=156418)[0m creating LLM with bundle_indices=[0, 1]
[36m(AsyncVLLMInferenceEngine pid=156418)[0m INFO 08-29 04:10:19 [__init__.py:241] Automatically detected platform cuda.
[36m(AsyncVLLMInferenceEngine pid=156418)[0m INFO 08-29 04:10:30 [__init__.py:711] Resolved architecture: Qwen2ForCausalLM
[36m(AsyncVLLMInferenceEngine pid=156418)[0m INFO 08-29 04:10:30 [__init__.py:1750] Using max model len 1536
[36m(AsyncVLLMInferenceEngine pid=156418)[0m WARNING 08-29 04:10:30 [arg_utils.py:1775] Detected VLLM_USE_V1=1 with Engine in background thread. Usage should be considered experimental. Please report any issues on Github.
[36m(AsyncVLLMInferenceEngine pid=156418)[0m INFO 08-29 04:10:30 [arg_utils.py:1133] Using ray runtime env: {'py_executable': 'uv run --extra vllm --extra dev --', 'working_dir': 'gcs://_ray_pkg_018ae2a4554a8be9.zip', 'env_vars': {'NCCL_CUMEM_ENABLE': '0', 'VLLM_ALLOW_INSECURE_SERIALIZATION': '1', 'VLLM_DISABLE_COMPILE_CACHE': '1', 'VLLM_ENABLE_V1_MULTIPROCESSING': '0', 'VLLM_USE_V1': '1'}}
[36m(AsyncVLLMInferenceEngine pid=156418)[0m INFO 08-29 04:10:30 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.
[36m(AsyncVLLMInferenceEngine pid=156418)[0m INFO 08-29 04:10:30 [__init__.py:3565] Cudagraph is disabled under eager mode
[36m(AsyncVLLMInferenceEngine pid=156418)[0m WARNING 08-29 04:10:31 [serial_utils.py:48] Allowing insecure serialization using pickle due to VLLM_ALLOW_INSECURE_SERIALIZATION=1
[36m(AsyncVLLMInferenceEngine pid=156418)[0m WARNING 08-29 04:10:31 [__init__.py:2921] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: In a Ray actor and can only be spawned
[36m(AsyncVLLMInferenceEngine pid=156418)[0m INFO 08-29 04:10:36 [__init__.py:241] Automatically detected platform cuda.
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m INFO 08-29 04:10:37 [core.py:636] Waiting for init message from front-end.
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m INFO 08-29 04:10:37 [core.py:74] Initializing a V1 LLM engine (v0.10.1.1) with config: model='Qwen/Qwen2.5-0.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-0.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=1536, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=Qwen/Qwen2.5-0.5B-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m INFO 08-29 04:10:37 [ray_utils.py:318] Using the existing placement group
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m INFO 08-29 04:10:37 [ray_distributed_executor.py:169] use_ray_spmd_worker: True
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(pid=157374)[0m INFO 08-29 04:10:43 [__init__.py:241] Automatically detected platform cuda.
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m INFO 08-29 04:10:44 [ray_env.py:63] RAY_NON_CARRY_OVER_ENV_VARS from config: set()
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m INFO 08-29 04:10:44 [ray_env.py:65] Copying the following environment variables to workers: ['VLLM_USE_RAY_COMPILED_DAG', 'VLLM_RAY_PER_WORKER_GPUS', 'LD_LIBRARY_PATH', 'VLLM_WORKER_MULTIPROC_METHOD', 'VLLM_ENABLE_V1_MULTIPROCESSING', 'VLLM_USE_V1', 'VLLM_DISABLE_COMPILE_CACHE', 'VLLM_RAY_BUNDLE_INDICES', 'VLLM_USE_RAY_SPMD_WORKER', 'VLLM_ALLOW_INSECURE_SERIALIZATION']
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m INFO 08-29 04:10:44 [ray_env.py:68] If certain env vars should NOT be copied, add them to /root/.config/vllm/ray_non_carry_over_env_vars.json file
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(RayWorkerWrapper pid=157405)[0m INFO 08-29 04:10:47 [worker_base.py:587] Injected <class 'skyrl_train.inference_engines.vllm.vllm_engine.WorkerWrap'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['destroy_weights_update_group', 'init_weight_update_communicator', 'test_rpc', 'update_weights', 'update_weights_cuda_ipc']
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700] EngineCore failed to start.
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700] Traceback (most recent call last):
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in run_engine_core
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]     engine_core = EngineCoreProc(*args, **kwargs)
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 492, in __init__
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]     super().__init__(vllm_config, executor_class, log_stats,
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 80, in __init__
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]     self.model_executor = executor_class(vllm_config)
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 264, in __init__
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]     super().__init__(*args, **kwargs)
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 54, in __init__
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]     self._init_executor()
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/v1/executor/ray_distributed_executor.py", line 49, in _init_executor
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]     super()._init_executor()
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/executor/ray_distributed_executor.py", line 107, in _init_executor
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]     self._init_workers_ray(placement_group)
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/executor/ray_distributed_executor.py", line 377, in _init_workers_ray
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]     self._run_workers("init_device")
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/executor/ray_distributed_executor.py", line 503, in _run_workers
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]     ray_worker_outputs = ray.get(ray_worker_outputs)
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 22, in auto_init_wrapper
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]     return fn(*args, **kwargs)
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]            ^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 104, in wrapper
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]     return func(*args, **kwargs)
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]            ^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2858, in get
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/ray/_private/worker.py", line 958, in get_objects
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]     raise value.as_instanceof_cause()
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700] ray.exceptions.RayTaskError(ValueError): [36mray::RayWorkerWrapper.execute_method()[39m (pid=157405, ip=172.18.0.2, actor_id=6a05c7ceeffeb33a6b15ab8802000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x77eb08e42e70>)
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/worker/worker_base.py", line 620, in execute_method
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]     raise e
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/worker/worker_base.py", line 611, in execute_method
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]     return run_method(self, method, args, kwargs)
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/utils/__init__.py", line 3007, in run_method
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]     return func(*args, **kwargs)
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]            ^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/worker/worker_base.py", line 603, in init_device
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]     self.worker.init_device()  # type: ignore[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m Process EngineCore_0:
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m Traceback (most recent call last):
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m   File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m     self.run()
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m   File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/multiprocessing/process.py", line 108, in run
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m     self._target(*self._args, **self._kwargs)
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 704, in run_engine_core
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m     raise e
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in run_engine_core
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 492, in __init__
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m     super().__init__(vllm_config, executor_class, log_stats,
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 80, in __init__
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m     self.model_executor = executor_class(vllm_config)
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 264, in __init__
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m     super().__init__(*args, **kwargs)
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 54, in __init__
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m     self._init_executor()
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/v1/executor/ray_distributed_executor.py", line 49, in _init_executor
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m     super()._init_executor()
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/executor/ray_distributed_executor.py", line 107, in _init_executor
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m     self._init_workers_ray(placement_group)
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/executor/ray_distributed_executor.py", line 377, in _init_workers_ray
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m     self._run_workers("init_device")
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/executor/ray_distributed_executor.py", line 503, in _run_workers
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m     ray_worker_outputs = ray.get(ray_worker_outputs)
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 22, in auto_init_wrapper
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m     return fn(*args, **kwargs)
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m            ^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 104, in wrapper
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m     return func(*args, **kwargs)
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2858, in get
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/ray/_private/worker.py", line 958, in get_objects
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m     raise value.as_instanceof_cause()
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ray.exceptions.RayTaskError(ValueError): [36mray::RayWorkerWrapper.execute_method()[39m (pid=157405, ip=172.18.0.2, actor_id=6a05c7ceeffeb33a6b15ab8802000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x77eb08e42e70>)
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/worker/worker_base.py", line 620, in execute_method
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m     raise e
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/worker/worker_base.py", line 611, in execute_method
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m     return run_method(self, method, args, kwargs)
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/utils/__init__.py", line 3007, in run_method
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m     return func(*args, **kwargs)
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/worker/worker_base.py", line 603, in init_device
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m     self.worker.init_device()  # type: ignore
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 179, in init_device
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m     raise ValueError(
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ValueError: Free memory on device (14.58/79.25 GiB) on startup is less than desired GPU memory utilization (0.6, 47.55 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m 2025-08-29 04:10:48,529	ERROR worker.py:427 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::RayWorkerWrapper.execute_method()[39m (pid=157374, ip=172.18.0.2, actor_id=ab05b825a5a5d6cf0c0886dd02000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x70a5ff542cc0>)
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/worker/worker_base.py", line 620, in execute_method
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m     raise e
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/worker/worker_base.py", line 611, in execute_method
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m     return run_method(self, method, args, kwargs)
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/utils/__init__.py", line 3007, in run_method
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m     return func(*args, **kwargs)
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/worker/worker_base.py", line 603, in init_device
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m     self.worker.init_device()  # type: ignore
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 179, in init_device
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m     raise ValueError(
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ValueError: Free memory on device (14.58/79.25 GiB) on startup is less than desired GPU memory utilization (0.6, 47.55 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [33m(raylet)[0m warning: `VIRTUAL_ENV=/root/tgriggs/SkyRL/skyrl-train/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead[32m [repeated 2x across cluster][0m

[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 179, in init_device
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700]     raise ValueError(
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m ERROR 08-29 04:10:48 [core.py:700] ValueError: Free memory on device (14.58/79.25 GiB) on startup is less than desired GPU memory utilization (0.6, 47.55 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m INFO 08-29 04:10:48 [ray_distributed_executor.py:120] Shutting down Ray distributed executor. If you see error log from logging.cc regarding SIGTERM received, please ignore because this is the expected termination process in Ray.
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(RayWorkerWrapper pid=157374)[0m ERROR 08-29 04:10:48 [worker_base.py:619] Error executing method 'init_device'. This might cause deadlock in distributed execution.
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(RayWorkerWrapper pid=157374)[0m ERROR 08-29 04:10:48 [worker_base.py:619] Traceback (most recent call last):
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(RayWorkerWrapper pid=157374)[0m ERROR 08-29 04:10:48 [worker_base.py:619]   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/worker/worker_base.py", line 611, in execute_method
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(RayWorkerWrapper pid=157374)[0m ERROR 08-29 04:10:48 [worker_base.py:619]     return run_method(self, method, args, kwargs)
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(RayWorkerWrapper pid=157374)[0m ERROR 08-29 04:10:48 [worker_base.py:619]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(RayWorkerWrapper pid=157374)[0m ERROR 08-29 04:10:48 [worker_base.py:619]   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/utils/__init__.py", line 3007, in run_method
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(RayWorkerWrapper pid=157374)[0m ERROR 08-29 04:10:48 [worker_base.py:619]     return func(*args, **kwargs)
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(RayWorkerWrapper pid=157374)[0m ERROR 08-29 04:10:48 [worker_base.py:619]            ^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(RayWorkerWrapper pid=157374)[0m ERROR 08-29 04:10:48 [worker_base.py:619]   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py", line 461, in _resume_span
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(RayWorkerWrapper pid=157374)[0m ERROR 08-29 04:10:48 [worker_base.py:619]     return method(self, *_args, **_kwargs)
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(RayWorkerWrapper pid=157374)[0m ERROR 08-29 04:10:48 [worker_base.py:619]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(RayWorkerWrapper pid=157374)[0m ERROR 08-29 04:10:48 [worker_base.py:619]   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/worker/worker_base.py", line 603, in init_device
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(RayWorkerWrapper pid=157374)[0m ERROR 08-29 04:10:48 [worker_base.py:619]     self.worker.init_device()  # type: ignore
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(RayWorkerWrapper pid=157374)[0m ERROR 08-29 04:10:48 [worker_base.py:619]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(RayWorkerWrapper pid=157374)[0m ERROR 08-29 04:10:48 [worker_base.py:619]   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 179, in init_device
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(RayWorkerWrapper pid=157374)[0m ERROR 08-29 04:10:48 [worker_base.py:619]     raise ValueError(
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(RayWorkerWrapper pid=157374)[0m ERROR 08-29 04:10:48 [worker_base.py:619] ValueError: Free memory on device (14.58/79.25 GiB) on startup is less than desired GPU memory utilization (0.6, 47.55 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(pid=157405)[0m INFO 08-29 04:10:43 [__init__.py:241] Automatically detected platform cuda.
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(RayWorkerWrapper pid=157374)[0m INFO 08-29 04:10:47 [worker_base.py:587] Injected <class 'skyrl_train.inference_engines.vllm.vllm_engine.WorkerWrap'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['destroy_weights_update_group', 'init_weight_update_communicator', 'test_rpc', 'update_weights', 'update_weights_cuda_ipc']
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(RayWorkerWrapper pid=157405)[0m ERROR 08-29 04:10:48 [worker_base.py:619] Error executing method 'init_device'. This might cause deadlock in distributed execution.
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(RayWorkerWrapper pid=157405)[0m ERROR 08-29 04:10:48 [worker_base.py:619] Traceback (most recent call last):
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(RayWorkerWrapper pid=157405)[0m ERROR 08-29 04:10:48 [worker_base.py:619]   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/worker/worker_base.py", line 611, in execute_method
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(RayWorkerWrapper pid=157405)[0m ERROR 08-29 04:10:48 [worker_base.py:619]     return run_method(self, method, args, kwargs)
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(RayWorkerWrapper pid=157405)[0m ERROR 08-29 04:10:48 [worker_base.py:619]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(RayWorkerWrapper pid=157405)[0m ERROR 08-29 04:10:48 [worker_base.py:619]   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/utils/__init__.py", line 3007, in run_method
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(RayWorkerWrapper pid=157405)[0m ERROR 08-29 04:10:48 [worker_base.py:619]     return func(*args, **kwargs)[36m(AsyncVLLMInferenceEngine pid=156418)[0m Exception raised in creation task: The actor died because of an error raised in its creation task, [36mray::AsyncVLLMInferenceEngine.__init__()[39m (pid=156418, ip=172.18.0.2, actor_id=c07edbb41ae7a4de2a9cc55d01000000, repr=<skyrl_train.inference_engines.vllm.vllm_engine.AsyncVLLMInferenceEngine object at 0x703081617b90>)
[36m(AsyncVLLMInferenceEngine pid=156418)[0m   File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 456, in result
[36m(AsyncVLLMInferenceEngine pid=156418)[0m     return self.__get_result()
[36m(AsyncVLLMInferenceEngine pid=156418)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m   File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
[36m(AsyncVLLMInferenceEngine pid=156418)[0m     raise self._exception
[36m(AsyncVLLMInferenceEngine pid=156418)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/skyrl_train/inference_engines/vllm/vllm_engine.py", line 168, in __init__
[36m(AsyncVLLMInferenceEngine pid=156418)[0m     self.llm = self._create_engine(*args, **kwargs)
[36m(AsyncVLLMInferenceEngine pid=156418)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/skyrl_train/inference_engines/vllm/vllm_engine.py", line 318, in _create_engine
[36m(AsyncVLLMInferenceEngine pid=156418)[0m     return vllm.AsyncLLMEngine.from_engine_args(engine_args)
[36m(AsyncVLLMInferenceEngine pid=156418)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 202, in from_engine_args
[36m(AsyncVLLMInferenceEngine pid=156418)[0m     return cls(
[36m(AsyncVLLMInferenceEngine pid=156418)[0m            ^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 120, in __init__
[36m(AsyncVLLMInferenceEngine pid=156418)[0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[36m(AsyncVLLMInferenceEngine pid=156418)[0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 102, in make_async_mp_client
[36m(AsyncVLLMInferenceEngine pid=156418)[0m     return AsyncMPClient(*client_args)
[36m(AsyncVLLMInferenceEngine pid=156418)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 767, in __init__
[36m(AsyncVLLMInferenceEngine pid=156418)[0m     super().__init__(
[36m(AsyncVLLMInferenceEngine pid=156418)[0m   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 446, in __init__
[36m(AsyncVLLMInferenceEngine pid=156418)[0m     with launch_core_engines(vllm_config, executor_class,
[36m(AsyncVLLMInferenceEngine pid=156418)[0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m   File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/contextlib.py", line 144, in __exit__
[36m(AsyncVLLMInferenceEngine pid=156418)[0m     next(self.gen)
[36m(AsyncVLLMInferenceEngine pid=156418)[0m   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 706, in launch_core_engines
[36m(AsyncVLLMInferenceEngine pid=156418)[0m     wait_for_engine_startup(
[36m(AsyncVLLMInferenceEngine pid=156418)[0m   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 759, in wait_for_engine_startup
[36m(AsyncVLLMInferenceEngine pid=156418)[0m     raise RuntimeError("Engine core initialization failed. "
[36m(AsyncVLLMInferenceEngine pid=156418)[0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [33m(raylet)[0m warning: `VIRTUAL_ENV=/root/tgriggs/SkyRL/skyrl-train/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead

[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(RayWorkerWrapper pid=157405)[0m ERROR 08-29 04:10:48 [worker_base.py:619]            ^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(RayWorkerWrapper pid=157405)[0m ERROR 08-29 04:10:48 [worker_base.py:619]   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py", line 461, in _resume_span
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(RayWorkerWrapper pid=157405)[0m ERROR 08-29 04:10:48 [worker_base.py:619]     return method(self, *_args, **_kwargs)
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(RayWorkerWrapper pid=157405)[0m ERROR 08-29 04:10:48 [worker_base.py:619]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(RayWorkerWrapper pid=157405)[0m ERROR 08-29 04:10:48 [worker_base.py:619]   File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 179, in init_device[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(RayWorkerWrapper pid=157405)[0m ERROR 08-29 04:10:48 [worker_base.py:619]     self.worker.init_device()  # type: ignore
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(RayWorkerWrapper pid=157405)[0m ERROR 08-29 04:10:48 [worker_base.py:619]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(RayWorkerWrapper pid=157405)[0m ERROR 08-29 04:10:48 [worker_base.py:619]     raise ValueError(
[36m(AsyncVLLMInferenceEngine pid=156418)[0m [1;36m(EngineCore_0 pid=157288)[0;0m [36m(RayWorkerWrapper pid=157405)[0m ERROR 08-29 04:10:48 [worker_base.py:619] ValueError: Free memory on device (14.58/79.25 GiB) on startup is less than desired GPU memory utilization (0.6, 47.55 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
FAILED

=================================== FAILURES ===================================
__________ test_policy_local_engines_e2e[no_colocate_nccl_fsdp_vllm] ___________

colocate_all = False, weight_sync_backend = 'nccl', strategy = 'fsdp'
backend = 'vllm', tp_size = 2

    @pytest.mark.parametrize(
        ("colocate_all", "weight_sync_backend", "strategy", "backend", "tp_size"),
        [
            pytest.param(False, "nccl", "fsdp", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(True, "nccl", "fsdp", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(False, "gloo", "fsdp", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(True, "gloo", "fsdp", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(False, "nccl", "deepspeed", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(True, "nccl", "deepspeed", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(False, "nccl", "fsdp2", "vllm", 2, marks=pytest.mark.vllm),
            pytest.param(True, "nccl", "fsdp2", "vllm", 2, marks=pytest.mark.vllm),
            # TODO(Charlie): add TP > 1 tests for sglang when we support it
            pytest.param(False, "nccl", "deepspeed", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(True, "nccl", "deepspeed", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(False, "nccl", "fsdp2", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(True, "nccl", "fsdp2", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(False, "gloo", "fsdp", "sglang", 1, marks=pytest.mark.sglang),
            pytest.param(True, "gloo", "fsdp", "sglang", 1, marks=pytest.mark.sglang),
        ],
        ids=[
            "no_colocate_nccl_fsdp_vllm",
            "colocate_nccl_fsdp_vllm",
            "no_colocate_gloo_fsdp_vllm",
            "colocate_gloo_fsdp_vllm",
            "no_colocate_nccl_deepspeed_vllm",
            "colocate_nccl_deepspeed_vllm",
            "no_colocate_nccl_fsdp2_vllm",
            "colocate_nccl_fsdp2_vllm",
            "no_colocate_nccl_deepspeed_sglang",
            "colocate_nccl_deepspeed_sglang",
            "no_colocate_nccl_fsdp2_sglang",
            "colocate_nccl_fsdp2_sglang",
            "no_colocate_gloo_fsdp_sglang",
            "colocate_gloo_fsdp_sglang",
        ],
    )
    def test_policy_local_engines_e2e(colocate_all, weight_sync_backend, strategy, backend, tp_size):
        """
        Tests initalizing the policy actor group and inference engine, syncing weights, and performing generation.
        """
        try:
            cfg = get_test_actor_config()
            cfg.trainer.placement.colocate_all = colocate_all
            cfg.generator.weight_sync_backend = weight_sync_backend
            cfg.trainer.strategy = strategy
            cfg.generator.backend = backend
            cfg.generator.inference_engine_tensor_parallel_size = tp_size
    
            # If colocate is True, this will load the engine, sleep, and wake up the engine
            client, pg = init_inference_engines(
                cfg=cfg,
                use_local=True,
                async_engine=cfg.generator.async_engine,
                tp_size=cfg.generator.inference_engine_tensor_parallel_size,
                colocate_all=cfg.trainer.placement.colocate_all,
                backend=backend,
            )
    
            policy = init_worker_with_type(
                "policy",
                shared_pg=pg,
                colocate_all=cfg.trainer.placement.colocate_all,
                num_gpus_per_node=cfg.generator.inference_engine_tensor_parallel_size,
                cfg=cfg,
            )
>           ray.get(policy.async_run_ray_method("pass_through", "init_weight_sync_state", client))

tests/gpu/test_policy_local_engines_e2e.py:154: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
.venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py:22: in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py:104: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.12/site-packages/ray/_private/worker.py:2858: in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <ray._private.worker.Worker object at 0x7027be78ff20>
object_refs = [ObjectRef(3d3e27c54ed1f5cfaa6dd6fa318a1c53b3075a9f0100000001000000), ObjectRef(cae5e964086715a4219fe5266efa5aaa7c98be0d0100000001000000)]
timeout = None, return_exceptions = False, skip_deserialization = False

    def get_objects(
        self,
        object_refs: list,
        timeout: Optional[float] = None,
        return_exceptions: bool = False,
        skip_deserialization: bool = False,
    ) -> Tuple[List[serialization.SerializedRayObject], bytes]:
        """Get the values in the object store associated with the IDs.
    
        Return the values from the local object store for object_refs. This
        will block until all the values for object_refs have been written to
        the local object store.
    
        Args:
            object_refs: A list of the object refs
                whose values should be retrieved.
            timeout: The maximum amount of time in
                seconds to wait before returning.
            return_exceptions: If any of the objects deserialize to an
                Exception object, whether to return them as values in the
                returned list. If False, then the first found exception will be
                raised.
            skip_deserialization: If true, only the buffer will be released and
                the object associated with the buffer will not be deserialized.
        Returns:
            list: List of deserialized objects or None if skip_deserialization is True.
            bytes: UUID of the debugger breakpoint we should drop
                into or b"" if there is no breakpoint.
        """
        # Make sure that the values are object refs.
        for object_ref in object_refs:
            if not isinstance(object_ref, ObjectRef):
                raise TypeError(
                    f"Attempting to call `get` on the value {object_ref}, "
                    "which is not an ray.ObjectRef."
                )
    
        timeout_ms = (
            int(timeout * 1000) if timeout is not None and timeout != -1 else -1
        )
        serialized_objects: List[
            serialization.SerializedRayObject
        ] = self.core_worker.get_objects(
            object_refs,
            timeout_ms,
        )
    
        debugger_breakpoint = b""
        for data, metadata, _ in serialized_objects:
            if metadata:
                metadata_fields = metadata.split(b",")
                if len(metadata_fields) >= 2 and metadata_fields[1].startswith(
                    ray_constants.OBJECT_METADATA_DEBUG_PREFIX
                ):
                    debugger_breakpoint = metadata_fields[1][
                        len(ray_constants.OBJECT_METADATA_DEBUG_PREFIX) :
                    ]
        if skip_deserialization:
            return None, debugger_breakpoint
    
        values = self.deserialize_objects(serialized_objects, object_refs)
        if not return_exceptions:
            # Raise exceptions instead of returning them to the user.
            for i, value in enumerate(values):
                if isinstance(value, RayError):
                    if isinstance(value, ray.exceptions.ObjectLostError):
                        global_worker.core_worker.dump_object_store_memory_usage()
                    if isinstance(value, RayTaskError):
>                       raise value.as_instanceof_cause()
E                       ray.exceptions.RayTaskError(ActorDiedError): [36mray::FSDPPolicyRayActorBase.init_weight_sync_state()[39m (pid=156541, ip=172.18.0.2, actor_id=aa6dd6fa318a1c53b3075a9f01000000, repr=<skyrl_train.workers.fsdp.fsdp_worker.FSDPPolicyRayActorBase object at 0x72cc85692600>)
E                         File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 449, in result
E                           return self.__get_result()
E                                  ^^^^^^^^^^^^^^^^^^^
E                         File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
E                           raise self._exception
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                         File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/skyrl_train/workers/worker.py", line 283, in init_weight_sync_state
E                           results = await asyncio.gather(*tasks)
E                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                         File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/skyrl_train/inference_engines/inference_engine_client.py", line 177, in init_weight_update_communicator
E                           assert engine.tp_size is not None, "Engine must have a tp_size"
E                                  ^^^^^^^^^^^^^^
E                         File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/skyrl_train/inference_engines/ray_wrapped_inference_engine.py", line 26, in tp_size
E                           return ray.get(self.inference_engine_actor.tp_size.remote())
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                                  ^^^^^^^^^^^^^^^^^^^
E                                  ^^^^^^^^^^^^^^^^^^^^^
E                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                       ray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, [36mray::AsyncVLLMInferenceEngine.__init__()[39m (pid=156418, ip=172.18.0.2, actor_id=c07edbb41ae7a4de2a9cc55d01000000, repr=<skyrl_train.inference_engines.vllm.vllm_engine.AsyncVLLMInferenceEngine object at 0x703081617b90>)
E                         File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 456, in result
E                           return self.__get_result()
E                                  ^^^^^^^^^^^^^^^^^^^
E                         File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
E                           raise self._exception
E                                  ^^^^^^^^^^^^^^^^^^^^^
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                         File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/skyrl_train/inference_engines/vllm/vllm_engine.py", line 168, in __init__
E                           self.llm = self._create_engine(*args, **kwargs)
E                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                         File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/skyrl_train/inference_engines/vllm/vllm_engine.py", line 318, in _create_engine
E                           return vllm.AsyncLLMEngine.from_engine_args(engine_args)
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                         File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 202, in from_engine_args
E                           return cls(
E                                  ^^^^
E                         File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 120, in __init__
E                           self.engine_core = EngineCoreClient.make_async_mp_client(
E                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                         File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 102, in make_async_mp_client
E                           return AsyncMPClient(*client_args)
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                         File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 767, in __init__
E                           super().__init__(
E                         File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 446, in __init__
E                           with launch_core_engines(vllm_config, executor_class,
E                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                         File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/contextlib.py", line 144, in __exit__
E                           next(self.gen)
E                         File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 706, in launch_core_engines
E                           wait_for_engine_startup(
E                         File "/tmp/ray/session_2025-08-29_04-09-57_541791_148592/runtime_resources/working_dir_files/_ray_pkg_018ae2a4554a8be9/.venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 759, in wait_for_engine_startup
E                           raise RuntimeError("Engine core initialization failed. "
E                       RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

.venv/lib/python3.12/site-packages/ray/_private/worker.py:958: RayTaskError(ActorDiedError)
=============================== warnings summary ===============================
<frozen importlib._bootstrap>:488
  <frozen importlib._bootstrap>:488: DeprecationWarning: Type google._upb._message.MessageMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14.

<frozen importlib._bootstrap>:488
  <frozen importlib._bootstrap>:488: DeprecationWarning: Type google._upb._message.ScalarMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14.

tests/gpu/test_policy_local_engines_e2e.py:92
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/test_policy_local_engines_e2e.py:92: PytestUnknownMarkWarning: Unknown pytest.mark.vllm - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytest.param(False, "nccl", "fsdp", "vllm", 2, marks=pytest.mark.vllm),

tests/gpu/test_policy_local_engines_e2e.py:93
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/test_policy_local_engines_e2e.py:93: PytestUnknownMarkWarning: Unknown pytest.mark.vllm - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytest.param(True, "nccl", "fsdp", "vllm", 2, marks=pytest.mark.vllm),

tests/gpu/test_policy_local_engines_e2e.py:94
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/test_policy_local_engines_e2e.py:94: PytestUnknownMarkWarning: Unknown pytest.mark.vllm - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytest.param(False, "gloo", "fsdp", "vllm", 2, marks=pytest.mark.vllm),

tests/gpu/test_policy_local_engines_e2e.py:95
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/test_policy_local_engines_e2e.py:95: PytestUnknownMarkWarning: Unknown pytest.mark.vllm - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytest.param(True, "gloo", "fsdp", "vllm", 2, marks=pytest.mark.vllm),

tests/gpu/test_policy_local_engines_e2e.py:96
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/test_policy_local_engines_e2e.py:96: PytestUnknownMarkWarning: Unknown pytest.mark.vllm - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytest.param(False, "nccl", "deepspeed", "vllm", 2, marks=pytest.mark.vllm),

tests/gpu/test_policy_local_engines_e2e.py:97
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/test_policy_local_engines_e2e.py:97: PytestUnknownMarkWarning: Unknown pytest.mark.vllm - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytest.param(True, "nccl", "deepspeed", "vllm", 2, marks=pytest.mark.vllm),

tests/gpu/test_policy_local_engines_e2e.py:98
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/test_policy_local_engines_e2e.py:98: PytestUnknownMarkWarning: Unknown pytest.mark.vllm - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytest.param(False, "nccl", "fsdp2", "vllm", 2, marks=pytest.mark.vllm),

tests/gpu/test_policy_local_engines_e2e.py:99
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/test_policy_local_engines_e2e.py:99: PytestUnknownMarkWarning: Unknown pytest.mark.vllm - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytest.param(True, "nccl", "fsdp2", "vllm", 2, marks=pytest.mark.vllm),

tests/gpu/test_policy_local_engines_e2e.py:101
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/test_policy_local_engines_e2e.py:101: PytestUnknownMarkWarning: Unknown pytest.mark.sglang - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytest.param(False, "nccl", "deepspeed", "sglang", 1, marks=pytest.mark.sglang),

tests/gpu/test_policy_local_engines_e2e.py:102
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/test_policy_local_engines_e2e.py:102: PytestUnknownMarkWarning: Unknown pytest.mark.sglang - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytest.param(True, "nccl", "deepspeed", "sglang", 1, marks=pytest.mark.sglang),

tests/gpu/test_policy_local_engines_e2e.py:103
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/test_policy_local_engines_e2e.py:103: PytestUnknownMarkWarning: Unknown pytest.mark.sglang - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytest.param(False, "nccl", "fsdp2", "sglang", 1, marks=pytest.mark.sglang),

tests/gpu/test_policy_local_engines_e2e.py:104
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/test_policy_local_engines_e2e.py:104: PytestUnknownMarkWarning: Unknown pytest.mark.sglang - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytest.param(True, "nccl", "fsdp2", "sglang", 1, marks=pytest.mark.sglang),

tests/gpu/test_policy_local_engines_e2e.py:105
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/test_policy_local_engines_e2e.py:105: PytestUnknownMarkWarning: Unknown pytest.mark.sglang - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytest.param(False, "gloo", "fsdp", "sglang", 1, marks=pytest.mark.sglang),

tests/gpu/test_policy_local_engines_e2e.py:106
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/test_policy_local_engines_e2e.py:106: PytestUnknownMarkWarning: Unknown pytest.mark.sglang - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytest.param(True, "gloo", "fsdp", "sglang", 1, marks=pytest.mark.sglang),

tests/gpu/test_policy_local_engines_e2e.py::test_policy_local_engines_e2e[no_colocate_nccl_fsdp_vllm]
  /root/tgriggs/SkyRL/skyrl-train/tests/gpu/test_policy_local_engines_e2e.py:32: UserWarning: 
  The version_base parameter is not specified.
  Please specify a compatability version level, or None.
  Will assume defaults for version 1.1
    with hydra.initialize_config_dir(config_dir=config_dir):

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED tests/gpu/test_policy_local_engines_e2e.py::test_policy_local_engines_e2e[no_colocate_nccl_fsdp_vllm]
======================= 1 failed, 17 warnings in 55.82s ========================
