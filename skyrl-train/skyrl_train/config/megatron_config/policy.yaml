# @package megatron_config.policy
tensor_model_parallel_size: 1
pipeline_model_parallel_size: 1
context_parallel_size: 1
expert_model_parallel_size: 1
expert_tensor_parallel_size: 1

# Allow to override Distributed Data Parallel (DDP) config
# TODO (erictang000): understand exactly what is going on here
override_ddp_config:
  grad_reduce_in_fp32: false
  overlap_grad_reduce: false
  overlap_param_gather: false
  average_in_collective: true
  data_parallel_sharding_strategy: "optim_grads_params"

seed: 42

override_model_config: {}

# additional transformer config like: num_layers_in_first(/last)_pipeline_stage
# oc.select: default val for ref.megatron.override_transformer_config
# NOTE (erictang000) these are
override_transformer_config:
  # Recompute configuration, same as in megatron.training.arguments
  # default use minimal performance-interference recompute methods
  # Recompute granualarity, choices: ["full", "selective"]
  recompute_granularity: null

  # Recompute modules, multiple choices: ["core_attn", "moe_act", "layernorm", "mla_up_proj", "mlp", "moe"]
  # Please use correct module in matched model
  recompute_modules: ["core_attn"]

  # 'uniform', 'block'
  # 'uniform' divides the total number of transformer layers and checkpoints the input activation of each chunk
  # 'block' checkpoints the specified number of layers per pipeline stage at the specified granularity
  recompute_method: null

  # 'full' will checkpoint the entire transformer layer and 'selective' only checkpoints memory intensive part of attention
  recompute_num_layers: null