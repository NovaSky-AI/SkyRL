data:
  train_data:
  - /mnt/shared_storage/datasets/r2e-1000/train.parquet
  val_data:
  - /mnt/shared_storage/datasets/r2e-1000/validation.parquet
trainer:
  placement:
    colocate_all: true
    colocate_policy_ref: true
    colocate_critic_reward: false
    policy_num_nodes: 2
    policy_num_gpus_per_node: 8
    critic_num_nodes: 1
    critic_num_gpus_per_node: 4
    ref_num_nodes: 2
    ref_num_gpus_per_node: 8
    reward_num_nodes: 1
    reward_num_gpus_per_node: 4
  sequence_parallel_backend: ulysses
  strategy: fsdp2
  policy:
    model:
      path: Qwen/Qwen3-32B
    deepspeed_config: ${deepspeed_config.train}
    optimizer_config:
      lr: 1.0e-06
      adam_betas:
      - 0.9
      - 0.999
      weight_decay: 0.01
      max_grad_norm: 1.0
      offload_after_step: true
      num_warmup_steps: 0
    fsdp_config:
      cpu_offload: true
      reshard_after_forward: true
      fsdp_size: -1
    sequence_parallel_size: 4
    use_torch_compile: false
    record_memory: false
  ref:
    sequence_parallel_size: 1
    deepspeed_config: ${deepspeed_config.eval}
    fsdp_config:
      cpu_offload: true
      reshard_after_forward: true
      fsdp_size: -1
  critic:
    model:
      path: null
    deepspeed_config: ${deepspeed_config.train}
    optimizer_config:
      lr: 5.0e-06
      adam_betas:
      - 0.9
      - 0.999
      weight_decay: 0.01
      max_grad_norm: 1.0
      offload_after_step: true
      num_warmup_steps: 0
    fsdp_config:
      cpu_offload: false
      reshard_after_forward: true
      fsdp_size: -1
    sequence_parallel_size: 1
  reward:
    model:
      path: null
    deepspeed_config: ${deepspeed_config.eval}
    fsdp_config:
      cpu_offload: true
      reshard_after_forward: true
      fsdp_size: -1
    sequence_parallel_size: 1
  algorithm:
    advantage_estimator: grpo
    kl_target: null
    init_kl_coef: 0.0
    use_kl_estimator_k3: true
    use_abs_kl: false
    use_kl_in_reward: false
    use_kl_loss: false
    kl_loss_coef: 0
    advantage_batch_normalize: false
    value_head_prefix: value_head
    policy_loss_type: dual_clip
    loss_reduction: token_mean
    lambd: 1.0
    gamma: 1.0
    eps_clip_low: 0.2
    eps_clip_high: 0.28
    clip_ratio_c: 3.0
    value_clip: 0.2
    normalize_reward: true
  gradient_checkpointing: true
  gradient_checkpointing_use_reentrant: false
  seed: 42
  resume_mode: latest
  resume_path: null
  ckpt_path: /mnt/shared_storage/ckpts/skyagent-32b-r2e-1000-acc-thinking-mini4/
  max_ckpts_to_keep: 10
  ckpt_interval: 1
  hf_save_interval: 10
  export_path: ${oc.env:HOME}/exports/
  bf16: true
  epochs: 100
  update_epochs_per_batch: 1
  train_batch_size: 32
  policy_mini_batch_size: 4
  critic_mini_batch_size: 256
  micro_train_batch_size_per_gpu: 1
  micro_forward_batch_size_per_gpu: 1
  update_ref_every_epoch: false
  use_sample_packing: true
  eval_batch_size: 256
  eval_before_train: false
  eval_interval: 1000
  max_prompt_length: 31232
  flash_attn: true
  disable_fast_tokenizer: false
  target_modules: all-linear
  use_orm_score: false
  project_name: skyagent-32b-r2e
  run_name: skyagent-32b-r2e-1000-acc-thinking-mini4
  logger: wandb
  dump_data_batch: false
  dump_eval_results: true
generator:
  model_dtype: bfloat16
  run_engines_locally: true
  num_inference_engines: 4
  backend: vllm
  weight_sync_backend: nccl
  inference_engine_tensor_parallel_size: 4
  n_samples_per_prompt: 8
  async_engine: true
  batched: false
  max_input_length: 31232
  vllm_v1_disable_multiproc: true
  enable_prefix_caching: true
  enable_chunked_prefill: true
  max_num_batched_tokens: 32768
  enforce_eager: false
  gpu_memory_utilization: 0.8
  max_num_seqs: 1024
  remote_inference_engine_urls:
  - 127.0.0.1:8001
  max_turns: 50
  override_existing_update_group: auto
  sampling_params:
    max_generate_length: 2000
    temperature: 1
    top_p: 0.95
    min_p: 0.0
    top_k: -1
  use_conversation_multi_turn: true
  eval_sampling_params:
    max_generate_length: ${generator.sampling_params.max_generate_length}
    temperature: 0.0
    top_p: 1.0
    min_p: 0.0
    top_k: -1
  eval_n_samples_per_prompt: 1
  zero_reward_on_non_stop: false
  apply_overlong_filtering: false
  task: ./examples/run_skyrl/skyrl_oh.yaml
environment:
  env_class: null
  skyrl_gym:
    max_env_workers: 32
    text2sql:
      db_path: /home/ray/default/sql_data
    llm_as_a_judge:
      model: gpt-4o-mini
      base_url: null
    search:
      log_requests: false
      search_url: http://127.0.0.1:8000/retrieve
      topk: 3
      timeout: 30
deepspeed_config:
  train:
    zero_optimization:
      stage: 3
      offload_param:
        device: none
      offload_optimizer:
        device: none
        pin_memory: true
      sub_group_size: auto
      reduce_bucket_size: auto
      stage3_param_persistence_threshold: auto
      stage3_prefetch_bucket_size: auto
      stage3_max_live_parameters: auto
      stage3_max_reuse_distance: auto
      round_robin_gradients: true
      zero_hpz_partition_size: 1
      zero_quantized_weights: false
      zero_quantized_gradients: false
    disable_trace_cache: false
    data_types:
      grad_accum_dtype: fp32
    gradient_clipping: 1.0
    wall_clock_breakdown: false
    prescale_gradient: false
  eval:
    zero_optimization:
      stage: 3
      stage3_param_persistence_threshold: auto
      offload_param:
        device: cpu
        pin_memory: true
    gradient_clipping: 1.0
    prescale_gradient: false
    wall_clock_breakdown: false
