---
title: "Generator"
description: "Generator API - The Generator generates trajectories for training."
---


## Core APIs

### <span className="bg-blue-100 text-blue-800 dark:bg-blue-900 dark:text-blue-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">class</span> `GeneratorInterface`

Bases: ABC

**Functions:**

Name | Description
---- | -----------
[`generate`](#method-abstractmethod-async-generate) | Generate trajectories for the input batch.

#### <span className="bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">method</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">async</span> `generate`

```python
generate(input_batch: GeneratorInput) -> GeneratorOutput
```

Generate trajectories for the input batch.

Returns outputs in the same order as the input batch.

**Parameters:**

Name | Type | Description | Default
---- | ---- | ----------- | -------
`input_batch` | [GeneratorInput](/docs/api-ref/skyrl/skyrl-train/data#class-generatorinput) | Input batch | *required*

Returns:
GeneratorOutput: Generated trajectories


<details>
<summary>Source code in `skyrl_train/generators/base.py:53-64`</summary>

```python
    @abstractmethod
    async def generate(self, input_batch: GeneratorInput) -> GeneratorOutput:
        """Generate trajectories for the input batch.

        Returns outputs in the same order as the input batch.

        Args:
            input_batch (GeneratorInput): Input batch
        Returns:
            GeneratorOutput: Generated trajectories
        """
        raise NotImplementedError
```

</details>

### <span className="bg-blue-100 text-blue-800 dark:bg-blue-900 dark:text-blue-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">class</span> `InferenceEngineInterface`

Bases: ABC

**Functions:**

Name | Description
---- | -----------
[`generate`](#method-abstractmethod-async-generate) |
[`sample`](#method-async-sample) | Generate multiple independent samples from a single prompt.
[`chat_completion`](#method-abstractmethod-async-chat_completion) | Handles OpenAI-compatible HTTP endpoint.
[`completion`](#method-abstractmethod-async-completion) | Handles OpenAI-compatible HTTP endpoint.
[`wake_up`](#method-abstractmethod-async-wake_up) |
[`sleep`](#method-abstractmethod-async-sleep) |
[`init_weight_update_communicator`](#method-abstractmethod-async-init_weight_update_communicator) | Initialize weight update communicator from init info.
[`update_named_weights`](#method-abstractmethod-async-update_named_weights) |
[`teardown`](#method-abstractmethod-async-teardown) |
[`reset_prefix_cache`](#method-abstractmethod-async-reset_prefix_cache) |
[`tp_size`](#method-abstractmethod-tp_size) | Return the tensor parallel size of this inference engine.
[`pp_size`](#method-abstractmethod-pp_size) | Return the pipeline parallel size of this inference engine.
[`dp_size`](#method-abstractmethod-dp_size) | Return the data parallel size of this inference engine.
[`abort_generation`](#method-abstractmethod-async-abort_generation) | Abort all running and waiting requests, which make the ongoing requests return the

#### <span className="bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">method</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">async</span> `generate`

```python
generate(input_batch: InferenceEngineInput) -> InferenceEngineOutput
```


<details>
<summary>Source code in `skyrl_train/inference_engines/base.py:36-38`</summary>

```python
    @abstractmethod
    async def generate(self, input_batch: InferenceEngineInput) -> InferenceEngineOutput:
        raise NotImplementedError
```

</details>

#### <span className="bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">method</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">async</span> `sample`

```python
sample(prompt_token_ids: List[int], num_samples: int, sampling_params: Dict[str, Any]) -> InferenceEngineOutput
```

Generate multiple independent samples from a single prompt.

This method provides Tinker-compatible token-in/token-out sampling semantics.

**Parameters:**

Name | Type | Description | Default
---- | ---- | ----------- | -------
`prompt_token_ids` | List\[int\] | Token IDs for a single prompt. | *required*
`num_samples` | int | Number of independent samples to generate. | *required*
`sampling_params` | Dict\[str, Any\] | Sampling parameters. | *required*

**Returns:**

Type | Description
---- | -----------
[InferenceEngineOutput](#inferenceengineoutput) | InferenceEngineOutput containing num_samples results: - response_ids: List of num_samples token ID lists - responses: List of num_samples decoded strings - stop_reasons: List of num_samples stop reasons - response_logprobs: Optional list of num_samples logprob lists


<details>
<summary>Source code in `skyrl_train/inference_engines/base.py:40-88`</summary>

```python
    async def sample(
        self,
        prompt_token_ids: List[int],
        num_samples: int,
        sampling_params: Dict[str, Any],
    ) -> InferenceEngineOutput:
        """Generate multiple independent samples from a single prompt.

        This method provides Tinker-compatible token-in/token-out sampling semantics.

        Args:
            prompt_token_ids: Token IDs for a single prompt.
            num_samples: Number of independent samples to generate.
            sampling_params: Sampling parameters.

        Returns:
            InferenceEngineOutput containing num_samples results:
                - response_ids: List of num_samples token ID lists
                - responses: List of num_samples decoded strings
                - stop_reasons: List of num_samples stop reasons
                - response_logprobs: Optional list of num_samples logprob lists
        """
        all_response_ids = []
        all_responses = []
        all_stop_reasons = []
        all_response_logprobs = []

        for _ in range(num_samples):
            input_batch: InferenceEngineInput = {
                "prompts": None,
                "prompt_token_ids": [prompt_token_ids],  # Wrap in list for batch of 1
                "sampling_params": sampling_params,
                "session_ids": None,
            }
            output = await self.generate(input_batch)

            # Extract single result from batch of 1
            all_response_ids.append(output["response_ids"][0])
            all_responses.append(output["responses"][0])
            all_stop_reasons.append(output["stop_reasons"][0])
            if output.get("response_logprobs") is not None:
                all_response_logprobs.append(output["response_logprobs"][0])

        return {
            "response_ids": all_response_ids,
            "responses": all_responses,
            "stop_reasons": all_stop_reasons,
            "response_logprobs": all_response_logprobs if all_response_logprobs else None,
        }
```

</details>

#### <span className="bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">method</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">abstractmethod</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">async</span> `chat_completion`

```python
chat_completion(request_payload: Dict[str, Any]) -> Dict[str, Any]
```

Handles OpenAI-compatible HTTP endpoint.

Accepts a JSON payload: \{"json": &lt;request-body&gt;, "headers": &lt;headers-dict&gt;\}.
The request body will be used to construct a ChatCompletionRequest.
Returns a plain dict, either a ChatCompletionResponse or an ErrorResponse.
The specific fields of the response/request depend on the engine's backend (e.g. for vllm
these are defined in vllm.entrypoints.openai.protocol).


<details>
<summary>Source code in `skyrl_train/inference_engines/base.py:90-100`</summary>

```python
    @abstractmethod
    async def chat_completion(self, request_payload: Dict[str, Any]) -> Dict[str, Any]:
        """Handles OpenAI-compatible HTTP endpoint.

        Accepts a JSON payload: {"json": <request-body>, "headers": <headers-dict>}.
        The request body will be used to construct a ChatCompletionRequest.
        Returns a plain dict, either a ChatCompletionResponse or an ErrorResponse.
        The specific fields of the response/request depend on the engine's backend (e.g. for vllm
        these are defined in vllm.entrypoints.openai.protocol).
        """
        raise NotImplementedError
```

</details>

#### <span className="bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">method</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">abstractmethod</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">async</span> `completion`

```python
completion(request_payload: Dict[str, Any]) -> Dict[str, Any]
```

Handles OpenAI-compatible HTTP endpoint.

Accepts a JSON payload: \{"json": &lt;request-body&gt;, "headers": &lt;headers-dict&gt;\}.
The request body will be used to construct a CompletionRequest.
Returns a plain dict, either a CompletionResponse or an ErrorResponse.
The specific fields of the response/request depend on the engine's backend (e.g. for vllm
these are defined in vllm.entrypoints.openai.protocol).


<details>
<summary>Source code in `skyrl_train/inference_engines/base.py:102-112`</summary>

```python
    @abstractmethod
    async def completion(self, request_payload: Dict[str, Any]) -> Dict[str, Any]:
        """Handles OpenAI-compatible HTTP endpoint.

        Accepts a JSON payload: {"json": <request-body>, "headers": <headers-dict>}.
        The request body will be used to construct a CompletionRequest.
        Returns a plain dict, either a CompletionResponse or an ErrorResponse.
        The specific fields of the response/request depend on the engine's backend (e.g. for vllm
        these are defined in vllm.entrypoints.openai.protocol).
        """
        raise NotImplementedError
```

</details>

#### <span className="bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">method</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">abstractmethod</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">async</span> `wake_up`

```python
wake_up(*args: Any, **kwargs: Any)
```


<details>
<summary>Source code in `skyrl_train/inference_engines/base.py:114-116`</summary>

```python
    @abstractmethod
    async def wake_up(self, *args: Any, **kwargs: Any):
        raise NotImplementedError
```

</details>

#### <span className="bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">method</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">abstractmethod</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">async</span> `sleep`

```python
sleep(*args: Any, **kwargs: Any)
```


<details>
<summary>Source code in `skyrl_train/inference_engines/base.py:118-120`</summary>

```python
    @abstractmethod
    async def sleep(self, *args: Any, **kwargs: Any):
        raise NotImplementedError
```

</details>

#### <span className="bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">method</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">abstractmethod</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">async</span> `init_weight_update_communicator`

```python
init_weight_update_communicator(init_info: WeightSyncInitInfo)
```

Initialize weight update communicator from init info.

**Parameters:**

Name | Type | Description | Default
---- | ---- | ----------- | -------
`init_info` | WeightSyncInitInfo | WeightSyncInitInfo from the sender containing all info needed to create the appropriate receiver. | *required*


<details>
<summary>Source code in `skyrl_train/inference_engines/base.py:122-130`</summary>

```python
    @abstractmethod
    async def init_weight_update_communicator(self, init_info: "WeightSyncInitInfo"):
        """Initialize weight update communicator from init info.

        Args:
            init_info: WeightSyncInitInfo from the sender containing all info needed
                to create the appropriate receiver.
        """
        raise NotImplementedError()
```

</details>

#### <span className="bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">method</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">abstractmethod</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">async</span> `update_named_weights`

```python
update_named_weights(request: WeightUpdateRequest)
```


<details>
<summary>Source code in `skyrl_train/inference_engines/base.py:132-134`</summary>

```python
    @abstractmethod
    async def update_named_weights(self, request: "WeightUpdateRequest"):
        raise NotImplementedError()
```

</details>

#### <span className="bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">method</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">abstractmethod</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">async</span> `teardown`

```python
teardown()
```


<details>
<summary>Source code in `skyrl_train/inference_engines/base.py:136-138`</summary>

```python
    @abstractmethod
    async def teardown(self):
        raise NotImplementedError
```

</details>

#### <span className="bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">method</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">abstractmethod</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">async</span> `reset_prefix_cache`

```python
reset_prefix_cache()
```


<details>
<summary>Source code in `skyrl_train/inference_engines/base.py:140-142`</summary>

```python
    @abstractmethod
    async def reset_prefix_cache(self):
        raise NotImplementedError
```

</details>

#### <span className="bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">method</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">abstractmethod</span> `tp_size`

```python
tp_size() -> int
```

Return the tensor parallel size of this inference engine.


<details>
<summary>Source code in `skyrl_train/inference_engines/base.py:144-147`</summary>

```python
    @abstractmethod
    def tp_size(self) -> int:
        """Return the tensor parallel size of this inference engine."""
        raise NotImplementedError
```

</details>

#### <span className="bg-yellow-100 text-yellow-800 dark:bg-yellow-900 dark:text-yellow-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">attr</span> `pp_size`

```python
pp_size() -> int
```

Return the pipeline parallel size of this inference engine.


<details>
<summary>Source code in `skyrl_train/inference_engines/base.py:149-152`</summary>

```python
    @abstractmethod
    def pp_size(self) -> int:
        """Return the pipeline parallel size of this inference engine."""
        raise NotImplementedError
```

</details>

#### <span className="bg-yellow-100 text-yellow-800 dark:bg-yellow-900 dark:text-yellow-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">attr</span> `dp_size`

```python
dp_size() -> int
```

Return the data parallel size of this inference engine.


<details>
<summary>Source code in `skyrl_train/inference_engines/base.py:154-157`</summary>

```python
    @abstractmethod
    def dp_size(self) -> int:
        """Return the data parallel size of this inference engine."""
        raise NotImplementedError
```

</details>

#### <span className="bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">method</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">abstractmethod</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">async</span> `abort_generation`

```python
abort_generation() -> None
```

Abort all running and waiting requests, which make the ongoing requests return the
already-generated tokens with a stop_reason of "abort". If the request was waiting,
it returns a response with zero completion tokens.


<details>
<summary>Source code in `skyrl_train/inference_engines/base.py:159-166`</summary>

```python
    @abstractmethod
    async def abort_generation(self) -> None:
        """
        Abort all running and waiting requests, which make the ongoing requests return the
        already-generated tokens with a stop_reason of "abort". If the request was waiting,
        it returns a response with zero completion tokens.
        """
        raise NotImplementedError
```

</details>

### <span className="bg-blue-100 text-blue-800 dark:bg-blue-900 dark:text-blue-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">class</span> `InferenceEngineClient`

```python
InferenceEngineClient(engines: List[InferenceEngineInterface], tokenizer: PreTrainedTokenizerBase, full_config: Union[SkyRLConfig, DictConfig])
```

Bases: [InferenceEngineInterface](#class-inferenceengineinterface)

Client to talk to a set of InferenceEngines.

Note that InferenceEngineClient sub-classes InferenceEngineInterface so it can be used as if talking to a single
engine.

**Functions:**

Name | Description
---- | -----------
[`__init__`](#method-__init__) | Args:
[`_run_on_all_engines`](#method-async-_run_on_all_engines) | Call a method on all engines concurrently and gather the results.
[`generate`](#method-async-generate) |
[`_select_engine_idx`](#method-_select_engine_idx) | Select an engine index for routing a request.
[`sample`](#method-async-sample) | Generate multiple independent samples from a single prompt.
[`_generate_single_with_retry`](#method-async-_generate_single_with_retry) | Generate a single response with retry mechanism.
[`_chat_completion_with_retry`](#method-async-_chat_completion_with_retry) | Keep sending `chat_completion` requests (with previous responses accumulated) until the finish_reason is not
[`chat_completion`](#method-async-chat_completion) |
[`completion`](#method-async-completion) | Handles an OpenAI /completions request.
[`wake_up`](#method-async-wake_up) |
[`sleep`](#method-async-sleep) |
[`init_weight_update_communicator`](#method-async-init_weight_update_communicator) | Initialize weight update communicator on all engines.
[`update_named_weights`](#method-async-update_named_weights) |
[`reset_prefix_cache`](#method-async-reset_prefix_cache) |
[`teardown`](#method-async-teardown) |
[`tp_size`](#method-tp_size) |
[`pp_size`](#method-pp_size) |
[`dp_size`](#method-dp_size) |
[`_wait_for_generation_to_resume`](#method-async-_wait_for_generation_to_resume) | Waits for generation to be resumed, intended for in-flight weight updates and partial rollouts.
[`pause_generation`](#method-async-pause_generation) | Pauses generation for all engines, intended for in-flight weight updates and partial rollouts.
[`resume_generation`](#method-async-resume_generation) | Resumes generation for all engines, intended for in-flight weight updates and partial rollouts.
[`abort_generation`](#method-async-abort_generation) |
[`__del__`](#method-__del__) | Destructor to shut down the HTTP endpoint if it was started.
[`__getstate__`](#method-__getstate__) | Override to avoid pickling the server thread and the threading.Event object, which are not picklable.
[`_spin_up_http_endpoint`](#method-_spin_up_http_endpoint) |

**Parameters:**

Name | Type | Description | Default
---- | ---- | ----------- | -------
`engines` | List\[[InferenceEngineInterface](#class-inferenceengineinterface)\] | List[InferenceEngineInterface] - The inference engines, remote or local. | *required*
`tokenizer` | PreTrainedTokenizerBase | PreTrainedTokenizerBase - The tokenizer to use. | *required*
`full_config` | Union\[SkyRLConfig, DictConfig\] | full training configuration | *required*

#### <span className="bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">method</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">async</span> `_run_on_all_engines`

```python
_run_on_all_engines(method_name: str, *args: str, **kwargs: str)
```

Call a method on all engines concurrently and gather the results.


<details>
<summary>Source code in `skyrl_train/inference_engines/inference_engine_client.py:77-84`</summary>

```python
    async def _run_on_all_engines(self, method_name: str, *args, **kwargs):
        """
        Call a method on all engines concurrently and gather the results.
        """
        assert len(self.engines) > 0, "No engines to call method on"

        awaitables = [getattr(engine, method_name)(*args, **kwargs) for engine in self.engines]
        return await asyncio.gather(*awaitables)
```

</details>

#### <span className="bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">method</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">async</span> `generate`

```python
generate(input_batch: InferenceEngineInput) -> InferenceEngineOutput
```


<details>
<summary>Source code in `skyrl_train/inference_engines/inference_engine_client.py:86-170`</summary>

```python
    async def generate(self, input_batch: InferenceEngineInput) -> InferenceEngineOutput:
        # 0. Extract input
        prompts = input_batch.get("prompts")
        prompt_token_ids = input_batch.get("prompt_token_ids")
        session_ids = input_batch.get("session_ids")
        sampling_params = input_batch.get("sampling_params")

        if (prompts is None and prompt_token_ids is None) or (prompts is not None and prompt_token_ids is not None):
            raise ValueError("Either `prompts` or `prompt_token_ids` must be provided, but not both.")
        if prompt_token_ids is None:
            prompt_token_ids = self.tokenizer.apply_chat_template(
                prompts,
                add_generation_prompt=True,
                add_special_tokens=False,
                return_dict=True,
                tokenize=True,
            )["input_ids"]

        num_prompts = len(prompt_token_ids)
        num_inference_engines = len(self.engines)

        # 1. Route prompts to engines
        engine_idx_to_prompt_ids: dict[int, list[int]] = route_prompts_to_engines(
            num_prompts=num_prompts,
            num_inference_engines=num_inference_engines,
            session_ids=session_ids,
        )

        # We do a shortcut for non-batched requests, which can support pause/continue generation for
        # in-flight weight updates.
        if num_prompts == 1:
            # Route to a single engine for this single prompt and use retry flow.
            assert len(engine_idx_to_prompt_ids) == 1
            ((engine_idx, prompt_ids_list),) = engine_idx_to_prompt_ids.items()
            assert prompt_ids_list == [0], "Single prompt should map to index [0]"
            original_prompt_ids = prompt_token_ids[0]
            return await self._generate_single_with_retry(
                engine_idx=engine_idx,
                original_prompt_ids=original_prompt_ids,
                sampling_params=sampling_params,
            )

        # For batched generate(), pause/continue cannot be supported.
        if self.generation_paused_event.is_set():
            raise RuntimeError("pause_generation is unsupported for batched InferenceEngineClient.generate().")

        # 2. Generate responses concurrently
        tasks: list[asyncio.Task] = []
        indices_list: list[list[int]] = []  # the original prompt indices that each task works on
        for engine_idx, prompt_ids in engine_idx_to_prompt_ids.items():
            # index prompt_token_ids with prompt_ids
            cur_prompt_token_ids = [prompt_token_ids[i] for i in prompt_ids]
            engine_input = InferenceEngineInput(
                prompt_token_ids=cur_prompt_token_ids,
                sampling_params=sampling_params,
            )
            tasks.append(asyncio.create_task(self.engines[engine_idx].generate(engine_input)))
            indices_list.append(prompt_ids)

        results = await asyncio.gather(*tasks)

        # 3. Reconstruct output in original order
        n = len(prompt_token_ids)
        responses: list[str] = [""] * n
        stop_reasons: list[str] = [""] * n
        response_logprobs: List[Optional[List[float]]] = [None for _ in range(n)]
        response_ids: List[List[int]] = [[] for _ in range(n)]
        # a bit hacky for now
        add_resp_logprobs = False

        for indices, result in zip(indices_list, results):
            for local_idx, original_idx in enumerate(indices):
                responses[original_idx] = result["responses"][local_idx]
                stop_reasons[original_idx] = result["stop_reasons"][local_idx]
                response_ids[original_idx] = result["response_ids"][local_idx]
                if result.get("response_logprobs", None):
                    add_resp_logprobs = True
                    response_logprobs[original_idx] = result["response_logprobs"][local_idx]

        return InferenceEngineOutput(
            responses=responses,
            stop_reasons=stop_reasons,
            response_ids=response_ids,
            response_logprobs=response_logprobs if add_resp_logprobs else None,
        )
```

</details>

#### <span className="bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">method</span> `_select_engine_idx`

```python
_select_engine_idx(session_id: Optional[Union[str, int]] = None) -> int
```

Select an engine index for routing a request.

**Parameters:**

Name | Type | Description | Default
---- | ---- | ----------- | -------
`session_id` | Optional\[Union\[str, int\]\] | Optional session ID for consistent routing (e.g., conversation ID for chat). If None, uses random load-balancing. | `None`

**Returns:**

Type | Description
---- | -----------
int | Engine index to route the request to.


<details>
<summary>Source code in `skyrl_train/inference_engines/inference_engine_client.py:172-185`</summary>

```python
    def _select_engine_idx(self, session_id: Optional[Union[str, int]] = None) -> int:
        """Select an engine index for routing a request.

        Args:
            session_id: Optional session ID for consistent routing (e.g., conversation ID for chat).
                       If None, uses random load-balancing.

        Returns:
            Engine index to route the request to.
        """
        if session_id is None:
            return random.randint(0, len(self.engines) - 1)
        else:
            return hash_with_sha256(str(session_id)) % len(self.engines)
```

</details>

#### <span className="bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">method</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">async</span> `sample`

```python
sample(prompt_token_ids: List[int], num_samples: int, sampling_params: Dict[str, Any], session_id: Optional[Union[str, int]] = None) -> InferenceEngineOutput
```

Generate multiple independent samples from a single prompt.

This method provides Tinker-compatible token-in/token-out sampling semantics.
Generates num_samples independent completions from the same prompt.

**Parameters:**

Name | Type | Description | Default
---- | ---- | ----------- | -------
`prompt_token_ids` | List\[int\] | Token IDs for a single prompt (not batched). | *required*
`num_samples` | int | Number of independent samples to generate. | *required*
`sampling_params` | Dict\[str, Any\] | Sampling parameters (temperature, max_tokens, etc.). | *required*
`session_id` | Optional\[Union\[str, int\]\] | Optional session ID for consistent engine routing (e.g., conversation ID). If None, uses random load-balancing. Tinker API should pass None since each sample() call is independent. | `None`

**Returns:**

Type | Description
---- | -----------
[InferenceEngineOutput](#inferenceengineoutput) | InferenceEngineOutput containing num_samples results.


<details>
<summary>Source code in `skyrl_train/inference_engines/inference_engine_client.py:187-221`</summary>

```python
    async def sample(
        self,
        prompt_token_ids: List[int],
        num_samples: int,
        sampling_params: Dict[str, Any],
        session_id: Optional[Union[str, int]] = None,
    ) -> InferenceEngineOutput:
        """Generate multiple independent samples from a single prompt.

        This method provides Tinker-compatible token-in/token-out sampling semantics.
        Generates num_samples independent completions from the same prompt.

        Args:
            prompt_token_ids: Token IDs for a single prompt (not batched).
            num_samples: Number of independent samples to generate.
            sampling_params: Sampling parameters (temperature, max_tokens, etc.).
            session_id: Optional session ID for consistent engine routing (e.g., conversation ID).
                       If None, uses random load-balancing. Tinker API should pass None since
                       each sample() call is independent.

        Returns:
            InferenceEngineOutput containing num_samples results.
        """
        # Wait for generation to resume if paused (for weight updates)
        await self._wait_for_generation_to_resume()

        # Select engine (random if session_id is None, consistent hash otherwise)
        engine_idx = self._select_engine_idx(session_id)
        engine = self.engines[engine_idx]

        return await engine.sample(
            prompt_token_ids=prompt_token_ids,
            num_samples=num_samples,
            sampling_params=sampling_params,
        )
```

</details>

#### <span className="bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">method</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">async</span> `_generate_single_with_retry`

```python
_generate_single_with_retry(engine_idx: int, original_prompt_ids: List[int], sampling_params: Optional[Dict[str, Any]]) -> InferenceEngineOutput
```

Generate a single response with retry mechanism.

This method is equivalent to `_chat_completion_with_retry()` but for the `generate()` codepath.
We keep sending `generate` requests (with previous responses accumulated) until the finish_reason
is not "abort". It is intended to be used in combination with `pause_generation()` and `resume_generation()` for
in-flight weight updates and partial rollouts.

This method is equivalent to a single `generate()` call if we do not use `pause_generation()`.

Since we operate purely in the token space, it is token-in-token-out, unlike `_chat_completion_with_retry()`
which re-encodes in each new request.

For subsequent retry requests (`InferenceEngineInput`), we:

- Update the `InferenceEngineInput.prompt_token_ids` with the accumulated output tokens.
- Skip accumulating `InferenceEngineOutput.responses` since we decode the final output.
- Adjust remaining max tokens if `max_tokens` or `max_completion_tokens` is present.

For the final response, we return `InferenceEngineOutput` with:

- `responses`: decoded at the end from `response_ids` if generation is completed in > 1 turns, otherwise
  the text response of the first turn.
- `response_ids`: the accumulated output tokens
- `stop_reasons`: the stop reason of the final response
- `response_logprobs`: the accumulated logprobs


<details>
<summary>Source code in `skyrl_train/inference_engines/inference_engine_client.py:223-321`</summary>

```python
    async def _generate_single_with_retry(
        self, engine_idx: int, original_prompt_ids: List[int], sampling_params: Optional[Dict[str, Any]]
    ) -> InferenceEngineOutput:
        """
        Generate a single response with retry mechanism.

        This method is equivalent to `_chat_completion_with_retry()` but for the `generate()` codepath.
        We keep sending `generate` requests (with previous responses accumulated) until the finish_reason
        is not "abort". It is intended to be used in combination with `pause_generation()` and `resume_generation()` for
        in-flight weight updates and partial rollouts.

        This method is equivalent to a single `generate()` call if we do not use `pause_generation()`.

        Since we operate purely in the token space, it is token-in-token-out, unlike `_chat_completion_with_retry()`
        which re-encodes in each new request.

        For subsequent retry requests (`InferenceEngineInput`), we:
        - Update the `InferenceEngineInput.prompt_token_ids` with the accumulated output tokens.
        - Skip accumulating `InferenceEngineOutput.responses` since we decode the final output.
        - Adjust remaining max tokens if `max_tokens` or `max_completion_tokens` is present.

        For the final response, we return `InferenceEngineOutput` with:
        - `responses`: decoded at the end from `response_ids` if generation is completed in > 1 turns, otherwise
            the text response of the first turn.
        - `response_ids`: the accumulated output tokens
        - `stop_reasons`: the stop reason of the final response
        - `response_logprobs`: the accumulated logprobs
        """
        if sampling_params is None:
            sampling_params = {}

        # 1. First determine original max tokens key and value (if any)
        max_key = None
        if "max_tokens" in sampling_params:
            max_key = "max_tokens"
        elif "max_completion_tokens" in sampling_params:
            max_key = "max_completion_tokens"
        original_max_tokens: Optional[int] = sampling_params.get(max_key) if max_key else None

        # 2. Initialize fields we want to accumulate or update in each loop iteration
        accum_response_ids: List[int] = []
        accum_response_logprobs: List[float] = []
        stop_reason: str = "abort"

        # We only use it if generation is completed in one turn to maintain original behavior with no retry.
        text_response: Optional[str] = None
        num_turns = 0

        # 3. Loop until geneartion is completed.
        while stop_reason == "abort":
            await self._wait_for_generation_to_resume()

            # 3.1. Prepare the request payload.
            cur_sampling_params = sampling_params.copy()
            if original_max_tokens is not None:
                new_max_tokens = original_max_tokens - len(accum_response_ids)
                assert new_max_tokens >= 0, f"Expect new_max_tokens to be non-negative, but got {new_max_tokens}"
                cur_sampling_params[max_key] = new_max_tokens
            new_prompt_ids = original_prompt_ids + accum_response_ids
            engine_input = InferenceEngineInput(
                prompt_token_ids=[new_prompt_ids],
                sampling_params=cur_sampling_params,
            )

            # 3.2. Send the request.
            logger.debug(f"generate() request sent (including potential retries): {engine_input}")
            partial_response: InferenceEngineOutput = await self.engines[engine_idx].generate(engine_input)

            # 3.3. Parse the partial response.
            assert len(partial_response["response_ids"]) == 1, "Expected exactly one response."
            new_response_ids: List[int] = partial_response["response_ids"][0]
            text_response = partial_response["responses"][0]
            stop_reason = partial_response["stop_reasons"][0]
            new_response_logprobs: Optional[List[float]] = None
            new_response_logprobs_list: Optional[List[List[float]]] = partial_response.get("response_logprobs", None)
            if new_response_logprobs_list is not None and len(new_response_logprobs_list) > 0:
                new_response_logprobs = new_response_logprobs_list[0]

            # 3.4 Aborted without generating tokens, so partial_response is useless.
            if stop_reason == "abort" and len(new_response_ids) == 0:
                continue

            # 3.5 Accumulate outputs
            accum_response_ids.extend(new_response_ids)
            if new_response_logprobs is not None:
                accum_response_logprobs.extend(new_response_logprobs)
            num_turns += 1

        # 4. Build the final response and return.
        if num_turns == 1:
            final_text_response = text_response
        else:
            final_text_response = self.tokenizer.decode(accum_response_ids, skip_special_tokens=True)
        return InferenceEngineOutput(
            responses=[final_text_response],
            stop_reasons=[stop_reason],
            response_ids=[accum_response_ids],
            response_logprobs=[accum_response_logprobs] if len(accum_response_logprobs) > 0 else None,
        )
```

</details>

#### <span className="bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">method</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">async</span> `_chat_completion_with_retry`

```python
_chat_completion_with_retry(engine_idx: int, original_request_payload: Dict[str, Any]) -> Dict[str, Any]
```

Keep sending `chat_completion` requests (with previous responses accumulated) until the finish_reason is not
"abort".

The retry mechanism is intended to be used in combination with `pause_generation()` and `resume_generation()`
for in-flight weight updates and partial rollouts.

This method is equivalent to a single `chat_completion()` call if we do not use `pause_generation()`.

For subsequent retry requests, we can reuse the original request with the following exceptions:

- Update the last assistant message content to accumulated content, where the role uses the first non-empty
  response's role.
- Set continue_final_message=True and add_generation_prompt=False.
- Adjust remaining max tokens if `max_tokens` or `max_completion_tokens` is present.
- If no tokens have been generated yet, resend the original request unchanged.

For the final response, we maintain all the first non-empty response's fields (i.e. prefilled already),
with the following exceptions:

- Accumulate the following across retry requests:
  - `choices[0]["logprobs"]["content"]`
  - `choices[0]["token_ids"]`
  - `choices[0]["message"]["content"]`
- Use the last response's finish_reason and stop_reason


<details>
<summary>Source code in `skyrl_train/inference_engines/inference_engine_client.py:323-432`</summary>

```python
    async def _chat_completion_with_retry(
        self, engine_idx: int, original_request_payload: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Keep sending `chat_completion` requests (with previous responses accumulated) until the finish_reason is not
        "abort".

        The retry mechanism is intended to be used in combination with `pause_generation()` and `resume_generation()`
        for in-flight weight updates and partial rollouts.

        This method is equivalent to a single `chat_completion()` call if we do not use `pause_generation()`.

        For subsequent retry requests, we can reuse the original request with the following exceptions:
        - Update the last assistant message content to accumulated content, where the role uses the first non-empty
          response's role.
        - Set continue_final_message=True and add_generation_prompt=False.
        - Adjust remaining max tokens if `max_tokens` or `max_completion_tokens` is present.
        - If no tokens have been generated yet, resend the original request unchanged.

        For the final response, we maintain all the first non-empty response's fields (i.e. prefilled already),
        with the following exceptions:
        - Accumulate the following across retry requests:
          - `choices[0]["logprobs"]["content"]`
          - `choices[0]["token_ids"]`
          - `choices[0]["message"]["content"]`
        - Use the last response's finish_reason and stop_reason
        """
        original_request_json: Dict[str, Any] = original_request_payload.get("json", {}).copy()
        headers: Dict[str, str] = original_request_payload.get("headers", {}).copy()

        assert not original_request_json.get(
            "continue_final_message", False
        ), "continue_final_message must be False for /chat/completions requests"

        # Accumulated fields for building subsequent requests and final response. It is inplace-updated
        # in `_parse_partial_response_and_inplace_update_accum()`.
        accum = AccumulatedResponse()

        # First non-empty response (i.e. the response that prefilled the prompt) to copy meta from.
        base_response: Optional[Dict[str, Any]] = None

        # Determine original max tokens key and value (if any)
        max_key = None
        if "max_tokens" in original_request_json:
            max_key = "max_tokens"
        elif "max_completion_tokens" in original_request_json:
            max_key = "max_completion_tokens"
        orig_max_tokens: Optional[int] = original_request_json.get(max_key) if max_key else None

        # Fields to be updated in each loop iteration
        finish_reason: str = "abort"
        stop_reason: Optional[str] = None
        response_role: Optional[str] = None

        # 1. Loop until the generation is completed.
        while finish_reason == "abort":
            await self._wait_for_generation_to_resume()

            # 1.1. Prepare the request payload.
            cur_request_json = _prepare_retry_request(
                original_request_json=original_request_json,
                accum=accum,
                response_role=response_role,
                orig_max_tokens=orig_max_tokens,
                max_key=max_key,
            )

            # 1.2. Send the request.
            logger.debug(f"/chat/completions request sent (including potential retries): {cur_request_json}")
            partial_response = await self.engines[engine_idx].chat_completion(
                {"json": cur_request_json, "headers": headers}
            )

            # 1.3. Parse partial response and in-place update accumulators.
            (
                finish_reason,
                stop_reason,
                response_role,
                aborted_without_generating,
            ) = _parse_partial_response_and_inplace_update_accum(
                partial_response=partial_response,
                accum=accum,
                response_role=response_role,
            )

            # 1.4. Aborted without generating tokens, so partial_response is useless.
            if aborted_without_generating:
                continue

            # At this point, either some tokens were generated and/or request completed with a non-"abort" finish_reason

            # 1.5. Update base response if it is the first non-empty response
            if base_response is None:
                if finish_reason != "abort":
                    # If we only made one request and it is not aborted, return the partial result directly.
                    # This is the codepath that will hit when we do not use `pause_generation()`
                    # or `resume_generation()`.
                    return partial_response
                # NOTE(Charlie): not doing deepcopy here to avoid copying large logprobs, so be careful when
                # modifying this.
                base_response = partial_response.copy()

        # 2. Build final response by combining fields
        assert base_response is not None, "Expected at least one non-empty response to build final response"
        return _build_final_response(
            base_response=base_response,
            accum=accum,
            finish_reason=finish_reason,
            stop_reason=stop_reason,
        )
```

</details>

#### <span className="bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">method</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">abstractmethod</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">async</span> `chat_completion`

```python
chat_completion(request_payload: Dict[str, Any]) -> Dict[str, Any]
```


<details>
<summary>Source code in `skyrl_train/inference_engines/inference_engine_client.py:434-441`</summary>

```python
    async def chat_completion(self, request_payload: Dict[str, Any]) -> Dict[str, Any]:
        session_id = request_payload["json"].pop("session_id", None)
        if session_id is not None:
            assert isinstance(session_id, (str, int)), "Session ID must be an integer or string for `/chat/completions`"
        engine_idx = self._select_engine_idx(session_id)

        # Always use the retry loop which also issues the first request inside
        return await self._chat_completion_with_retry(engine_idx, request_payload)
```

</details>

#### <span className="bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">method</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">abstractmethod</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">async</span> `completion`

```python
completion(request_payload: Dict[str, Any]) -> Dict[str, Any]
```

Handles an OpenAI /completions request.

Since `request["prompt"]` can be `Union[list[int], list[list[int]], str, list[str]]`,
(i.e. \{batched, single\} x \{string, token IDs\}), we need to route the request to engines
differently, based on whether it's a single or batched request, and whether `request["session_id"]`
is provided. This is similar to `generate()` method.

For single, we do the same routing logic as `chat_completion()`. For batched, we route by
`request["session_id"]` if present, and if not we split evenly across engines.

Regardless, the order will be maintained, i.e. `output["choices"][i]` corresponds to `request["prompt"][i]`.


<details>
<summary>Source code in `skyrl_train/inference_engines/inference_engine_client.py:443-541`</summary>

```python
    async def completion(self, request_payload: Dict[str, Any]) -> Dict[str, Any]:
        """
        Handles an OpenAI /completions request.

        Since `request["prompt"]` can be `Union[list[int], list[list[int]], str, list[str]]`,
        (i.e. {batched, single} x {string, token IDs}), we need to route the request to engines
        differently, based on whether it's a single or batched request, and whether `request["session_id"]`
        is provided. This is similar to `generate()` method.

        For single, we do the same routing logic as `chat_completion()`. For batched, we route by
        `request["session_id"]` if present, and if not we split evenly across engines.

        Regardless, the order will be maintained, i.e. `output["choices"][i]` corresponds to `request["prompt"][i]`.
        """
        if self.generation_paused_event.is_set():
            raise RuntimeError("pause_generation is unsupported for /completions requests.")
        body = request_payload.get("json", {})

        # NOTE(Charlie): do not reuse headers here as the single request may become various new requests
        headers = {"Content-Type": "application/json"}

        # 1. Postprocess prompt, session_id, and validate request.
        prompt = body.get("prompt")
        session_id_value = body.pop("session_id", None)
        ret = postprocess_completion_request(prompt, session_id_value)
        session_id_list: Optional[Union[List[int], List[str], ErrorResponse]] = ret[0]
        prompt: Union[List[List[int]], List[str]] = ret[1]
        if isinstance(session_id_list, ErrorResponse):
            return session_id_list.model_dump()

        num_prompts = len(prompt)
        num_inference_engines = len(self.engines)
        assert num_prompts > 0, "Number of prompts must be greater than 0"

        # 1. Route prompts to engines
        engine_idx_to_prompt_ids: dict[int, list[int]] = route_prompts_to_engines(
            num_prompts=num_prompts,
            num_inference_engines=num_inference_engines,
            session_ids=session_id_list,
        )

        # 2. Generate responses concurrently
        tasks: list[asyncio.Task] = []
        indices_list: list[list[int]] = []  # the original prompt indices that each task works on
        for engine_idx, prompt_ids in engine_idx_to_prompt_ids.items():
            cur_prompt = [prompt[i] for i in prompt_ids]
            # reuse the exact same request except for the prompt
            cur_json = dict(body)
            cur_json["prompt"] = cur_prompt
            coro = self.engines[engine_idx].completion({"json": cur_json, "headers": headers})
            tasks.append(asyncio.create_task(coro))
            indices_list.append(prompt_ids)

        results = await asyncio.gather(*tasks)

        # 3. Check for errors.
        # results can be ErrorResponse or CompletionResponse. If one of the sub-requests fails, we
        # return an error response. That is, there is no partial success, following vLLM and SGLang's behavior.
        for result in results:
            if "error" in result or result.get("object", "") == "error":
                # former is vllm format, latter is sglang format
                error_details = result.get("error", result)  # resolves vllm/sglang format difference
                error_code = error_details["code"]
                error_type = error_details["type"]
                error_message = error_details["message"]
                return ErrorResponse(
                    error=ErrorInfo(
                        message=f"In one of the engines that SkyRL manages, an error occurred: {error_message}",
                        type=error_type,
                        code=error_code,
                    ),
                ).model_dump()

        # 4. Combine choices and preserve original order.
        # If there is only one result, we return it directly.
        if len(results) == 1:
            return results[0]

        # Use the first result as base response. There are some fields that cannot be shared
        # across sub-requests. For now it is just the usage field.
        final_response = dict(results[0])
        final_response["usage"] = aggregate_completion_usage_info(results, self.backend)

        # Aggregate choices. TODO(Charlie): improve logic when we need to support n > 1
        # vLLM sets index positions per sub-batch, so we reset indices to be 0..n-1 for the combined response.
        combined_choices: list[Dict[str, Any]] = [None] * num_prompts
        for indices, result in zip(indices_list, results):
            # indices are the original prompt indices that the task's response corresponds to
            for local_idx, original_idx in enumerate(indices):
                choice = result["choices"][local_idx]
                choice["index"] = original_idx  # overwrite index with the global position
                combined_choices[original_idx] = choice

        # sanity check that the index is correct
        for new_idx in range(len(combined_choices)):
            assert combined_choices[new_idx]["index"] == new_idx

        final_response["choices"] = combined_choices
        return final_response
```

</details>

#### <span className="bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">method</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">abstractmethod</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">async</span> `wake_up`

```python
wake_up(*args: Any, **kwargs: Any)
```


<details>
<summary>Source code in `skyrl_train/inference_engines/inference_engine_client.py:543-544`</summary>

```python
    async def wake_up(self, *args: Any, **kwargs: Any):
        return await self._run_on_all_engines("wake_up", *args, **kwargs)
```

</details>

#### <span className="bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">method</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">abstractmethod</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">async</span> `sleep`

```python
sleep(*args: Any, **kwargs: Any)
```


<details>
<summary>Source code in `skyrl_train/inference_engines/inference_engine_client.py:546-547`</summary>

```python
    async def sleep(self, *args: Any, **kwargs: Any):
        return await self._run_on_all_engines("sleep", *args, **kwargs)
```

</details>

#### <span className="bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">method</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">abstractmethod</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">async</span> `init_weight_update_communicator`

```python
init_weight_update_communicator(init_info: 'WeightSyncInitInfo')
```

Initialize weight update communicator on all engines.

**Parameters:**

Name | Type | Description | Default
---- | ---- | ----------- | -------
`init_info` | `'WeightSyncInitInfo'` | WeightSyncInitInfo from the sender. | *required*

**Note:**

Per-engine adjustments (e.g., rank_offset for broadcast) are handled
by init_info.for_engine().


<details>
<summary>Source code in `skyrl_train/inference_engines/inference_engine_client.py:549-563`</summary>

```python
    async def init_weight_update_communicator(self, init_info: "WeightSyncInitInfo"):
        """Initialize weight update communicator on all engines.

        Args:
            init_info: WeightSyncInitInfo from the sender.

        Note:
            Per-engine adjustments (e.g., rank_offset for broadcast) are handled
            by init_info.for_engine().
        """
        tasks = []
        for i, engine in enumerate(self.engines):
            engine_init_info = init_info.for_engine(i, engine.tp_size(), engine.pp_size())
            tasks.append(engine.init_weight_update_communicator(engine_init_info))
        await asyncio.gather(*tasks)
```

</details>

#### <span className="bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">method</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">abstractmethod</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">async</span> `update_named_weights`

```python
update_named_weights(request: WeightUpdateRequest)
```


<details>
<summary>Source code in `skyrl_train/inference_engines/inference_engine_client.py:565-566`</summary>

```python
    async def update_named_weights(self, request: WeightUpdateRequest):
        return await self._run_on_all_engines("update_named_weights", request=request)
```

</details>

#### <span className="bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">method</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">abstractmethod</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">async</span> `reset_prefix_cache`

```python
reset_prefix_cache()
```


<details>
<summary>Source code in `skyrl_train/inference_engines/inference_engine_client.py:568-569`</summary>

```python
    async def reset_prefix_cache(self):
        return await self._run_on_all_engines("reset_prefix_cache")
```

</details>

#### <span className="bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">method</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">abstractmethod</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">async</span> `teardown`

```python
teardown()
```


<details>
<summary>Source code in `skyrl_train/inference_engines/inference_engine_client.py:571-572`</summary>

```python
    async def teardown(self):
        return await self._run_on_all_engines("teardown")
```

</details>

#### <span className="bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">method</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">abstractmethod</span> `tp_size`

```python
tp_size() -> int
```


<details>
<summary>Source code in `skyrl_train/inference_engines/inference_engine_client.py:574-575`</summary>

```python
    def tp_size(self) -> int:
        raise NotImplementedError("InferenceEngineClient does not implement tp_size()")
```

</details>

#### <span className="bg-yellow-100 text-yellow-800 dark:bg-yellow-900 dark:text-yellow-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">attr</span> `pp_size`

```python
pp_size() -> int
```


<details>
<summary>Source code in `skyrl_train/inference_engines/inference_engine_client.py:577-578`</summary>

```python
    def pp_size(self) -> int:
        raise NotImplementedError("InferenceEngineClient does not implement pp_size()")
```

</details>

#### <span className="bg-yellow-100 text-yellow-800 dark:bg-yellow-900 dark:text-yellow-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">attr</span> `dp_size`

```python
dp_size() -> int
```


<details>
<summary>Source code in `skyrl_train/inference_engines/inference_engine_client.py:580-581`</summary>

```python
    def dp_size(self) -> int:
        raise NotImplementedError("InferenceEngineClient does not implement dp_size()")
```

</details>

#### <span className="bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">method</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">async</span> `_wait_for_generation_to_resume`

```python
_wait_for_generation_to_resume() -> None
```

Waits for generation to be resumed, intended for in-flight weight updates and partial rollouts.


<details>
<summary>Source code in `skyrl_train/inference_engines/inference_engine_client.py:586-589`</summary>

```python
    async def _wait_for_generation_to_resume(self) -> None:
        """Waits for generation to be resumed, intended for in-flight weight updates and partial rollouts."""
        while self.generation_paused_event.is_set():
            await asyncio.sleep(0.5)
```

</details>

#### <span className="bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">method</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">async</span> `pause_generation`

```python
pause_generation() -> None
```

Pauses generation for all engines, intended for in-flight weight updates and partial rollouts.

Currently only supported for `/chat/completions` and not `/completions` or `generate()`.

Both in-flight and incoming requests will be blocked until `resume_generation` is called.

1. Set the paused event to avoid new requests from being submitted while aborting requests.
1. Wait for a grace period to ensure all in-flight requests have entered the engine's
   scheduler and hence can be aborted. Otherwise, there can be requests already submitted
   but not yet entered the scheduler, which can miss the abort request.
1. Finally, we abort requests on all engines. This will cause the requests sent from
   InferenceEngineClient to `InferenceEngineClient.engines` to return the already-generated tokens.
   The request to `InferenceEngineClient` will not yet return until requests are completed with
   stop reason that is not `abort`.


<details>
<summary>Source code in `skyrl_train/inference_engines/inference_engine_client.py:591-611`</summary>

```python
    async def pause_generation(self) -> None:
        """
        Pauses generation for all engines, intended for in-flight weight updates and partial rollouts.

        Currently only supported for `/chat/completions` and not `/completions` or `generate()`.

        Both in-flight and incoming requests will be blocked until `resume_generation` is called.
        1. Set the paused event to avoid new requests from being submitted while aborting requests.
        2. Wait for a grace period to ensure all in-flight requests have entered the engine's
           scheduler and hence can be aborted. Otherwise, there can be requests already submitted
           but not yet entered the scheduler, which can miss the abort request.
        3. Finally, we abort requests on all engines. This will cause the requests sent from
           InferenceEngineClient to `InferenceEngineClient.engines` to return the already-generated tokens.
           The request to `InferenceEngineClient` will not yet return until requests are completed with
           stop reason that is not `abort`.
        """
        if self.generation_paused_event.is_set():
            raise RuntimeError("Generation is already paused, cannot pause again.")
        self.generation_paused_event.set()
        await asyncio.sleep(ABORT_GENERATION_GRACE_PERIOD_SECONDS)
        await self._run_on_all_engines("abort_generation")
```

</details>

#### <span className="bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">method</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">async</span> `resume_generation`

```python
resume_generation() -> None
```

Resumes generation for all engines, intended for in-flight weight updates and partial rollouts.

Resume all in-flight requests with the previously-generated tokens, and unblock incoming requests
that were blocked by `pause_generation()`.


<details>
<summary>Source code in `skyrl_train/inference_engines/inference_engine_client.py:613-622`</summary>

```python
    async def resume_generation(self) -> None:
        """
        Resumes generation for all engines, intended for in-flight weight updates and partial rollouts.

        Resume all in-flight requests with the previously-generated tokens, and unblock incoming requests
        that were blocked by `pause_generation()`.
        """
        if not self.generation_paused_event.is_set():
            raise RuntimeError("Generation is not paused, cannot resume.")
        self.generation_paused_event.clear()
```

</details>

#### <span className="bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">method</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">abstractmethod</span> <span className="bg-gray-100 text-gray-700 dark:bg-gray-800 dark:text-gray-300 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">async</span> `abort_generation`

```python
abort_generation() -> None
```


<details>
<summary>Source code in `skyrl_train/inference_engines/inference_engine_client.py:624-628`</summary>

```python
    async def abort_generation(self) -> None:
        raise NotImplementedError(
            "InferenceEngineClient does not implement abort_generation(), but calls "
            "`abort_generation` on all engines in `pause_generation()`."
        )
```

</details>

#### <span className="bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">method</span> `__del__`

```python
__del__()
```

Destructor to shut down the HTTP endpoint if it was started.


<details>
<summary>Source code in `skyrl_train/inference_engines/inference_engine_client.py:634-661`</summary>

```python
    def __del__(self):
        """
        Destructor to shut down the HTTP endpoint if it was started.
        """
        # TODO(Charlie): __del__ is not guaranteed to be called in general. Add to `teardown` method
        # when the `_handle_termination` flow is implemented. See `skyrl_train/workers/worker.py`
        # comments on `_handle_termination` for more details.
        if (
            self.enable_http_endpoint
            and hasattr(
                self, "_server_thread"
            )  # don't want to shut down the server when it is pickled as a ray method argument.
            and self._server_thread is not None
        ):
            try:
                from skyrl_train.inference_engines.inference_engine_client_http_endpoint import (
                    shutdown_server,
                )

                shutdown_server(
                    host=self.http_endpoint_host,
                    port=self.http_endpoint_port,
                    max_wait_seconds=10,
                )
                if hasattr(self, "_server_thread") and self._server_thread.is_alive():
                    self._server_thread.join(timeout=10)
            except Exception as e:
                logger.error(f"Error shutting down HTTP endpoint: {e}")
```

</details>

#### <span className="bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">method</span> `__getstate__`

```python
__getstate__()
```

Override to avoid pickling the server thread and the threading.Event object, which are not picklable.
Needed when passing InferenceEngineClient as an argument to async_run_ray_method(), mainly for
invoking `init_weight_sync_state()` and `broadcast_to_inference_engines()`, which do
not need these attributes.


<details>
<summary>Source code in `skyrl_train/inference_engines/inference_engine_client.py:663-673`</summary>

```python
    def __getstate__(self):
        """
        Override to avoid pickling the server thread and the threading.Event object, which are not picklable.
        Needed when passing InferenceEngineClient as an argument to async_run_ray_method(), mainly for
        invoking `init_weight_sync_state()` and `broadcast_to_inference_engines()`, which do
        not need these attributes.
        """
        state = self.__dict__.copy()
        state["_server_thread"] = None
        state["generation_paused_event"] = None
        return state
```

</details>

#### <span className="bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200 inline-block px-1.5 py-0.5 rounded text-xs font-medium align-middle">method</span> `_spin_up_http_endpoint`

```python
_spin_up_http_endpoint()
```


<details>
<summary>Source code in `skyrl_train/inference_engines/inference_engine_client.py:675-699`</summary>

```python
    def _spin_up_http_endpoint(self):
        from skyrl_train.inference_engines.inference_engine_client_http_endpoint import (
            serve,
            wait_for_server_ready,
        )

        self._server_thread = threading.Thread(
            target=serve,
            args=(self,),
            kwargs={
                "host": self.http_endpoint_host,
                "port": self.http_endpoint_port,
                "log_level": "warning",
            },
            daemon=True,
        )
        self._server_thread.start()
        wait_for_server_ready(
            host=self.http_endpoint_host,
            port=self.http_endpoint_port,
            max_wait_seconds=30,
        )
        logger.info(
            f"InferenceEngineClient HTTP endpoint started on {self.http_endpoint_host}:{self.http_endpoint_port}"
        )
```

</details>
