---
title: "Logging"
---

By default, SkyRL separates **training progress** from **infrastructure logs**:

- **stdout** shows only what you care about during a run: configuration, dataset loading, training steps, rewards, and metrics.
- **Infrastructure logs** (vLLM engine startup, model loading, KV cache allocation, weight syncing, worker initialization) are written to a log file on disk.

This keeps your terminal clean while preserving full diagnostic detail for debugging.

## Log File Location

Infrastructure logs are written to:

```
{SKYRL_LOG_DIR}/{run_name}/infra.log
```

With default settings this is `/tmp/skyrl-logs/{run_name}/infra.log`, where `run_name` comes from `trainer.run_name` in your config.

## Configuration

Logging behavior is controlled by two environment variables:

`SKYRL_LOG_DIR`
  - **Default**: `/tmp/skyrl-logs`
  - **Purpose**: Base directory for infrastructure log files
  - Unlike `trainer.ckpt_path` / `trainer.export_path`, this is an env var because it's a local-only path (cannot use cloud storage) and may need to differ per node in multi-node setups

`SKYRL_DUMP_INFRA_LOG_TO_STDOUT`
  - **Default**: `False` (disabled)
  - **Purpose**: When set to `1`, infrastructure logs are shown on stdout instead of being redirected to the log file. Useful for debugging startup issues.

> `SKYRL_LOG_FILE` is set automatically by `initialize_ray()` — you do not need to set it yourself.

## Usage

**Normal run** — clean stdout, infrastructure logs to file `/tmp/skyrl-logs/my-run/infra.log`:

```bash
bash examples/gsm8k/run_gsm8k.sh
```

**Custom log directory:**

```bash
SKYRL_LOG_DIR=/home/user/logs bash examples/gsm8k/run_gsm8k.sh
```

**Dump infrastructure logs to stdout** (no file redirection):

```bash
SKYRL_DUMP_INFRA_LOG_TO_STDOUT=1 bash examples/gsm8k/run_gsm8k.sh
```

## How It Works

SkyRL uses OS-level file descriptor redirection (`os.dup2`) to route each Ray actor's stdout/stderr to a shared log file. The key design principle is selective redirection:

- **vLLM inference engines** and **training workers** redirect their output to the log file at actor initialization time.
- The **training entrypoint** (`skyrl_entrypoint`) does *not* redirect, so training progress flows to your terminal as usual.

Because redirection happens at the file descriptor level, it captures all output — including logs from vLLM's `EngineCore` subprocess (model loading, KV cache setup) that would bypass Python-level logging intercepts.

The redirect logic lives in `skyrl_train/utils/ray_logging.py` and is called from:

- `BaseVLLMInferenceEngine.__init__()` — covers both sync and async vLLM engines
- `DistributedTorchRayActor.__init__()` — covers policy and reference model workers

## Multi-Node

In multi-node training, each node writes its own `infra.log` at the same local path. The log directory is created automatically on each node when the first actor starts. This is consistent with how Ray's own per-node logs work (`/tmp/ray/session_*/logs/`).

## Known Limitations

- **Ray system messages still appear on stdout.** A small number of `(raylet)` log lines are emitted by Ray itself before any actors start. These are not captured by actor-level redirection.

- **All actors share one log file per node.** vLLM engines and workers on the same node all append to the same `infra.log`. Under heavy logging, lines from different actors may interleave.
