---
title: "Cookbook Scripts"
---

The [tinker-cookbook](https://github.com/ThinkingMachinesLab/tinker-cookbook) is a library provided by Thinking Machines with ready-to-run training recipes. This page describes how to run a few example recipes on SkyRL, and provides example curves from our experiments.

## Setup

### Start the SkyRL Server

```bash
cd SkyRL/skyrl-tx

uv run --extra tinker --extra skyrl_train -m tx.tinker.api \
    --base-model "Qwen/Qwen3-0.6B" \
    --backend skyrl_train
```

Wait for the server to be ready (you'll see `Uvicorn running on http://0.0.0.0:8000`).

### Clone the Cookbook

```bash
git clone https://github.com/ThinkingMachinesLab/tinker-cookbook.git
cd tinker-cookbook
```

## Recipes

### Supervised Learning Loop (`sl_loop`)

Fine-tunes a model on the [No Robots](https://huggingface.co/datasets/HuggingFaceH4/no_robots) dataset using cross-entropy loss with a linear learning rate decay.

```bash
uv run --with tinker --with datasets \
    python -m tinker_cookbook.recipes.sl_loop \
    base_url=http://localhost:8000 \
    model_name="Qwen/Qwen3-0.6B" \
    log_path="/tmp/tinker-sl" \
    train_on_what=LAST_ASSISTANT_MESSAGE
```

<!-- TODO(tyler): add a full fine-tuning example (no LoRA) -->

TODO: example curve(s)

### RL Training Loop (`rl_loop`)

Trains a model on [GSM8K](https://huggingface.co/datasets/openai/gsm8k) math problems using GRPO-style reward centering with importance sampling.

```bash
uv run --with tinker --with datasets --with torch \
    python -m tinker_cookbook.recipes.rl_loop \
    base_url=http://localhost:8000 \
    model_name="Qwen/Qwen3-0.6B" \
    batch_size=8 \
    group_size=4 \
    lora_rank=32 \
    max_tokens=128 \
    learning_rate=4e-5 \
    save_every=5 \
    log_path="/tmp/tinker-rl"
```

Note: `rl_loop` uses persistent mode by default, saving a full model checkpoint to disk on every weight sync. For long training runs, consider using ephemeral mode (via `save_weights_and_get_sampling_client()` with no `name` argument) to avoid large disk usage â€” see [Weight Sync](./architecture#weight-sync) for details.

TODO: example curve(s)

### Code RL (`code_rl`)

RL training for code generation tasks. Uses the same `importance_sampling` loss with code execution-based rewards.

```bash
uv run --with tinker --with datasets --with torch \
    python -m tinker_cookbook.recipes.code_rl.train \
    base_url=http://localhost:8000 \
    model_name="Qwen/Qwen3-0.6B" \
    batch_size=8 \
    group_size=4 \
    lora_rank=32
```

### Math RL (`math_rl`)

RL training specifically for mathematical reasoning, with specialized reward grading.

```bash
uv run --with tinker --with datasets --with torch \
    python -m tinker_cookbook.recipes.math_rl.train \
    base_url=http://localhost:8000 \
    model_name="Qwen/Qwen3-0.6B" \
    batch_size=8 \
    group_size=4 \
    lora_rank=32
```