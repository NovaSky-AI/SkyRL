---
title: "Configuration"
---

This page describes how to configure the SkyRL Tinker backend, including GPU allocation, training parameters, and inference settings.

<Callout type="warn">
The backend configuration system is undergoing a migration to simplify and reorganize these options. The keys and structure described below may change in a future release.
</Callout>

The `--backend-config` flag accepts a JSON dictionary of dot-notation overrides that are applied to the underlying SkyRL-Train configuration. For example:

```bash
uv run --extra tinker --extra fsdp -m skyrl.tinker.api \
    --base-model "Qwen/Qwen3-0.6B" --backend fsdp \
    --backend-config '{"trainer.placement.policy_num_gpus_per_node": 4, "generator.num_inference_engines": 4}'
```

Any field in the [SkyRL-Train config](https://docs.skyrl.ai/docs/configuration/config) can be overridden this way (see the [default config YAML](https://github.com/NovaSky-AI/SkyRL/blob/main/skyrl/skyrl/train/config/ppo_base_config.yaml) for all available keys and defaults). The most commonly used options are listed below.

### GPU and Parallelism

| Key | Default | Description |
|-----|---------|-------------|
| `trainer.placement.policy_num_gpus_per_node` | `1` | Number of GPUs for training |
| `trainer.placement.policy_num_nodes` | `1` | Number of nodes for training |
| `generator.num_inference_engines` | `1` | Number of vLLM inference engines for sampling |
| `generator.inference_engine_tensor_parallel_size` | `1` | Tensor parallel size per inference engine |

When running on multiple GPUs, you typically want to set `policy_num_gpus_per_node` and `num_inference_engines` to the same value. For example, on a 4-GPU node:

```bash
--backend-config '{"trainer.placement.policy_num_gpus_per_node": 4, "generator.num_inference_engines": 4}'
```

For large models that don't fit on a single GPU for inference, increase `inference_engine_tensor_parallel_size` and decrease `num_inference_engines` accordingly. For example, on 4 GPUs with TP=2:

```bash
--backend-config '{"trainer.placement.policy_num_gpus_per_node": 4, "generator.num_inference_engines": 2, "generator.inference_engine_tensor_parallel_size": 2}'
```

### Training

| Key | Default | Description |
|-----|---------|-------------|
| `trainer.micro_train_batch_size_per_gpu` | `1` | Micro-batch size per GPU (for gradient accumulation) |
| `trainer.gradient_checkpointing` | `true` | Enable activation checkpointing to reduce memory |
| `trainer.policy.optimizer_config.lr` | `1e-6` | Learning rate (note: Tinker clients typically manage LR externally) |
| `trainer.policy.optimizer_config.max_grad_norm` | `1.0` | Gradient clipping norm |
| `trainer.policy.fsdp_config.cpu_offload` | `false` | Offload parameters and optimizer state to CPU |

### Inference

| Key | Default | Description |
|-----|---------|-------------|
| `generator.gpu_memory_utilization` | `0.8` | Fraction of GPU memory for vLLM KV cache |
| `generator.max_num_batched_tokens` | `8192` | Maximum tokens per vLLM batch |
| `generator.enforce_eager` | `true` | Disable CUDA graphs (more stable, slightly slower) |
| `generator.enable_prefix_caching` | `true` | Enable vLLM prefix caching |

### Checkpoints

| Key | Default | Description |
|-----|---------|-------------|
| `trainer.ckpt_path` | `~/ckpts/` | Directory for full training checkpoints |
| `trainer.ckpt_interval` | `10` | Save a full checkpoint every N steps |

## LoRA

LoRA is configured from the **client side**, not the server. When creating a model via the Tinker SDK, pass a `lora_config` with the desired rank. For example, in tinker-cookbook recipes:

```bash
# LoRA training (default in most recipes)
python -m tinker_cookbook.recipes.sl_loop ... lora_rank=32

# Full fine-tuning
python -m tinker_cookbook.recipes.sl_loop ... lora_rank=0
```

No server-side configuration is needed to switch between LoRA and full fine-tuning.

## Full Config Reference

For the complete list of configuration options, see the [SkyRL-Train configuration docs](https://docs.skyrl.ai/docs/configuration/config) and the [default config YAML](https://github.com/NovaSky-AI/SkyRL/blob/main/skyrl/skyrl/train/config/ppo_base_config.yaml).
