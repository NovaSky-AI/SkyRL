---
title: "Limitations & Roadmap"
---

The Tinker integration is under active development as we continue to close the feature gap between SkyRL and Thinking Machines' hosted service. This page documents current limitations and planned improvements.

## Current Limitations

### Single Model

Only one training model and one set of sampling weights can be loaded at a time. Calling `create_model` when a model already exists will return an error. After a weight sync, all subsequent `sample()` calls use the updated weights â€” there is no support for maintaining multiple sampling snapshots concurrently. To switch models, restart the server.

### Single-tenant LoRA
Related to the above limitation, even when training with LoRA adaptors, the SkyRL-Train backend only supports one training model and one set of sampling weights. We plan to support training and sampling on multiple LoRA adaptors concurrently in the future.

### Vision Language Models

Vision language models (VLMs) are not yet supported through the Tinker integration. Only text-based models can be used for training and sampling.


### Batch Size Constraint

The batch size must be evenly divisible by the data parallelism size (number of GPUs). For example, with 4 GPUs you cannot use a batch size of 5.

### No Prompt Logprobs

The `sample()` API does not yet return prompt logprobs, even when requested. A warning is logged but no error is raised. This may affect scripts that rely on prompt logprobs for KL penalty computation.

### RL Loss Functions

Only `cross_entropy` and `importance_sampling` are currently wired through the Tinker data conversion path. SkyRL's `PolicyLossRegistry` contains implementations for PPO (`regular`), `cispo`, and others, but the Tinker backend's `_to_training_batch()` does not yet populate the `advantages` and `action_log_probs` fields these loss functions require.

## Roadmap

### Near-Term

- **RL loss data conversion**: Populate `advantages` and `action_log_probs` in `_to_training_batch()` so that PPO, CISPO, and other RL losses work through the Tinker API
- **PPO loss alias**: Register `ppo` as an alias for `regular` in `PolicyLossRegistry`
- **DRO loss**: Implement and register `dro` (Distributionally Robust Optimization) loss function
- **`forward_backward_custom` support**: Validate end-to-end once `forward()` is implemented, enabling DPO and other custom loss recipes
- **Prompt logprobs** in `sample()` responses
- **Backend config passthrough** for overriding SkyRL training parameters (learning rate schedule, optimizer config, parallelism strategy)
- **Automatic database cleanup** on server startup
- **Multi-model support** for serving multiple LoRA adapters concurrently

### Medium-Term

- **Full tinker-cookbook validation** across all recipe categories (distillation, preference, multi-player RL)
- **Training curves** demonstrating convergence parity with other Tinker backends
- **Megatron strategy** testing and stabilization for large-scale models
- **Full fine-tuning** support (in addition to LoRA)

### Long-Term

- **Multi-node deployment** with Ray clusters spanning multiple machines
- **Dynamic batching** in the engine layer for better GPU utilization
- **Streaming metrics** via WebSocket or SSE instead of polling
